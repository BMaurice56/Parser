<article>
  <preamble>Boudin-Torres-2006.pdf</preamble>
  <titre>A Scalable MMR Approach to Sentence Scoring
for Multi-Document Update Summarization</titre>
  <auteurs>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>√âcole Polytechnique de Montr√©al
CP 6079 Succ. Centre Ville H3C 3A7
Montr√©al (Qu√©bec), Canada.</affiliation>
    </auteur>
    <auteur>
      <name>Florian Boudin</name>
      <mail>florian.boudin@univ-avignon.fr</mail>
      <affiliation>Laboratoire Informatique d‚ÄôAvignon
339 chemin des Meinajaries, BP1228,
84911 Avignon Cedex 9, France.</affiliation>
    </auteur>
    <auteur>
      <name>Marc El-B√®ze</name>
      <mail>marc.elbeze@univ-avignon.fr</mail>
      <affiliation>Laboratoire Informatique d‚ÄôAvignon
339 chemin des Meinajaries, BP1228,
84911 Avignon Cedex 9, France.</affiliation>
    </auteur>
  </auteurs>
  <abstract>We present S MMR , a scalable sentence
scoring method for query-oriented up-
date summarization. Sentences are scored
thanks to a criterion combining query rele-
vance and dissimilarity with already read
documents (history). As the amount of
data in history increases, non-redundancy
is prioritized over query-relevance. We
show that S MMR achieves promising re-
sults on the DUC 2007 update corpus.</abstract>
  <introduction>
Extensive experiments on query-oriented multi-
document summarization have been carried out
over the past few years. Most of the strategies
to produce summaries are based on an extrac-
tion method, which identiÔ¨Åes salient textual seg-
ments, most often sentences, in documents. Sen-
tences containing the most salient concepts are se-
lected, ordered and assembled according to their
relevance to produce summaries (also called ex-
tracts) (Mani and Maybury, 1999).
Recently emerged from the Document Under-
standing Conference (DUC) 20071, update sum-
marization attempts to enhance summarization
when more information about knowledge acquired
by the user is available. It asks the following ques-
tion: has the user already read documents on the
topic? In the case of a positive answer, producing
an extract focusing on only new facts is of inter-
est. In this way, an important issue is introduced:
c/circlecopyrt2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Document Understanding Conferences are conducted
since 2000 by the National Institute of Standards and Tech-
nology (NIST), http://www-nlpir.nist.govredundancy with previously read documents (his-
tory) has to be removed from the extract.
A natural way to go about update summarization
would be extracting temporal tags (dates, elapsed
times, temporal expressions...) (Mani and Wilson,
2000) or to automatically construct the timeline
from documents (Swan and Allan, 2000). These
temporal marks could be used to focus extracts on
the most recently written facts. However, most re-
cently written facts are not necessarily new facts.
Machine Reading (MR) was used by (Hickl et
al., 2007) to construct knowledge representations
from clusters of documents. Sentences contain-
ing ‚Äúnew‚Äù information (i.e. that could not be in-
ferred by any previously considered document)
are selected to generate summary. However, this
highly efÔ¨Åcient approach (best system in DUC
2007 update) requires large linguistic resources.
(Witte et al., 2007) propose a rule-based system
based on fuzzy coreference cluster graphs. Again,
this approach requires to manually write the sen-
tence ranking scheme. Several strategies remain-
ing on post-processing redundancy removal tech-
niques have been suggested. Extracts constructed
from history were used by (Boudin and Torres-
Moreno, 2007) to minimize history‚Äôs redundancy.
(Lin et al., 2007) have proposed a modiÔ¨Åed Max-
imal Marginal Relevance (MMR) (Carbonell and
Goldstein, 1998) re-ranker during sentence selec-
tion, constructing the summary by incrementally
re-ranking sentences.
In this paper, we propose a scalable sentence
scoring method for update summarization derived
from MMR. Motivated by the need for relevant
novelty, candidate sentences are selected accord-
ing to a combined criterion of query relevance and
dissimilarity with previously read sentences. The
rest of the paper is organized as follows. Section 2 introduces our proposed sentence scoring method
and Section 3 presents experiments and evaluates
our approach.
</introduction>
  <corps>The underlying idea of our method is that as the
number of sentences in the history increases, the
likelihood to have redundant information within
candidate sentences also increases. We propose
a scalable sentence scoring method derived from
MMR that, as the size of the history increases,
gives more importance to non-redundancy that to
query relevance. We deÔ¨Åne Hto represent the pre-
viously read documents (history), Qto represent
the query and sthe candidate sentence. The fol-
lowing subsections formally deÔ¨Åne the similarity
measures and the scalable MMR scoring method.
2.1 A query-oriented multi-document
summarizer
We have Ô¨Årst started by implementing a simple
summarizer for which the task is to produce query-
focused summaries from clusters of documents.
Each document is pre-processed: documents are
segmented into sentences, sentences are Ô¨Åltered
(words which do not carry meaning are removed
such as functional words or common words) and
normalized using a lemmas database (i.e. inÔ¨Çected
forms ‚Äúgo‚Äù, ‚Äúgoes‚Äù, ‚Äúwent‚Äù, ‚Äúgone‚Äù... are replaced
by ‚Äúgo‚Äù). An N-dimensional term-space Œì, where
Nis the number of different terms found in the
cluster, is constructed. Sentences are represented
inŒìby vectors in which each component is the
term frequency within the sentence. Sentence scor-
ing can be seen as a passage retrieval task in Infor-
mation Retrieval (IR). Each sentence sis scored by
computing a combination of two similarity mea-
sures between the sentence and the query. The Ô¨Årst
measure is the well known cosine angle (Salton et
al., 1975) between the sentence and the query vec-
torial representations in Œì(denoted respectively /vector s
and/vectorQ). The second similarity measure is based
on the Jaro-Winkler distance (Winkler, 1999). The
original Jaro-Winkler measure, denoted J W, uses
the number of matching characters and transposi-
tions to compute a similarity score between two
terms, giving more favourable ratings to terms that
match from the beginning. We have extended this
measure to calculate the similarity between thesentencesand the query Q:
JWe(s,Q) =1
|Q|¬∑/summationdisplay
q‚ààQmax
m‚ààS/primeJW(q,m)(1)
whereS/primeis the term set of sin which the terms
mthat already have maximized J W(q,m)are re-
moved. The use of J Wesmooths normalization and
misspelling errors. Each sentence sis scored using
the linear combination:
Sim 1(s,Q) =Œ±¬∑cosine (/vector s,/vectorQ)
+ (1‚àíŒ±)¬∑JWe(s,Q)(2)
whereŒ±= 0.7, optimally tuned on the past DUCs
data (2005 and 2006). The system produces a list
of ranked sentences from which the summary is
constructed by arranging the high scored sentences
until the desired size is reached.
2.2 A scalable MMR approach
MMR re-ranking algorithm has been successfully
used in query-oriented summarization (Ye et al.,
2005). It strives to reduce redundancy while main-
taining query relevance in selected sentences. The
summary is constructed incrementally from a list
of ranked sentences, at each iteration the sentence
which maximizes MMR is chosen:
MMR = arg max
s‚ààS[Œª¬∑Sim 1(s,Q)
‚àí(1‚àíŒª)¬∑max
sj‚ààESim 2(s,sj) ] (3)
whereSis the set of candidates sentences and E
is the set of selected sentences. Œªrepresents an
interpolation coefÔ¨Åcient between sentence‚Äôs rele-
vance and non-redundancy. Sim 2(s,sj)is a nor-
malized Longest Common Substring (LCS) mea-
sure between sentences sandsj. Detecting sen-
tence rehearsals, LCS is well adapted for redun-
dancy removal.
We propose an interpretation of MMR to tackle
the update summarization issue. Since Sim 1and
Sim 2are ranged in [0,1], they can be seen as prob-
abilities even though they are not. Just as rewriting
(3) as (NR stands for Novelty Relevance):
NR= arg max
s‚ààS[Œª¬∑Sim 1(s,Q)
+ (1‚àíŒª)¬∑(1‚àímax
sh‚ààHSim 2(s,sh)) ] (4)
We can understand that (4) equates to an OR com-
bination. But as we are looking for a more intu-
itive AND and since the similarities are indepen-
dent, we have to use the product combination. The scoring method deÔ¨Åned in (2) is modiÔ¨Åed into a
double maximization criterion in which the best
ranked sentence will be the most relevant to the
query AND the most different to the sentences in
H.
SMMR (s) =Sim 1(s,Q)
¬∑/parenleftbigg
1‚àímax
sh‚ààHSim 2(s,sh)/parenrightbiggf(H)
(5)
Decreasing Œªin (3) with the length of the sum-
mary was suggested by (Murray et al., 2005) and
successfully used in the DUC 2005 by (Hachey
et al., 2005), thereby emphasizing the relevance
at the outset but increasingly prioritizing redun-
dancy removal as the process continues. Sim-
ilarly, we propose to follow this assumption in
SMMR using a function denoted fthat as the
amount of data in history increases, prioritize non-
redundancy ( f(H)‚Üí0).
3 Experiments
The method described in the previous section has
been implemented and evaluated by using the
DUC 2007 update corpus2. The following subsec-
tions present details of the different experiments
we have conducted.
3.1 The DUC 2007 update corpus
We used for our experiments the DUC 2007 up-
date competition data set. The corpus is composed
of 10 topics, with 25 documents per topic. The up-
date task goal was to produce short ( ‚àº100 words)
multi-document update summaries of newswire ar-
ticles under the assumption that the user has al-
ready read a set of earlier articles. The purpose
of each update summary will be to inform the
reader of new information about a particular topic.
Given a DUC topic and its 3 document clusters: A
(10 documents), B (8 documents) and C (7 doc-
uments), the task is to create from the documents
three brief, Ô¨Çuent summaries that contribute to sat-
isfying the information need expressed in the topic
statement.
1. A summary of documents in cluster A.
2. An update summary of documents in B, un-
der the assumption that the reader has already
read documents in A.
2More information about the DUC 2007 corpus is avail-
able at http://duc.nist.gov/.3. An update summary of documents in C, un-
der the assumption that the reader has already
read documents in A and B.
Within a topic, the document clusters must be pro-
cessed in chronological order. Our system gener-
ates a summary for each cluster by arranging the
high ranked sentences until the limit of 100 words
is reached.
3.2 Evaluation
Most existing automated evaluation methods work
by comparing the generated summaries to one or
more reference summaries (ideally, produced by
humans). To evaluate the quality of our generated
summaries, we choose to use the R OUGE3(Lin,
2004) evaluation toolkit, that has been found to be
highly correlated with human judgments. R OUGE -
Nis an-gram recall measure calculated between
a candidate summary and a set of reference sum-
maries. In our experiments R OUGE -1, R OUGE -2
and R OUGE -SU4 will be computed.
3.3 Results
Table 1 reports the results obtained on the DUC
2007 update data set for different sentence scor-
ing methods. cosine +JWestands for the scor-
ing method deÔ¨Åned in (2) and NR improves it
with sentence re-ranking deÔ¨Åned in equation (4).
SMMR is the combined adaptation we have pro-
posed in (5). The function f(H)used in S MMR is
the simple rational function1
H, whereHincreases
with the number of previous clusters ( f(H) = 1
for cluster A,1
2for cluster B and1
3for cluster C).
This function allows to simply test the assumption
that non-redundancy have to be favoured as the
size of history grows. Baseline results are obtained
on summaries generated by taking the leading sen-
tences of the most recent documents of the cluster,
up to 100 words (ofÔ¨Åcial baseline of DUC). The
table also lists the three top performing systems at
DUC 2007 and the lowest scored human reference.
As we can see from these results, S MMR out-
performs the other sentence scoring methods. By
ways of comparison our system would have been
ranked second at the DUC 2007 update competi-
tion. Moreover, no post-processing was applied to
the selected sentences leaving an important margin
of progress. Another interesting result is the high
performance of the non-update speciÔ¨Åc method
(cosine +JWe) that could be due to the small size
3ROUGE is available at http://haydn.isi.edu/ROUGE/. of the corpus (little redundancy between clusters).
ROUGE -1 ROUGE -2 ROUGE -SU4
Baseline 0.26232 0.04543 0.08247
3rdsystem 0.35715 0.09622 0.13245
2ndsystem 0.36965 0.09851 0.13509
cosine +JWe 0.35905 0.10161 0.13701
NR 0.36207 0.10042 0.13781
SMMR 0.36323 0.10223 0.13886
1stsystem 0.37032 0.11189 0.14306
Worst human 0.40497 0.10511 0.14779
Table 1: R OUGE average recall scores computed
on the DUC 2007 update corpus.</corps>
  <conclusion>Aucune conclusion</conclusion>
  <discussion>In this paper we have described S MMR , a scal-
able sentence scoring method based on MMR that
achieves very promising results. An important as-
pect of our sentence scoring method is that it does
not requires re-ranking nor linguistic knowledge,
which makes it a simple and fast approach to the
issue of update summarization. It was pointed out
at the DUC 2007 workshop that Question Answer-
ing and query-oriented summarization have been
converging on a common task. The value added
by summarization lies in the linguistic quality. Ap-
proaches mixing IR techniques are well suited for
query-oriented summarization but they require in-
tensive work for making the summary Ô¨Çuent and
coherent. Among the others, this is a point that we
think is worthy of further investigation.</discussion>
  <bibliographie>Boudin, F. and J.M. Torres-Moreno. 2007. A Co-
sine Maximization-Minimization approach for User-
Oriented Multi-Document Update Summarization.
InRecent Advances in Natural Language Processing
(RANLP) , pages 81‚Äì87.
Carbonell, J. and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval , pages 335‚Äì336.
ACM Press New York, NY , USA.Hachey, B., G. Murray, and D. Reitter. 2005. The
Embra System at DUC 2005: Query-oriented Multi-
document Summarization with a Very Large Latent
Semantic Space. In Document Understanding Con-
ference (DUC) .
Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC‚Äôs
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. In Document Understanding
Conference (DUC) .
Lin, Z., T.S. Chua, M.Y . Kan, W.S. Lee, L. Qiu, and
S. Ye. 2007. NUS at DUC 2007: Using Evolu-
tionary Models of Text. In Document Understanding
Conference (DUC) .
Lin, C.Y . 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Workshop on Text Sum-
marization Branches Out , pages 25‚Äì26.
Mani, I. and M.T. Maybury. 1999. Advances in Auto-
matic Text Summarization . MIT Press.
Mani, I. and G. Wilson. 2000. Robust temporal pro-
cessing of news. In 38th Annual Meeting on Asso-
ciation for Computational Linguistics , pages 69‚Äì76.
Association for Computational Linguistics Morris-
town, NJ, USA.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Recordings. In Ninth Eu-
ropean Conference on Speech Communication and
Technology . ISCA.
Salton, G., A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM , 18(11):613‚Äì620.
Swan, R. and J. Allan. 2000. Automatic generation
of overview timelines. In 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval , pages 49‚Äì56.
Winkler, W. E. 1999. The state of record linkage and
current research problems. In Survey Methods Sec-
tion, pages 73‚Äì79.
Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
ing Update Summaries for DUC 2007. In Document
Understanding Conference (DUC) .
Ye, S., L. Qiu, T.S. Chua, and M.Y . Kan. 2005. NUS
at DUC 2005: Understanding documents via con-
cept links. In Document Understanding Conference
(DUC) .</bibliographie>

  <preamble>Das_Martins.pdf</preamble>
  <titre>A Survey on Automatic Text Summarization</titre>
  <auteurs>
    <auteur>
      <name>Dipanjan Das</name>
      <mail>dipanjan@cs.cmu.edu</mail>
      <affiliation>Language Technologies Institute
Carnegie Mellon University</affiliation>
    </auteur>
    <auteur>
      <name>Andr√© F.T.</name>
      <mail>afm@cs.cmu.edu</mail>
      <affiliation>Language Technologies Institute
Carnegie Mellon University</affiliation>
    </auteur>
  </auteurs>
  <abstract>The increasing availability of online information has necessitated intensive
research in the area of automatic text summarization within the Natural Lan-
guage Processing (NLP) community. Over the past half a century, the prob-
lem has been addressed from many different perspectives, in varying domains
and using various paradigms. This survey intends to investigate some of the
most relevant approaches both in the areas of single-document and multiple-
document summarization, giving special emphasis to empirical methods and
extractive techniques. Some promising approaches that concentrate on specific
details of the summarization problem are also discussed. Special attention is
devoted to automatic evaluation of summarization systems, as future research
on summarization is strongly dependent on progress in this area.</abstract>
  <introduction>
The subfield of summarization has been investigated by the NLP community for
nearly the last half century. Radev et al. (2002) define a summary as \a text that
is produced from one or more texts, that conveys important information in the
original text(s), and that is no longer than half of the original text(s) and usually
significantly less than that". This simple definition captures three important aspects
that characterize research on automatic summarization:
Summaries may be produced from a single document ormultiple documents ,
Summaries should preserve important information,
Summaries should be short.
Even if we agree unanimously on these points, it seems from the literature that
any attempt to provide a more elaborate definition for the task would result in
disagreement within the community. In fact, many approaches differ on the manner
of their problem formulations. We start by introducing some common terms in the
summarization dialect: extraction is the procedure of identifying important sections
of the text and producing them verbatim; abstraction aims to produce important
material in a new way; fusion combines extracted parts coherently; and compression
aims to throw out unimportant sections of the text (Radev et al., 2002).
Earliest instances of research on summarizing scientific documents proposed
paradigms for extracting salient sentences from text using features like word and
phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key
phrases (Edmundson, 1969). Various work published since then has concentrated on
other domains, mostly on newswire data. Many approaches addressed the problem
by building systems depending of the type of the required summary. While extractive
summarization is mainly concerned with what the summary content should be, usu-
ally relying solely on extraction of sentences, abstractive summarization puts strong
emphasis on the form, aiming to produce a grammatical summary, which usually
requires advanced language generation techniques. In a paradigm more tuned to
information retrieval (IR), one can also consider topic-driven summarization , that
assumes that the summary content depends on the preference of the user and can
be assessed via a query , making the final summary focused on a particular topic.
A crucial issue that will certainly drive future research on summarization is
evaluation . During the last fifteen years, many system evaluation competitions like
TREC,1DUC2and MUC3have created sets of training material and have estab-
lished baselines for performance levels. However, a universal strategy to evaluate
summarization systems is still absent.
In this survey, we primarily aim to investigate how empirical methods have been
used to build summarization systems. The rest of the paper is organized as fol-
lows: Section 2 describes single-document summarization, focusing on extractive
techniques. Section 3 progresses to discuss the area of multi-document summariza-
tion, where a few abstractive approaches that pioneered the field are also considered.
Section 4 brie
y discusses some unconventional approaches that we believe can be
useful in the future of summarization research. Section 5 elaborates a few eval-
uation techniques and describes some of the standards for evaluating summaries
automatically. Finally, Section 6 concludes the survey.
</introduction>
  <corps>Usually, the
ow of information in a given document is not uniform, which means
that some parts are more important than others. The major challenge in summa-
rization lies in distinguishing the more informative parts of a document from the
less ones. Though there have been instances of research describing the automatic
creation of abstracts , most work presented in the literature relies on verbatim ex-
traction of sentences to address the problem of single-document summarization. In
1Seehttp://trec.nist.gov/ .
2Seehttp://duc.nist.gov/ .
3Seehttp://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/.
muc7toc.html
this section, we describe some eminent extractive techniques. First, we look at early
work from the 1950s and 60s that kicked off research on summarization. Second,
we concentrate on approaches involving machine learning techniques published in
the 1990s to today. Finally, we brie
y describe some techniques that use a more
complex natural language analysis to tackle the problem.
2.1 Early Work
Most early work on single-document summarization focused on technical documents .
Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de-
scribes research done at IBM in the 1950s. In his work, Luhn proposed that the
frequency of a particular word in an article provides an useful measure of its sig-
nificance. There are several key ideas put forward in this paper that have assumed
importance in later work on summarization. As a first step, words were stemmed to
their root forms, and stop words were deleted. Luhn then compiled a list of content
words sorted by decreasing frequency, the index providing a significance measure of
the word. On a sentence level, a significance factor was derived that re
ects the
number of occurrences of significant words within a sentence, and the linear distance
between them due to the intervention of non-significant words. All sentences are
ranked in order of their significance factor, and the top ranking sentences are finally
selected to form the auto-abstract.
Related work (Baxendale, 1958), also done at IBM and published in the same
journal, provides early insight on a particular feature helpful in finding salient parts
of documents: the sentence position . Towards this goal, the author examined 200
paragraphs to find that in 85% of the paragraphs the topic sentence came as the first
one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate
way to select a topic sentence would be to choose one of these two. This positional
feature has since been used in many complex machine learning based systems.
Edmundson (1969) describes a system that produces document extracts. His
primary contribution was the development of a typical structure for an extractive
summarization experiment. At first, the author developed a protocol for creating
manual extracts, that was applied in a set of 400 technical documents. The two
features of word frequency and positional importance were incorporated from the
previous two works. Two other features were used: the presence of cue words
(presence of words like significant , or hardly ), and the skeleton of the document
(whether the sentence is a title or heading). Weights were attached to each of these
features manually to score each sentence. During evaluation, it was found that about
44% of the auto-extracts matched the manual extracts.
2.2 Machine Learning Methods
In the 1990s, with the advent of machine learning techniques in NLP, a series of semi-
nal publications appeared that employed statistical techniques to produce document
extracts. While initially most systems assumed feature independence and relied on
naive-Bayes methods, others have focused on the choice of appropriate features and
on learning algorithms that make no independence assumptions. Other significant
approaches involved hidden Markov models and log-linear models to improve ex-
tractive summarization. A very recent paper, in contrast, used neural networks and
third party features (like common words in search engine queries) to improve purely
extractive single document summarization. We next describe all these approaches
in more detail.
2.2.1 Naive-Bayes Methods
Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able
to learn from data. The classification function categorizes each sentence as worthy
of extraction or not, using a naive-Bayes classifier . Letsbe a particular sentence,
Sthe set of sentences that make up the summary, and F1;:::;Fkthe features.
Assuming independence of the features:
P(s2SjF1;F2;::Fk) =Qk
i=1P(Fijs2S)P(s2S)Qk
i=1P(Fi)(1)
The features were compliant to (Edmundson, 1969), but additionally included the
sentence length and the presence of uppercase words . Each sentence was given a
score according to (1), and only the ntop sentences were extracted. To evaluate
the system, a corpus of technical documents with manual abstracts was used in
the following way: for each sentence in the manual abstract, the authors manually
analyzed its match with the actual document sentences and created a mapping
(e.g. exact match with a sentence, matching a join of two sentences, not matchable,
etc.). The auto-extracts were then evaluated against this mapping. Feature analysis
revealed that a system using only the position and the cue features, along with the
sentence length sentence feature, performed best.
Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richer
features. They describe a system called DimSum that made use of features like
term frequency ( tf) and inverse document frequency ( idf) to derive signature words .4
Theidfwas computed from a large corpus of the same domain as the concerned
documents. Statistically derived two-noun word collocations were used as units for
counting, along with single words. A named-entity tagger was used and each entity
was considered as a single token. They also employed some shallow discourse analysis
like reference to same entities in the text, maintaining cohesion. The references
were resolved at a very shallow level by linking name aliases within a document
like \U.S." to \United States", or \IBM" for \International Business Machines".
Synonyms and morphological variants were also merged while considering lexical
terms, the former being identified by using Wordnet (Miller, 1995). The corpora
used in the experiments were from newswire, some of which belonged to the TREC
evaluations.
4Words that indicate key concepts in a document.
2.2.2 Rich Features and Decision Trees
Lin and Hovy (1997) studied the importance of a single feature, sentence position .
Just weighing a sentence by its position in text, which the authors term as the
\position method", arises from the idea that texts generally follow a predictable
discourse structure, and that the sentences of greater topic centrality tend to occur in
certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse
structure significantly varies over domains, the position method cannot be defined
as naively as in (Baxendale, 1958). The paper makes an important contribution by
investigating techniques of tailoring the position method towards optimality over a
genre and how it can be evaluated for effectiveness. A newswire corpus was used, the
collection of Ziff-Davis texts produced from the TIPSTER5program; it consists of
text about computer and related hardware, accompanied by a set of key topic words
and a small abstract of six sentences. For each document in the corpus, the authors
measured the yield of each sentence position against the topic keywords. They then
ranked the sentence positions by their average yield to produce the Optimal Position
Policy (OPP) for topic positions for the genre.
Two kinds of evaluation were performed. Previously unseen text was used for
testing whether the same procedure would work in a different domain. The first
evaluation showed contours exactly like the training documents. In the second eval-
uation, word overlap of manual abstracts with the extracted sentences was measured.
Windows in abstracts were compared with windows on the selected sentences and
corresponding precision and recall values were measured. A high degree of coverage
indicated the effectiveness of the position method.
In later work, Lin (1999) broke away from the assumption that features are
independent of each other and tried to model the problem of sentence extraction
using decision trees , instead of a naive-Bayes classifier. He examined a lot of fea-
tures and their effect on sentence extraction. The data used in this work is a
publicly available collection of texts, classified into various topics, provided by the
TIPSTER-SUMMAC6evaluations, targeted towards information retrieval systems.
The dataset contains essential text fragments (phrases, clauses, and sentences) which
must be included in summaries to answer some TREC topics. These fragments were
each evaluated by a human judge. The experiments described in the paper are with
the SUMMARIST system developed at the University of Southern California. The
system extracted sentences from the documents and those were matched against
human extracts, like most early work on extractive summarization.
Some novel features were the query signature (normalized score given to sen-
tences depending on number of query words that they contain), IR signature (the
mmost salient words in the corpus, similar to the signature words of (Aone et al.,
1999)), numerical data (boolean value 1 given to sentences that contained a num-
ber in them), proper name (boolean value 1 given to sentences that contained a
proper name in them), pronoun or adjective (boolean value 1 given to sentences
5Seehttp://www.itl.nist.gov/iaui/894.02/related_projects/tipster/ .
6Seehttp://www-nlpir.nist.gov/related_projects/tipster_summac/index.html .
that contained a pronoun or adjective in them), weekday or month (similar as pre-
vious feature) and quotation (similar as previous feature). It is worth noting that
some features like the query signature are question-oriented because of the setting
of the evaluation, unlike a generalized summarization framework.
The author experimented with various baselines, like using only the positional
feature, or using a simple combination of all features by adding their values. When
evaluated by matching machine extracted and human extracted sentences, the deci-
sion tree classifier was clearly the winner for the whole dataset, but for three topics,
a naive combination of features beat it. Lin conjectured that this happened because
some of the features were independent of each other. Feature analysis suggested
that the IR signature was a valuable feature, corroborating the early findings of
Luhn (1958).
2.2.3 Hidden Markov Models
In contrast with previous approaches, that were mostly feature-based and non-
sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentence
from a document using a hidden Markov model (HMM). The basic motivation for
using a sequential model is to account for local dependencies between sentences.
Only three features were used: position of the sentence in the document (built into
the state structure of the HMM), number of terms in the sentence, and likeliness of
the sentence terms given the document terms.
no3 2 1 no no no
Figure 1: Markov model to extract to three summary sentences from a document
(Conroy and O'leary, 2001).
The HMM was structured as follows: it contained 2 s+ 1 states, alternating be-
tweenssummary states ands+1nonsummary states . The authors allowed \hesita-
tion" only in nonsummary states and \skipping next state" only in summary states.
Figure 1 shows an example HMM with 7 nodes, corresponding to s= 3. Using the
TREC dataset as training corpus, the authors obtained the maximum-likelihood
estimate for each transition probability, forming the transition matrix estimate ^M,
whose element ( i;j) is the empirical probability of transitioning from state itoj.
Associated with each state iwas an output function, bi(O) = Pr(Ojstatei) where
Ois an observed vector of features. They made a simplifying assumption that the
features are multivariate normal. The output function for each state was thus esti-
mated by using the training data to compute the maximum likelihood estimate of
its mean and covariance matrix. They estimated 2 s+1 means, but assumed that all
of the output functions shared a common covariance matrix. Evaluation was done
by comparing with human generated extracts.
2.2.4 Log-Linear Models
Osborne (2002) claims that existing approaches to summarization have always as-
sumed feature independence. The author used log-linear models to obviate this
assumption and showed empirically that the system produced better extracts than
a naive-Bayes model, with a prior appended to both models. Let cbe a label, s
the item we are interested in labeling, fithei-th feature, and ithe corresponding
feature weight. The conditional log-linear model used by Osborne (2002) can be
stated as follows:
P(cjs) =1
Z(s)exp X
iifi(c;s)!
; (2)
whereZ(s) =P
cexp (P
iifi(c;s)). In this domain, there are only two possible
labels: either the sentence is to be extracted or it is not. The weights were trained
by conjugate gradient descent. The authors added a non-uniform prior to the model,
claiming that a log-linear model tends to reject too many sentences for inclusion in
a summary. The same prior was also added to a naive-Bayes model for comparison.
The classification took place as follows:
label(s) = arg max
c2CP(c)P(s;c) = arg max
c2C
logP(c) +X
iifi(c;s)!
:(3)
The authors optimized the prior using the f2 score of the classifier as an objective
function on a part of the dataset (in the technical domain). The summaries were
evaluated using the standard f2 score where f2 =2pr
p+r, where the precision and recall
measures were measured against human generated extracts. The features included
word pairs (pairs of words with all words truncated to ten characters), sentence
length ,sentence position , and naive discourse features like inside introduction or
inside conclusion . With respect to f2 score, the log-linear model outperformed the
naive-Bayes classifier with the prior, exhibiting the former's effectiveness.
2.2.5 Neural Networks and Third Party Features
In 2001-02, DUC issued a task of creating a 100-word summary of a single news
article. However, the best performing systems in the evaluations could not outper-
form the baseline with statistical significance. This extremely strong baseline has
been analyzed by Nenkova (2005) and corresponds to the selection of the first n
sentences of a newswire article. This surprising result has been attributed to the
journalistic convention of putting the most important part of an article in the initial
paragraphs. After 2002, the task of single-document summarization for newswire
was dropped from DUC. Svore et al. (2007) propose an algorithm based on neu-
ral nets and the use of third party datasets to tackle the problem of extractive
summarization, outperforming the baseline with statistical significance.
The authors used a dataset containing 1365 documents gathered from CNN.com,
each consisting of the title, timestamp, three or four human generated story high-
lights and the article text. They considered the task of creating three machine
highlights. The human generated highlights were notverbatim extractions from the
article itself. The authors evaluated their system using two metrics: the first one
concatenated the three highlights produced by the system, concatenated the three
human generated highlights, and compared these two blocks ; the second metric con-
sidered the ordering and compared the sentences on an individual level.
Svore et al. (2007) trained a model from the labels and the features for each
sentence of an article, that could infer the proper ranking of sentences in a test
document. The ranking was accomplished using RankNet (Burges et al., 2005), a
pair-based neural network algorithm designed to rank a set of inputs that uses the
gradient descent method for training. For the training set, they used ROUGE-1
(Lin, 2004) to score the similarity of a human written highlight and a sentence
in the document. These similarity scores were used as soft labels during training,
contrasting with other approaches where sentences are \hard-labeled", as selected
or not.
Some of the used features based on position or n-grams frequencies have been
observed in previous work. However, the novelty of the framework lay in the use
of features that derived information from query logs from Microsoft's news search
engine7and Wikipedia8entries. The authors conjecture that if a document sentence
contained keywords used in the news search engine, or entities found in Wikipedia
articles, then there is a greater chance of having that sentence in the highlight. The
extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically
significant improvements over the baseline of selecting the first three sentences in a
document.
2.3 Deep Natural Language Analysis Methods
In this subsection, we describe a set of papers that detail approaches towards single-
document summarization involving complex natural language analysis techniques.
None of these papers solve the problem using machine learning, but rather use a set
of heuristics to create document extracts. Most of these techniques try to model the
text's discourse structure.
Barzilay and Elhadad (1997) describe a work that used considerable amount of
linguistic analysis for performing the task of summarization. For a better under-
standing of their method, we need to define a lexical chain : it is a sequence of related
words in a text, spanning short (adjacent words or sentences) or long distances (en-
tire text). The authors' method progressed with the following steps: segmentation
of the text, identification of lexical chains, and using strong lexical chains to identify
the sentences worthy of extraction. They tried to reach a middle ground between
(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep
7Seehttp://search.live.com/news .
8Seehttp://en.wikipedia.org .
semantic structure of the text, while the latter relied on word statistics of the doc-
uments. The authors describe the notion of cohesion in text as a means of sticking
together different parts of the text. Lexical cohesion is a notable example where
semantically related words are used. For example, let us take a look at the following
sentence.9
John bought a Jag: He loves the car: (4)
Here, the word carrefers to the word Jagin the previous sentence, and exemplifies
lexical cohesion. The phenomenon of cohesion occurs not only at the word level,
but at word sequences too, resulting in lexical chains, which the authors used as
a source representation for summarization. Semantically related words and word
sequences were identified in the document, and several chains were extracted, that
form a representation of the document. To find out lexical chains, the authors used
Wordnet (Miller, 1995), applying three generic steps:
1. Selecting a set of candidate words.
2. For each candidate word, finding an appropriate chain relying on a relatedness
criterion among members of the chains,
3. If it is found, inserting the word in the chain and updating it accordingly.
The relatedness was measured in terms of Wordnet distance. Simple nouns and
noun compounds were used as starting point to find the set of candidates. In the
final steps, strong lexical chains were used to create the summaries. The chains were
scored by their length and homogeneity. Then the authors used a few heuristics to
select the significant sentences.
In another paper, Ono et al. (1994) put forward a computational model of dis-
course for Japanese expository writings, where they elaborate a practical procedure
for extracting the discourse rhetorical structure , a binary tree representing relations
between chunks of sentences (rhetorical structure trees are used more intensively in
(Marcu, 1998a), as we will see below). This structure was extracted using a series
of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can-
didate generation and preference judgement. Evaluation was based on the relative
importance of rhetorical relations. In the following step, the nodes of the rhetori-
cal structure tree were pruned to reduce the sentence, keeping its important parts.
Same was done for paragraphs to finally produce the summary. Evaluation was done
with respect to sentence coverage and 30 editorial articles of a Japanese newspaper
were used as the dataset. The articles had corresponding sets of key sentences and
most important key sentences judged by human subjects. The key sentence coverage
was about 51% and the most important key sentence coverage was 74%, indicating
encouraging results.
Marcu (1998a) describes a unique approach towards summarization that, unlike
most other previous work, does not assume that the sentences in a document form
a
at sequence. This paper used discourse based heuristics with the traditional
9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html .
features that have been used in the summarization literature. The discourse theory
used in this paper is the Rhetorical Structure Theory (RST) that holds between
two non-overlapping pieces of text spans: the nucleus and the satellite . The author
mentions that the distinction between nuclei and satellites comes from the empir-
ical observation that the nucleus expresses what is more essential to the writer's
purpose than the satellite; and that the nucleus of a rhetorical relation is compre-
hensible independent of the satellite, but not vice versa. Marcu (1998b) describes
the details of a rhetorical parser producing a discourse tree. Figure 2 shows an
example discourse tree for a text example detailed in the paper. Once such a dis-
Antithesis2
Elaboration
Elaboration2
2
Elaboration3
Justification8
Exemplification
1 2 34 57 8
458 10
9 10
5 6Contrast
EvidenceConcession
Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the
nodes denote sentence numbers from the text example. The text below the number
in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and
the normals ones are the NUCLEI.
course structure is created, a partial ordering of important units can be developed
from the tree. Each equivalence class in the partial ordering is derived from the
new sentences at a particular level of the discourse tree. In Figure 2, we observe
that sentence 2 is at the root, followed by sentence 8 in the second level. In the
third level, sentence 3 and 10 are observed, and so forth. The equivalence classes
are 2&gt;8&gt;3;10&gt;1;4;5;7;9&gt;6.
If it is specified that the summary should contain the top k% of the text, the first
k% of the units in the partial ordering can be selected to produce the summary. The
author talks about a summarization system based just on this method in (Marcu,
1998b) and in one of his earlier papers. In this paper, he merged the discourse
based heuristics with traditional heuristics. The metrics used were clustering based
metric (each node in the discourse tree was assigned a cluster score; for leaves the
score was 0, for the internal nodes it was given by the similarity of the immediate
children; discourse tree A was chosen to be better than B if its clustering score
was higher), marker based metric (a discourse structure A was chosen to be better
than a discourse structure B if A used more rhetorical relations than B), rhetorical
clustering based technique (measured the similarity between salient units of two text
spans), shape based metric (preferred a discourse tree A over B if A was more skewed
towards the right than B), title based metric ,position based metric ,connectedness
based metric (cosine similarity of an unit to all other text units, a discourse structure
A was chosen to be better than B if its connectedness measure was more than B).
A weighted linear combination of all these scores gave the score of a discourse
structure. To find the best combination of heuristics, the author computed the
weights that maximized the F-score on the training dataset, which was constituted
by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al.,
1992) that performed a greedy search in a seven dimensional space of the metrics.
For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved
for the 10% summaries which was 3.5% higher than a baseline lead based algorithm,
which was very encouraging.
3 Multi-Document Summarization
Extraction of a single summary from multiple documents has gained interest since
mid 1990s, most applications being in the domain of news articles. Several Web-
based news clustering systems were inspired by research on multi-document summa-
rization, for example Google News ,10Columbia NewsBlaster ,11orNews In Essence .12
This departs from single-document summarization since the problem involves mul-
tiple sources of information that overlap and supplement each other, being contra-
dictory at occasions. So the key tasks are not only identifying and coping with
redundancy across documents, but also recognizing novelty and ensuring that the
final summary is both coherent and complete.
The field seems to have been pioneered by the NLP group at Columbia University
(McKeown and Radev, 1995), where a summarization system called SUMMONS13
was developed by extending already existing technology for template-driven message
understanding systems. Although in that early stage multi-document summariza-
tion was mainly seen as a task requiring substantial capabilities of both language
interpretation and generation, it later gained autonomy, as people coming from dif-
ferent communities added new perspectives to the problem. Extractive techniques
have been applied, making use of similarity measures between pairs of sentences.
Approaches vary on how these similarities are used: some identify common themes
through clustering and then select one sentence to represent each cluster (McKeown
10Seehttp://news.google.com .
11Seehttp://newsblaster.cs.columbia.edu .
12Seehttp://NewsInEssence.com .
13SUMMarizing Online NewS articles.
et al., 1999; Radev et al., 2000), others generate a composite sentence from each
cluster (Barzilay et al., 1999), while some approaches work dynamically by includ-
ing each candidate passage only if it is considered novel with respect to the previous
included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998).
Some recent work extends multi-document summarization to multilingual environ-
ments (Evans, 2005).
The way the problem is posed has also varied over time. While in some pub-
lications it is claimed that extractive techniques would not be effective for multi-
document summarization (McKeown and Radev, 1995; McKeown et al., 1999), some
years later that claim was overturned, as extractive systems like MEAD14(Radev
et al., 2000) achieved good performance in large scale summarization of news arti-
cles. This can be explained by the fact that summarization systems often distinguish
among themselves about what their goal actually is. While some systems, like SUM-
MONS, are designed to work in strict domains, aiming to build a sort of briefing
that highlights differences and updates accross different news reports, putting much
emphasis on how information is presented to the user, others, like MEAD, are large
scale systems that intend to work in general domains, being more concerned with
information content rather than form. Consequently, systems of the former kind re-
quire a strong effort on language generation to produce a grammatical and coherent
summary, while latter systems are probably more close to the information retrieval
paradigm. Abstractive systems like SUMMONS are difficult to replicate, as they
heavily rely on the adaptation of internal tools to perform information extraction
and language generation. On the other hand, extractive systems are generally easy
to implement from scratch, and this makes them appealing when sophisticated NLP
tools are not available.
3.1 Abstraction and Information Fusion
As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown,
1998) is the first historical example of a multi-document summarization system. It
tackles single events about a narrow domain (news articles about terrorism) and
produces a briefing merging relevant information about each event and how reports
by different news agencies have evolved over time. The whole thread of reports is
then presented, as illustrated in the following example of a \good" summary:
\In the afternoon of February 26, 1993, Reuters reported that a suspect
bomb killed at least five people in the World Trade Center. However,
Associated Press announced that exactly five people were killed in the
blast. Finally, Associated Press announced that Arab terrorists were
possibly responsible for the terrorist act."
Rather than working with raw text, SUMMONS reads a database previously
built by a template-based message understanding system. A full multi-document
14Available for download at http://www.summarization.com/mead/ .
summarizer is built by concatenating the two systems, first processing full text as
input and filling template slots , and then synthesizing a summary from the extracted
information. The architecture of SUMMONS consists of two major components: a
content planner that selects the information to include in the summary through
combination of the input templates, and a linguistic generator that selects the right
words to express the information in grammatical and coherent text. The latter
component was devised by adapting existing language generation tools, namely the
FUF/SURGE system15. Content planning, on the other hand, is made through
summary operators , a set of heuristic rules that perform operations like \change of
perspective", \contradiction", \refinement", etc. Some of these operations require
resolving con
icts , i.e., contradictory information among different sources or time
instants; others complete pieces of information that are included in some articles
and not in others, combining them into a single template. At the end, the linguis-
tic generator gathers all the combined information and uses connective phrases to
synthesize a summary.
While this framework seems promising when the domain is narrow enough so that
the templates can be designed by hand, a generalization for broader domains would
be problematic. This was improved later by McKeown et al. (1999) and Barzilay
et al. (1999), where the input is now a set of related documents in raw text, like
those retrieved by a standard search engine in response to a query. The system starts
by identifying themes , i.e., sets of similar text units (usually paragraphs). This is
formulated as a clustering problem. To compute a similarity measure between text
units, these are mapped to vectors of features, that include single words weighted
by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet
database and a database of semantic classes of verbs. For each pair of paragraphs, a
vector is computed that represents matches on the different features. Decision rules
that were learned from data are then used to classify each pair of text units either
assimilar ordissimilar ; this in turn feeds a subsequent algorithm that places the
most related paragraphs in the same theme .
Once themes are identified, the system enters its second stage: information fu-
sion. The goal is to decide which sentences of a theme should be included in the
summary. Rather than just picking a sentence that is a group representative, the
authors propose an algorithm which compares and intersects predicate argument
structures of the phrases within each theme to determine which are repeated often
enough to be included in the summary. This is done as follows: first, sentences are
parsed through Collins' statistical parser (Collins, 1999) and converted into depen-
dency trees , which allows capturing the predicate-argument structure and identify
functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence
representation.
The comparison algorithm then traverses these dependency trees recursively,
adding identical nodes to the output tree. Once full phrases (a verb with at least
two constituents) are found, they are marked to be included in the summary. If two
15FUF, SURGE, and other tools developed by the Columbia NLP group are available at
http://www1.cs.columbia.edu/nlp/tools.cgi .
andKan1998].Wematchtwoverbsthatsharethesame
semanticclassinthisclassiÔ¨Åcation.
Inadditiontotheaboveprimitivefeaturesthatallcom-
paresingleitemsfromeachtextunit,weusecompositefea-
turesthatcombinepairsofprimitivefeatures.Ourcompos-
itefeaturesimposeparticularconstraintsontheorderofthe
twoelementsinthepair,onthemaximumdistancebetween
thetwoelements,andonthesyntacticclassesthatthetwo
elementscomefrom.Theycanvaryfromasimplecom-
bination(e.g.,‚Äútwotextunitsmustsharetwowordstobe
similar‚Äù)tocomplexcaseswithmanyconditions(e.g.,‚Äútwo
textunitsmusthavematchingnounphrasesthatappearin
thesameorderandwithrelativedifferenceinpositionno
morethanÔ¨Åve‚Äù).Inthismanner,wecaptureinformation
onhowsimilarlyrelatedelementsarespacedoutinthetwo
textunits,aswellassyntacticinformationonwordcombi-
nations.Matchesoncompositefeaturesindicatecombined
evidenceforthesimilarityofthetwounits.
Todeterminewhethertheunitsmatchoverall,weemploy
amachinelearningalgorithm[Cohen1996]thatinducesde-
cisionrulesusingthefeaturesthatreallymakeadifference.
Asetofpairsofunitsalreadymarkedassimilarornotbya
humanisusedfortrainingtheclassiÔ¨Åer.Wehavemanually
markedasetof8,225paragraphcomparisonsfromtheTDT
corpusfortrainingandevaluatingoursimilarityclassiÔ¨Åer.
Forcomparison,wealsouseanimplementationofthe
TF*IDFmethodwhichisstandardformatchingtextsinin-
formationretrieval.Wecomputethetotalfrequency(TF)of
wordsineachtextunitandthenumberofunitsinourtrain-
ingseteachwordappearsin(DF,ordocumentfrequency).
TheneachtextunitisrepresentedasavectorofTF*IDF
scores,calculatedas
TF(wordi)¬∑logTotalnumberofunits
DF(wordi)
Similaritybetweentextunitsismeasuredbythecosineof
theanglebetweenthecorrespondingtwovectors(i.e.,the
normalizedinnerproductofthetwovectors),andtheopti-
malvalueofathresholdforjudgingtwounitsassimilaris
computedfromthetrainingset.
Afterallpairwisesimilaritiesbetweentextunitshave
beencalculated,weutilizeaclusteringalgorithmtoiden-
tifythemes.Asaparagraphmaybelongtomultiplethemes,
moststandardclusteringalgorithms,whichpartitiontheir
inputset,arenotsuitableforourtask.Weuseagreedy,
one-passalgorithmthatÔ¨Årstconstructsgroupsfromthemost
similarparagraphs,seedingthegroupswiththefullycon-
nectedsubcomponentsofthegraphthatthesimilarityrela-
tionshipinducesoverthesetofparagraphs,andthenplaces
additionalparagraphswithinagroupifthefractionofthe
membersofthegrouptheyaresimilartoexceedsapreset
threshold.
LanguageGeneration
Givenagroupofsimilarparagraphs‚Äîatheme‚Äîtheprob-
lemistocreateaconciseandÔ¨Çuentfusionofinformationin
thistheme,reÔ¨Çectingfactscommontoallparagraphs.A
straightforwardmethodwouldbetopickarepresentativesubject
class: noun
27
class: cardinalbombing
class: nounMcVeigh with
class: prepositiondefinite: yescharge
class: verb voice :passive
polarity: + tense: past
Figure4:Dependencygrammarrepresentationofthesen-
tence‚ÄúMcVeigh,27,waschargedwiththebombing‚Äù.
sentencethatmeetssomecriteria(e.g.,athresholdnumber
ofcommoncontentwords).Inpractice,however,anyrepre-
sentativesentencewillusuallyincludeembeddedphrase(s)
containinginformationthatisnotcommontoallsentences
inthetheme.Furthermore,othersentencesinthethemeof-
tencontainadditionalinformationnotpresentedintherep-
resentativesentence.Ourapproach,therefore,usesinter-
sectionamongthemesentencestoidentifyphrasescommon
tomostparagraphsandthengeneratesanewsentencefrom
identiÔ¨Åedphrases.
IntersectionamongThemeSentences
Intersectioniscarriedoutinthecontentplanner,whichuses
aparserforinterpretingtheinputsentences,withournew
workfocusingonthecomparisonofphrases.Themesen-
tencesareÔ¨Årstrunthroughastatisticalparser[Collins1996]
andthen,inordertoidentifyfunctionalroles(e.g.,subject,
object),areconvertedtoadependencygrammarrepresenta-
tion[KittredgeandMel‚ÄôÀácuk1983],whichmakespredicate-
argumentstructureexplicit.
Wedevelopedarule-basedcomponenttoproducefunc-
tionalroles,whichtransformsthephrase-structureoutputof
Collins‚Äôparsertodependencygrammar;functionwords(de-
terminersandauxiliaries)areeliminatedfromthetreeand
correspondingsyntacticfeaturesareupdated.Anexample
ofathemesentenceanditsdependencygrammarrepresen-
tationareshowninFigure4.Eachnon-auxiliarywordinthe
sentencehasanodeintherepresentation,andthisnodeis
connectedtoitsdirectdependents.
Thecomparisonalgorithmstartswithallsubtreesrooted
atverbsfromtheinputdependencystructure,andtraverses
themrecursively:iftwonodesareidentical,theyareadded
totheoutputtree,andtheirchildrenarecompared.Once
afullphrase(verbwithatleasttwoconstituents)hasbeen
found,itisconÔ¨Årmedforinclusioninthesummary.
DifÔ¨Åcultiesarisewhentwonodesarenotidentical,butare
similar.Suchphrasesmaybeparaphrasesofeachotherand
stillconveyessentiallythesameinformation.Sincetheme
sentencesareaprioriclosesemantically,thissigniÔ¨ÅcantlyFigure 3: Dependency tree representing the sentence \McVeigh, 27, was charged
with the bombing" (extracted from (McKeown et al., 1999)).
phrases, rooted at some node, are not identical but yet similar, the hypothesis that
they are paraphrases of each other is considered; to take this into account, corpus-
driven paraphrasing rules are written to allow paraphrase intersection.16Once the
summary content (represented as predicate-argument structures) is decided, a gram-
matical text is generated by translating those structures into the arguments expected
by the FUF/SURGE language generation system.
3.2 Topic-driven Summarization and MMR
Carbonell and Goldstein (1998) made a major contribution to topic-driven sum-
marization by introducing the maximal marginal relevance (MMR) measure. The
idea is to combine query relevance with information novelty ; it may be applicable
in several tasks ranging from text retrieval to topic-driven summarization. MMR
simultaneously rewards relevant sentences and penalizes redundant ones by consid-
ering a linear combination of two similarity measures.
LetQbe a query or user profile and Ra ranked list of documents retrieved by
a search engine. Consider an incremental procedure that selects documents, one at
a time, and adds them to a set S. So letSbe the set of already selected documents
in a particular step, and RnSthe set of yet unselected documents in R. For each
candidate document Di2RnS, its marginal relevance MR(Di) is computed as:
MR(Di) :=Sim 1(Di;Q) (1 ) max
Dj2SSim 2(Di;Dj) (5)
whereis a parameter lying in [0 ;1] that controls the relative importance given
torelevance versus redundancy . Sim 1and Sim 2are two similarity measures; in the
16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al.,
1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization
in different syntactic categories (e.g. classifier vs. apposition), change in grammatical features
(active/passive, time, number, etc.), head omission, transformation from one POS to another,
using semantically related words (e.g. synonyms), etc.
experiments both were set to the standard cosine similarity traditionally used in the
vector space model, Sim 1(x;y) = Sim 2(x;y) =hx;yi
kxkkyk. The document achieving the
highest marginal relevance, DMMR = arg max Di2RnSMR(Di), is then selected, i.e.,
added toS, and the procedure continues until a maximum number of documents
are selected or a minimum relevance threshold is attained. Carbonell and Goldstein
(1998) found experimentally that choosing dynamically the value of turns out to be
more effective than keeping it fixed, namely starting with small values ( 0:3) to
give more emphasis to novelty, and then increasing it ( 0:7) to focus on the most
relevant documents. To perform summarization, documents can be first segmented
into sentences or paragraphs, and after a query is submitted, the MMR algorithm
can be applied followed by a selection of the top ranking passages, reordering them as
they appeared in the original documents, and presenting the result as the summary.
One of the attractive points in using MMR for summarization is its topic-oriented
feature, through its dependency on the query Q, which makes it particularly ap-
pealing to generate summaries according to a user profile : as the authors claim, \a
different user with different information needs may require a totally different sum-
mary of the same document." This assertion was not being taken into account by
previous multi-document summarization systems.
3.3 Graph Spreading Activation
Mani and Bloedorn (1997) describe an information extraction framework for sum-
marization, a graph-based method to find similarities and dissimilarities in pairs
of documents. Albeit no textual summary is generated, the summary content is
represented via entities ( concepts ) and relations that are displayed respectively as
nodes and edges of a graph. Rather than extracting sentences, they detect salient
regions of the graph via a spreading activation technique.17
This approach shares with the method described in Section 3.2 the property
of being topic-driven; there is an additional input that stands for the topic with
respect to which the summary is to be generated. The topic is represented through
a set of entry nodes in the graph. A document is represented as a graph as follows:
each node represents the occurrence of a single word (i.e., one word together with
its position in the text). Each node can have several kinds of links: adjacency
links (ADJ) to adjacent words in the text, SAME links to other occurrences of the
same word, and ALPHA links encoding semantic relationships captured through
Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of
adjacent nodes which belong to the same phrase, and NAME and COREF links
stand for co-referential name occurrences; Fig. 4 shows some of these links.
Once the graph is built, topic nodes are identified by stem comparison and be-
come the entry nodes . A search for semantically related text is then propagated from
these to the other nodes of the graph, in a process called spreading activation . Salient
17The name \spreading activation" is borrowed from a method used in information retrieval
(Salton and Buckley, 1988) to expand the search vocabulary.
18Seehttp://www.netowl.com .
1.39: Aoki, the Japanese ambassador, said in telephone calls to
Fujimori.Japanese broadcaster NHK that the rebels wanted to talk directly to
1.43: According to some estimates, only a couple hundred armed
followers remain.2.19 They are freeing us to show
not doing us any harm," said one woman.1.12:Police said they slipped through security
driving into the compound with champagne andbyposing aswaiters,
hors d‚Äôoeuvres.Associated Press Reuters
...
2.27:Although the MRTA gained support in its
early days in the mid-1980s as a Robin
give to the poor, it lost public sympathy after
turning increasingly to kidnapping, bombing
billion in damage to the country‚Äôs infrastructure
since 1980.and drug activities. 2.28:Guerilla conflicts in
Peru have cost at least 30,000 lives and $25...
close ties with Japan.
1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and
the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea,
...
...
...2.26:The MRTA called Tuesday‚Äôs
"Breaking The Silence."1.32: President Alberto Fujimori, who is of Japanese ancestry, has had
Germany, Austria and Venezuela.operation
Hood-style movement that robbed the rich to2.1: Peru rebels hold 200 in Japanese
2.3:LIMA - Heavily armed guerrillas threatened
from within the embassy residence.
2.13:The rebels said they had 400 to 500ambassador‚Äôs home
rebels.Peruvian government freed imprisoned fellow2.2:By Andrew Cawthorne
on Wednesday to kill at least 200 hostages,
Japanese ambassador‚Äôs residence unless themany of them high-ranking officials, held at the
was imprisoned in 1992. 2:14 They also called
for a review of Peru‚Äôs judicial system and direct
negotiations with the government beginning at
dawn on Wednesday.
...
...
2.22:The attack was a major blow to
Fujimori‚Äôs government, which had claimed
virtual victory in a 16-year war on communist
rebels belonging to the MRTA and the larger
and better-known Maoist Shining Path.1.2: Copyright Nando.net Copyright The Associated Press
1.3: *U.S. ambassador not among hostages in Peru
1.4:*Peru embassy attackers thought defeated in 1992
1.5:LIMA, Peru(Dec 18, 1996 05:54 a.m. EST) Well-armed guerillas
posing as waiters and carrying bottles of champagne sneaked into a
glittering reception and seized hundreds of diplomats and other guests.
1.6:As police ringed the building early Wednesday, an excited rebel
...
compound at the start of the reception, which was in honor of Japanese
Emperor Akihito‚Äôs birthday.
...
...
1.28:Many leaders of the Tupac Amaru which is smaller than Peru‚Äôs
was captured in June 1992 and is serving a life sentence, as is his2.4:"If they do not release our prisoners, we
will all die in here," a guerrilla from the
comrades in jail and said their highest priority
was release of Victor Polay, their leader whoMovement (MRTA) told a local radio stationCuban-inspired Tupac Amaru Revolutionary
soon after her release that she had been eating and drinking in an elegant
us: ‚ÄòDon‚Äôt lift your heads up or you will be shot."1.19:ADJ
hostages," a rebel who did not give his name told  a local radio station in
a telephone call from inside the compound."The guerillas stalked around the residence grounds threatening
lieutenant, Peter Cardenas. that they areTopic: Tupac Amaru
1.1:Rebels in Peru hold hundreds of hostages inside Japanese diplomatic
residence
threatened to start killing the hostages.
1.11:The group of 23 rebels, including three women entered the
1.17:Another guest, BBC correspondant Sally Bowen said in a report
marquee on the lawn when the explosions occurred....
1.25: "We are clear: the liberation of all our comrades, or we die with all the
1.30: Other top commanders conceded defeat July 1993. and surrendered in...
COREFCOREFSAME
Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay,ADJ
1.38:Fujimori whose sister was among the
anemergency cabinet meeting today.hostages released, calledALPHAADJ, the rebels threatened to kill the remaining
captives.1.24:Early Wednesday
Figure5:Textsoftworelatedarticles.Thetop5salientsentencescontainingcommonwordshavethesecommon
wordsinboldface;likewise,thetop5salientsentencescontaininguniquewordshavetheseuniquewordsinitalics.Figure 4: Examples of nodes and links in the graph for a particular sentence (detail
extracted from from a figure in (Mani and Bloedorn, 1997)).
words and phrases are initialized according to their TF-IDF score. The weight of
neighboring nodes depends on the node link traveled and is an exponentially decay-
ing function of the distance of the traversed path. Traveling within a sentence is
made cheaper than across sentence boundaries, which in turn is cheaper than across
paragraph boundaries. Given a pair of document graphs, common nodes are identi-
fied either by sharing the same stem or by being synonyms. Analogously, difference
nodes are those that are not common. For each sentence in both documents, two
scores are computed: one score that re
ects the presence of common nodes, which
is computed as the average weight of these nodes; and another score that computes
instead the average weights of difference nodes. Both scores are computed after
spreading activation . In the end, the sentences that have higher common and dif-
ferent scores are highlighted, the user being able to specify the maximal number of
common and different sentences to control the output. In the future, the authors
expect to use these structure to actually compose abstractive summaries, rather
than just highlighting pieces of text.
3.4 Centroid-based Summarization
Although clustering techniques were already being employed by McKeown et al.
(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000)
pioneered the use of cluster centroids to play a central role in summarization. A full
description of the centroid-based approach that underlies the MEAD system can
be found in (Radev et al., 2004); here we sketch brie
y the main points. Perhaps
the most appealing feature is the fact that it does not make use of any language
generation module, unlike most previous systems. All documents are modeled as
bags-of-words. The system is also easily scalable and domain-independent.
The first stage consists of topic detection, whose goal is to group together news
articles that describe the same event. To accomplish this task, an agglomerative
clustering algorithm is used that operates over the TF-IDF vector representations
of the documents, successively adding documents to clusters and recomputing the
centroids according to
cj=P
d2Cj~d
jCjj(6)
where cjis the centroid of the j-th cluster, Cjis the set of documents that belong
to that cluster, its cardinality being jCjj, and ~dis a \truncated version" of dthat
vanishes on those words whose TF-IDF scores are below a threshold. Centroids
can thus be regarded as pseudo-documents that include those words whose TF-
IDF scores are above a threshold in the documents that constitute the cluster. Each
event cluster is a collection of (typically 2 to 10) news articles from multiple sources,
chronologically ordered, describing an event as it develops over time.
The second stage uses the centroids to identify sentences in each cluster that
are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics
are defined that resemble the two summands in the MMR (see Section 3.2): cluster-
based relative utility (CBRU) and cross-sentence informational subsumption (CSIS).
The first accounts for how relevant a particular sentence is to the general topic of
the entire cluster; the second is a measure of redundancy among sentences. Unlike
MMR, these metrics are not query-dependent. Given one cluster Cof documents
segmented into nsentences, and a compression rate R, a sequence of nRsentences
are extracted in the same order as they appear in the original documents, which in
turn are ordered chronologically. The selection of the sentences is made by approx-
imating their CBRU and CSIS.19For each sentence si, three different features are
used:
Itscentroid value (Ci), defined as the sum of the centroid values of all the
words in the sentence,
Apositional value (Pi), that is used to make leading sentences more important.
LetCmaxbe the centroid value of the highest ranked sentence in the document.
ThenPi=n i+1
nCmax.
The first-sentence overlap (Fi), defined as the inner product between the word
occurrence vector of sentence iand that of the first sentence of the document.
The final score of each sentence is a combination of the three scores above minus a
redundancy penalty ( Rs) for each sentence that overlaps highly ranked sentences.
3.5 Multilingual Multi-document Summarization
Evans (2005) addresses the task of summarizing documents written in multiple
languages; this had already been sketched by Hovy and Lin (1999). Multilingual
summarization is still at an early stage, but this framework looks quite useful for
newswire applications that need to combine information from foreign news agen-
cies. Evans (2005) considered the scenario where there is a preferred language in
which the summary is to be written, and multiple documents in the preferred and
19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details).
in foreign languages are available. In their experiments, the preferred language was
English and the documents are news articles in English and Arabic. The rationale is
to summarize the English articles without discarding the information contained in
the Arabic documents. The IBM's statistical machine translation system is first ap-
plied to translate the Arabic documents to English. Then a search is made, for each
translated text unit, to see whether there is a similar sentence or not in the English
documents. If so, and if the sentence is found relevant enough to be included in the
summary, the similar English sentence is included instead of the Arabic-to-English
translation. This way, the final summary is more likely to be grammatical, since
machine translation is known to be far from perfect. On the other hand, the result
is also expected to have higher coverage than using just the English documents,
since the information contained in the Arabic documents can help to decide about
the relevance of each sentence. In order to measure similarity between sentences, a
tool named SimFinder20was employed: this is a tool for clustering text based on
similarity over a variety of lexical and syntactic features using a log-linear regression
model.
4 Other Approaches to Summarization
This section describes brie
y some unconventional approaches that, rather than
aiming to build full summarization systems, investigate some details that underlie
the summarization process, and that we conjecture to have a role to play in future
research on this field.
4.1 Short Summaries
Witbrock and Mittal (1999) claim that extractive summarization is not very pow-
erful in that the extracts are not concise enough when very short summaries are
required. They present a system that generated headline style summaries. The cor-
pus used in this work was newswire articles from Reuters and the Associated Press,
publicly available at the LDC21. The system learned statistical models of the rela-
tionship between source text units and headline units. It attempted to model both
the order and the likelihood of the appearance of tokens in the target documents.
Both the models, one for content selection and the other for surface realization were
used to co-constrain each other during the search in the summary generation task.
For content selection, the model learned a translation model between a docu-
ment and its summary (Brown et al., 1993). This model in the simplest case can be
thought as a mapping between a word in the document and the likelihood of some
word appearing in the summary. To simplify the model, the authors assumed that
the probability of a word appearing in a summary is independent of its structure.
This mapping boils down to the fact that the probability of a particular summary
20Seehttp://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder .
21Seehttp://ldc.upenn.edu .
candidate is the product of the probabilities of the summary content and that con-
tent being expressed using a particular structure.
The surface realization model used was a bigram model. Viterbi beam search
was used to efficiently find a near-optimal summary. The Markov assumption was
violated by using backtracking at every state to strongly discourage paths that
repeated terms, since bigrams that start repeating often seem to pathologically
overwhelm the search otherwise. To evaluate the system, the authors compared
its output against the actual headlines for a set of input newswire stories. Since
phrasing could not be compared, they compared the generated headlines against
the actual headlines, as well as the top ranked summary sentence of the story. Since
the system did not have a mechanism to determine the optimal length of a headline,
six headlines for each story were generated, ranging in length from 4 to 10 words
and they measured the term-overlap between each of the generated headlines and
the test. For headline length 4, there was 0.89 overlap in the headline and there was
0.91 overlap amongst the top scored sentence, indicating useful results.
4.2 Sentence Compression
Knight and Marcu (2000) introduced a statistical approach to sentence compression .
The authors believe that understanding the simpler task of compressing a sentence
may be a fruitful first step to later tackle the problems of single and multi-document
summarization.
Sentence compression is defined as follows: given a sequence of words W=
w1w2:::wnthat constitute a sentence, find a subsequence wi1wi2:::wik, with
1i1&lt; i2&lt; :::ikn, that is a compressed version ofW. Note that there
are 2npossibilities of output. Knight and Marcu (2000) considered two different
approaches: one that is inspired by the noisy-channel model , and another one based
ondecision trees . Due to its simplicity and elegance, we describe the first approach
here.
The noisy-channel model considers that one starts with a short summary s,
drawn according to the source model P(s), which is then subject to channel noise to
become the full sentence t, in a process guided by the channel model P(tjs). When
the stringtis observed, one wants to recover the original summary according to:
^s= arg max
sP(sjt) = arg max
sP(s)P(tjs): (7)
This model has the advantage of decoupling the goals of producing a short text that
looks grammatical (incorporated in the source model) and of preserving important
information (which is done through the channel model). In (Knight and Marcu,
2000), the source and channel models are simple models inspired by probabilistic
context-free grammars (PCFGs). The following probability mass functions are de-
fined over parse trees rather than strings: Ptree(s), the probability of a parse tree
that generates s, andPexpand tree(tjs), the probability that a small parse tree that
generatessisexpanded to a longer one that generates t.
The sentence tis first parsed by using Collins' parser (Collins, 1999). Then,
rather than computing Ptree(s) over all the 2nhypotheses for s, which would be
exponential in the sentence length, a shaded-forest structure is used: the parse
tree oftis traversed and the grammar (learned from the Penn Treebank22) is used
to check recursively which nodes may be removed from each production in order
to achieve another valid production. This algorithm allows to compute efficiently
Ptree(s) andPexpand tree(tjs) for all possible grammatical summariess. Conceptually,
the noisy channel model works the other way around: summaries are the original
strings that are expanded via expansion templates . Expansion operations have the
effect of decreasing the probability Pexpand tree(tjs). The probabilities Ptree(s) and
Pexpand tree(tjs) consist in the usual factorized expression for PCFGs times a bigram
distribution over the leaves of the tree (i.e. the words). In the end, the log probability
is (heuristically) divided by the length of the sentence sin order not to penalize
excessively longer sentences (this is done commonly in speech recognition).
More recently, Daum√© e III and Marcu (2002) extended this approach to document
compression by using rhetorical structure theory as in Marcu (1998a), where the
entire document is represented as a tree, hence allowing not only to compress relevant
sentences, but also to drop irrelevant ones. In this framework, Daum√© e III and Marcu
(2004) employed kernel methods to decide for each node in the tree whether or not
it should be kept.
4.3 Sequential document representation
We conclude this section by mentioning some recent work that concerns document
representation, with applications in summarization. In the bag-of-words representa-
tion (Salton et al., 1975) each document is represented as a sparse vector in a very
large Euclidean space, indexed by words in the vocabulary V. A well-known tech-
nique in information retrieval to capture word correlation is latent semantic indexing
(LSI), that aims to find a linear subspace of dimension kjVjwhere documents
may be approximately represented by their projections.
These classical approaches assume by convenience that Euclidean geometry is
a proper model for text documents. As an alternative, Gous (1999) and Hall and
Hofmann (2000) used the framework of information geometry (Amari and Nagaoka,
2001) to generalize LSI to the multinomial manifold , which can be identified with
theprobability simplex
Pn 1=(
x2RnjnX
i=1xi= 1; xi0 fori= 1;:::;n)
: (8)
Instead of finding a linear subspace, as in the Euclidean case, they learn a subman-
ifold of Pn 1. To illustrate this idea, Gous (1999) split a book (Machiavelli's The
Prince ) into several text blocks (its numbered pages), considered each page as a
point in PjVj 1, and projected data into a 2-dimensional submanifold. The result is
22Seehttp://www.cis.upenn.edu/ ~treebank/ .
the representation of the book as a sequential path in R2, tracking the evolution of
the subject matter of the book over the course of its pages (see Fig. 5). Inspired by
Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex-
tracted from (Gous, 1999)). The in
ection around page 85 re
ects a real change in
the subject matter, where the book shifts from political theory to a more biograph-
ical discourse.
this framework, Lebanon et al. (2007) suggested representing a document as a sim-
plicial curve (i.e. a curve in the probability simplex), yielding the locally weighted
bag-of-words (lowbow) model. According to this representation, a length-normalized
document is a functionx: [0;1]V!R+such that
X
wj2Vx(t;wj) = 1;for anyt2[0;1]. (9)
We can regard the document as a continuous signal, and x(t;wj) as expressing
the relevance of word wjat instantt. This generalizes both the pure sequential
representation and the (global) bag-of-words model. Let y= (y1;:::;yn)2Vnbe
an-length document. The pure sequential representation of yarises by defining
x=xseqwith:
xseq(t;wj) =1;ifwj=ydtne
0;ifwj6=ydtne;(10)
wheredaedenotes the smallest integer greater than a. The global bag-of-words
representation of xcorresponds to defining x=xbow, where
xbow(;wj) =Z1
0xseq(t;wj)dt; 2[0;1]; j= 1;:::;jVj: (11)
In this case, the curve degenerates into a single point in the simplex, which is
the maximum likelihood estimate of the multinomial parameters. An intermediate
representation arises by smoothing (10) via a function f;: [0;1]!R++, where
2[0;1] and2R++are respectively a location and a scale parameter. An
example of such a smoothing function is the truncated Gaussian defined in [0 ;1]
and normalized. This allows defining the lowbow representation at of then-lenght
document ( y1;:::;yn)2Vnas the function x: [0;1]V!R+such that:
x(;wj) =Z1
0xseq(t;wj)f;(t)dt: (12)
The scale of the smoothing function controls the amount of locality/globality in
the document representation (see Fig. 6): when !1 we recover the global bow
representation (11); when !0, we approach the pure sequential representation
(10).
Figure 6: The lowbow representation of a document with jVj= 3, for several values
of the scale parameter (extracted from (Lebanon, 2006)).
Representing a document as a simplicial curve allows us to characterize geomet-
rically several properties of the document. For example, the tangent vector field
along the curve describes sequential \topic trends" and their change; the curvature
measures the amount of wigglyness or deviation from a geodesic path. This prop-
erties can be useful for tasks like text segmentation or summarization; for example
plotting the velocity of the curve jj_x()jjalong time offers a visualization of the doc-
ument where local maxima tend to correspond to topic boundaries (see (Lebanon
et al., 2007) for more information).
5 Evaluation
Evaluating a summary is a difficult task because there does not exist an ideal sum-
mary for a given document or set of documents. From papers surveyed in the previ-
ous sections and elsewhere in literature, it has been found that agreement between
human summarizers is quite low, both for evaluating and generating summaries.
More than the form of the summary, it is difficult to evaluate the summary con-
tent. Another important problem in summary evaluation is the widespread use of
disparate metrics. The absence of a standard human or automatic evaluation met-
ric makes it very hard to compare different systems and establish a baseline. This
problem is not present in other NLP problems, like parsing. Besides this, manual
evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation
of summaries as in the DUC conferences would require over 3000 hours of human ef-
forts. Hence, an evaluation metric having high correlation with human scores would
obviate the process of manual evaluation. In this section, we would look at some im-
portant recent papers that have been able to create standards in the summarization
community.
5.1 Human and Automatic Evaluation
Lin and Hovy (2002) describe and compare various human and automatic metrics to
evaluate summaries. They focus on the evaluation procedure used in the Document
Understanding Conference 2001 (DUC-2001), where the Summary Evaluation En-
vironment (SEE) interface was used to support the human evaluation part. NIST
assessors in DUC-2001 compared manually written ideal summaries with summaries
generated automatically by summarization systems and baseline summaries. Each
text was decomposed into a list of units (sentences) and displayed in separate win-
dows in SEE. To measure the content of summaries, assessors stepped through each
model unit (MU) from the ideal summaries and marked all system units (SU) shar-
ing content with the current model unit, rating them with scores in the range 1  4
to specify that the marked system units express all(4),most (3),some (2) or hardly
any(1) of the content of the current model unit. Grammaticality, cohesion, and co-
herence were also rated similarly by the assessors. The weighted recall at threshold
t(wheretrange from 1 to 4) is then defined as
Recallt=Number of MUs marked at or above t
Number of MUs in the model summary: (13)
An interesting study is presented that shows how unstable the human markings
for overlapping units are. For multiple systems, the coverage scores assigned to the
same units were different by human assessors 18% of the time for the single document
task and 7:6% of the time for multi-document task. The authors also observe that
inter-human agreement is quite low in creating extracts from documents ( 40% for
single-documents and 29% for multi-documents). To overcome the instability of
human evaluations, they proposed using automatic metrics for summary evaluation.
Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001),
they outline an accumulative n-gram matching score (which they call NAMS),
NAMS = a 1NAM 1+ a2NAM 2+ a3NAM 3+ a4NAM 4; (14)
where the NAM nn-gram hit ratio is defined as:
# of matched n-grams between MU and S
total # ofn-grams in MU(15)
withSdenoting here the whole system summary, and where only content words
were used in forming the n-grams. Different configurations of a iwere tried; the
best correlation with human judgement (using Spearman's rank order correlation
coefficient) was achieved using a configuration giving 2 =3 weight to bigram matches
and 1=3 to unigrams matches with stemming done by the Porter stemmer.
5.2 ROUGE
Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist-
ing Evaluation (ROUGE)23that have become standards of automatic evaluation of
summaries.
In what follows, let R=fr1;:::;rmgbe a set of reference summaries , and letsbe
a summary generated automatically by some system. Let  n(d) be a binary vector
representing the n-grams contained in a document d; thei-th component i
n(d) is 1
if thei-thn-gram is contained in dand 0 otherwise. The metric ROUGE-N is an
n-gram recall based statistic that can be computed as follows:
ROUGE-N( s) =P
r2Rhn(r);n(s)iP
r2Rhn(r);n(r)i; (16)
whereh:;:idenotes the usual inner product of vectors. This measure is closely related
to BLEU which is a precision related measure. Unlike other measures previously
considered, ROUGE-N can be used for multiple reference summaries, which is quite
useful in practical situations. An alternative is taking the most similar summary in
the reference set,
ROUGE-N multi(s) = max
r2Rhn(r);n(s)i
hn(r);n(r)i: (17)
Another metric in (Lin, 2004) applies the concept of longest common subse-
quences24(LCS). The rationale is: the longer the LCS between two summary sen-
tences, the more similar they are. Let r1;:::;rube the reference sentences of the
documents in R, andsa candidate summary (considered as a concatenation of
sentences). The ROUGE-L is defined as an LCS based F-measure:
ROUGE-L( s) =(1 +fi2)RLCSPLCS
RLCS+fi2PLCS(18)
23Seehttp://openrouge.com/default.aspx .
24Asubsequence of a strings=s1:::snis a string of the form si1:::sinwhere 1 i1&lt;:::i nn.
whereRLCS(s) =Pu
i=1LCS (ri;s)Pu
i=1jrij,PLCS(s) =Pu
i=1LCS (ri;s)
jsj,jxjdenotes the length of
sentencex,LCS (x;y) denotes the length of the LCS between sentences xandy,
andfiis a (usually large) parameter to balance precision and recall. Notice that
the LCS function may be computed by a simple dynamic programming approach.
The metric (18) is further refined by including weights that penalize subsequence
matches that are not consecutive, yielding a new measure denoted ROUGE-W.
Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen
as a gappy version of ROUGE-N for n= 2 and is aptly called skip bigram . Let 	 2(d)
be a binary vector indexed by ordered pairs of words; the i-th component  i
2(d) is
1 if thei-th pair is a subsequence of dand 0 otherwise. The metric ROUGE-S is
computed as follows:
ROUGE-S( s) =(1 +fi2)RSPS
RS+fi2PS(19)
whereRS(s) =Pu
i=1h	2(ri);	2(s)iPu
i=1h	2(ri);	2(ri)iandPS(s) =Pu
i=1h	2(ri);	2(s)i
h	2(s);	2(s)i.
The various versions of ROUGE were evaluated by computing the correlation
coefficient between ROUGE scores and human judgement scores. ROUGE-2 per-
formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and
ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How-
ever, correlation achieved with human judgement for multi-document summarization
was not as high as single-document ones; improvement on this side of the paradigm
is an open research topic.
5.3 Information-theoretic Evaluation of Summaries
A very recent approach (Lin et al., 2006) proposes to use an information-theoretic
method to automatic evaluation of summaries. The central idea is to use a diver-
gence measure between a pair of probability distributions, in this case the Jensen-
Shannon divergence , where the first distribution is derived from an automatic sum-
mary and the second from a set of reference summaries. This approach has the
advantage of suiting both the single-document and the multi-document summariza-
tion scenarios.
LetD=fd1;:::;dngbe the set of documents to summarize (which is a singleton
set in the case of single-document summarization). Assume that a distribution
parameterized by Rgenerates reference summaries of the documents in D. The
task of summarization can be seen as that of estimating R. Analogously, assume
that every summarization system is governed by some distribution parameterized
byA. Then, we may define a good summarizer as one for which Ais close toR.
One information-theoretic measure between distributions that is adequate for this
is the KL divergence (Cover and Thomas, 1991),
KL(pAjjpR) =mX
i=1pA
ilogpA
i
pR
i: (20)
However, the KL divergence is unbounded and goes to infinity whenever pA
ivanishes
andpR
idoes not, which requires using some kind of smoothing when estimating the
distributions. Lin et al. (2006) claims that the measure used here should also be
symmetric ,25another thing that the KL divergence is not. Hence, they propose to
use the Jensen-Shannon divergence which is bounded and symmetric:26
JS(pAjjpR) =1
2KL(pAjjr) +1
2KL(pRjjr) =
=H(r) 1
2H(pA) 1
2H(pA); (21)
wherer=1
2pA+1
2pRis the average distribution .
To evaluate a summary SAgiven a reference summary SR, the authors propose
to use the negative JS divergence between the estimates of pAandpRgiven the
summaries,
score(SAjSR) = JS(p^Ajjp^R) (22)
The parameters are estimated via a posteriori maximization assuming a multi-
nomial generation model for each summary (which means that they are modeled as
bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial
family). So:
^A= arg max
Ap(SAjA)p(A); (23)
where (mbeing the number of distinct words, a1;:::;ambeing the word counts in
the summary, a0=Pm
i=1ai)
p(SAjA) = (a0+ 1)Qm
i=1 (ai+ 1)mY
i=1A;iai(24)
and
p(A) = (ff0)Qm
i=1 (ffi)mY
i=1A;iffi 1(25)
whereffiare hyper-parameters and ff0=Pm
i=1ffi. After some algebra, we get
^A;i=ai+ffi 1
a0+ff0 m(26)
which is similar to MLE with smoothing.27^Ris estimated analogously using the
reference summary SR. Not surprisingly, if we have more than one reference sum-
mary, the MAP estimation given all summaries equals MAP estimation given their
concatenation into a single summary.
25However, the authors do not give much support for this claim. In our view, there is no reason
to require symmetry.
Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfies
the axioms to be a squared metric , as shown by Endres and Schindelin (2003). It has also a plethora
of properties that are presented elsewhere, but this is out of scope of this survey.
27In particular if ffi= 1 it is just maximum likelihood estimation (MLE).
The authors experimented three automatic evaluation schemes (JS with smooth-
ing, JS without smoothing, and KL divergence) against manual evaluation; the best
performance was achieved by JS without smoothing. This is not surprising since, as
seen above, the JS divergence is bounded, unlike the KL divergence, and so it does
not require smoothing. Smoothing has the effect of pulling the two distributions
more close to the uniform distribution.</corps>
  <conclusion>The rate of information growth due to the World Wide Web has called for a need
to develop efficient and accurate summarization systems. Although research on
summarization started about 50 years ago, there is still a long trail to walk in
this field. Over time, attention has drifted from summarizing scientific articles to
news articles, electronic mail messages, advertisements, and blogs. Both abstractive
and extractive approaches have been attempted, depending on the application at
hand. Usually, abstractive summarization requires heavy machinery for language
generation and is difficult to replicate or extend to broader domains. In contrast,
simple extraction of sentences have produced satisfactory results in large-scale ap-
plications, specially in multi-document summarization. The recent popularity of
effective newswire summarization systems confirms this claim.
This survey emphasizes extractive approaches to summarization using statisti-
cal methods. A distinction has been made between single document and multi-
document summarization. Since a lot of interesting work is being done far from
the mainstream research in this field, we have chosen to include a brief discussion
on some methods that we found relevant to future research, even if they focus only
on small details related to a general summarization process and not on building an
entire summarization system.
Finally, some recent trends in automatic evaluation of summarization systems
have been surveyed. The low inter-annotator agreement figures observed during
manual evaluations suggest that the future of this research area heavily depends on
the ability to find efficient ways of automatically evaluating these systems and on
the development of measures that are objective enough to be commonly accepted
by the research community.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla-
tions of Mathematical Monographs) . Oxford University Press. [20]
Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable
summarizer with knowledge acquired from robust nlp techniques. In Mani, I.
and Maybury, M. T., editors, Advances in Automatic Text Summarization , pages
71{80. MIT Press. [4, 5]
Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization.
InProceedings ISTS'97 . [8]
Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the
context of multi-document summarization. In Proceedings of ACL '99 . [12, 13,
14, 16]
Baxendale, P. (1958). Machine-made index for technical literature - an experiment.
IBM Journal of Research Development , 2(4):354{361. [2, 3, 5]
Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The
mathematics of statistical machine translation: parameter estimation. Comput.
Linguist. , 19(2):263{311. [18]
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and
Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05:
Proceedings of the 22nd international conference on Machine learning , pages 89{
96, New York, NY, USA. ACM. [8]
Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking
for reordering documents and producing summaries. In Proceedings of SIGIR '98 ,
pages 335{336, New York, NY, USA. [12, 14, 15]
Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing .
PhD thesis, University of Pennsylvania. [13, 20]
Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov
models. In Proceedings of SIGIR '01 , pages 406{407, New York, NY, USA. [6]
Cover, T. and Thomas, J. (1991). Elements of Information Theory . Wiley. [25]
Daum√© e III, H. and Marcu, D. (2002). A noisy-channel model for document com-
pression. In Proceedings of the Conference of the Association of Computational
Linguistics (ACL 2002). [20]
Daum√© e III, H. and Marcu, D. (2004). A tree-position kernel for document compres-
sion. In Proceedings of DUC2004 . [20]
Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the
ACM , 16(2):264{285. [2, 3, 4]
Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu-
tions. IEEE Transactions on Information Theory , 49(7):1858{1860. [26]
Evans, D. K. (2005). Similarity-based multilingual multi-document summarization.
Technical Report CUCS-014-05, Columbia University. [12, 17]
Gous, A. (1999). Spherical subfamily models. [20, 21]
Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for
natural language processing and information retrieval. In Proc. 17th International
Conf. on Machine Learning , pages 351{358. Morgan Kaufmann, San Francisco,
CA. [20]
Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In
Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza-
tion, pages 81{94. MIT Press. [17]
Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen-
tence compression. In AAAI/IAAI , pages 703{710. [19]
Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer.
InProceedings SIGIR '95 , pages 68{73, New York, NY, USA. [4]
Lebanon, G. (2006). Sequential document representations and simplicial curves. In
Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence . [22]
Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words
framework for document representation. J. Mach. Learn. Res. , 8:2405{2441. [21,
22]
Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of
CIKM '99 , pages 55{62, New York, NY, USA. [5]
Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In
Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro-
ceedings of the ACL-04 Workshop , pages 74{81, Barcelona, Spain. [8, 23, 24,
25]
Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap-
proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL
'06, pages 463{470, Morristown, NJ, USA. [25, 26]
Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of
the Fifth conference on Applied natural language processing , pages 283{290, San
Francisco, CA, USA. [5]
Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In
Proceedings of the ACL-02 Workshop on Automatic Summarization , pages 45{51,
Morristown, NJ, USA. [23]
Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of
Research Development , 2(2):159{165. [2, 3, 6, 8]
Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search
and matching. In AAAI/IAAI , pages 622{628. [15, 16]
Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In
Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215 , pages
206{215, Montreal, Canada. [9, 10, 20]
Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of
natural language texts . PhD thesis, University of Toronto. Adviser-Graeme Hirst.
[10]
McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E.
(1999). Towards multidocument summarization by reformulation: Progress and
prospects. In AAAI/IAAI , pages 453{460. [11, 12, 13, 14, 16]
McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news
articles. In Proceedings of SIGIR '95 , pages 74{82, Seattle, Washington. [8, 11,
12]
Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM ,
38(11):39{41. [4, 9]
Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned
from the document understanding conference. In Proceedings of AAAI 2005,
Pittsburgh, USA . [7]
Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical
structure extraction. In Proceedings of Coling '94 , pages 344{348, Morristown,
NJ, USA. [9]
Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings
of the ACL'02 Workshop on Automatic Summarization , pages 1{8, Morristown,
NJ, USA. [7]
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for
automatic evaluation of machine translation. In Proceedings of ACL '02 , pages
311{318, Morristown, NJ, USA. [24]
Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue
on summarization. Computational Linguistics. , 28(4):399{408. [1, 2]
Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization
of multiple documents: sentence extraction, utility-based evaluation, and user
studies. In NAACL-ANLP 2000 Workshop on Automatic summarization , pages
21{30, Morristown, NJ, USA. [12, 16, 17]
Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza-
tion of multiple documents. Information Processing and Management 40 (2004) ,
40:919{938. [16, 17]
Radev, D. R. and McKeown, K. (1998). Generating natural language summaries
from multiple on-line sources. Computational Linguistics , 24(3):469{500. [12]
Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in
automatic information. In Proceedings of SIGIR '88 , pages 147{160, New York,
NY, USA. [15]
Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic
indexing. Communications of the ACM , 18:229{237. [20]
Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving
hard satisfiability problems. In AAAI , pages 440{446. [11]
Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document
summarization by combining RankNet and third-party sources. In Proceedings of
the EMNLP-CoNLL , pages 448{457. [7, 8]
Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract):
a statistical approach to generating highly condensed non-extractive summaries.
InProceedings of SIGIR '99 , pages 315{316, New York, NY, USA. [18]
</bibliographie>

  <preamble>Gonzalez_2018_Wisebe.pdf</preamble>
  <titre>WiSeBE: Window-Based Sentence
Boundary Evaluation</titre>
  <auteurs>
    <auteur>
      <name>Carlos-Emiliano Gonz√°lez-Gallardo</name>
      <mail>carlos-emiliano.gonzalez-gallardo@alumni.univ-avignon.fr</mail>
      <affiliation>LIA - Universit√© d‚ÄôAvignon et des Pays de Vaucluse, 339 chemin des Meinajaries,
84140 Avignon, France</affiliation>
    </auteur>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>LIA - Universit√© d‚ÄôAvignon et des Pays de Vaucluse, 339 chemin des Meinajaries,
84140 Avignon, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>Sentence Boundary Detection (SBD) has been a major
research topic since Automatic Speech Recognition transcripts have been
used for further Natural Language Processing tasks like Part of SpeechTagging, Question Answering or Automatic Summarization. But what
about evaluation? Do standard evaluation metrics like precision, recall,
F-score or classiÔ¨Åcation error; and more important, evaluating an auto-matic system against a unique reference is enough to conclude how well
a SBD system is performing given the Ô¨Ånal application of the transcript?
In this paper we propose Window-based Sentence Boundary Evaluation(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary
Detection systems based on multi-reference (dis)agreement. We evalu-
ate and compare the performance of diÔ¨Äerent SBD systems over a setof Youtube transcripts using WiSeBE and standard metrics. This dou-
ble evaluation gives an understanding of how WiSeBE is a more reliable
metric for the SBD task.</abstract>
  <introduction>
The goal of Automatic Speech Recognition (ASR) is to transform spoken data
into a written representation, thus enabling natural human-machine interaction[33] with further Natural Language Processing (NLP) tasks. Machine transla-
tion, question answering, semantic parsing, POS tagging, sentiment analysis and
automatic text summarization; originally developed to work with formal writ-ten texts, can be applied over the transcripts made by ASR systems [ 2,25,31].
However, before applying any of these NLP tasks a segmentation process called
Sentence Boundary Detection (SBD) should be performed over ASR transcriptsto reach a minimal syntactic information in the text.
To measure the performance of a SBD system, the automatically segmented
transcript is evaluated against a single reference normally done by a human. But
c/circlecopyrtSpringer Nature Switzerland AG 2018
I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119‚Äì131, 2018.https://doi.org/10.1007/978-3-030-04497-8
_10
given a transcript, does it exist a unique reference? Or, is it possible that the
same transcript could be segmented in Ô¨Åve diÔ¨Äerent ways by Ô¨Åve diÔ¨Äerent peoplein the same conditions? If so, which one is correct; and more important, how
to fairly evaluate the automatically segmented transcript? These questions are
the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), anew semi-supervised metric for evaluating SBD systems based on multi-reference
(dis)agreement.
The rest of this article is organized as follows. In Sect. 2we set the frame of
SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3,
followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE
and discussion over the method and alternative multi-reference evaluation ispresented in Sect. 5. Finally, Sect. 6concludes the paper.
</introduction>
  <corps>Sentence Boundary Detection (SBD) has been a major research topic science
ASR moved to more general domains as conversational speech [ 17,24,26]. Per-
formance of ASR systems has improved over the years with the inclusion andcombination of new Deep Neural Networks methods [ 5,9,33]. As a general rule,
the output of ASR systems lacks of any syntactic information such as capital-
ization and sentence boundaries, showing the interest of ASR systems to obtain
the correct sequence of words with almost no concern of the overall structure of
the document [ 8].
Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence
Boundary Disambiguation. This task aims to segment a formal written text into
well formed sentences based on the existent punctuation marks [ 11,19,20,29]. In
this context a sentence is deÔ¨Åned (for English) by the Cambridge Dictionary
1
as:
‚Äúa group of words, usually containing a verb, that expresses a thought in
the form of a statement, question, instruction, or exclamation and starts
with a capital letter when written‚Äù .
PMD carries certain complications, some given the ambiguity of punctuation
marks within a sentence. A period can denote an acronym, an abbreviation, the
end of the sentence or a combination of them as in the following example:
The U.S. president, Mr. Donald Trump, is meeting with the F.B.I.
director Christopher A. Wray next Thursday at 8 p.m.
However its diÔ¨Éculties, DPM proÔ¨Åts of morphological and lexical information
to achieve a correct sentence segmentation. By contrast, segmenting an ASRtranscript should be done without any (or almost any) lexical information and
a Ô¨Çurry deÔ¨Ånition of sentence.
1https://dictionary.cambridge.org/ .
The obvious division in spoken language may be considered speaker utter-
ances. However, in a normal conversation or even in a monologue, the way ideasare organized diÔ¨Äers largely from written text. This diÔ¨Äerences, added to disÔ¨Çu-
encies like revisions, repetitions, restarts, interruptions and hesitations make the
deÔ¨Ånition of a sentence unclear thus complicating the segmentation task [ 27].
Table 1exempliÔ¨Åes some of the diÔ¨Éculties that are present when working with
spoken language.
Table 1. Sentence Boundary Detection example
Speech transcript SBD applied to transcript
two two women can look out after
ak i ds ob a da sam a na n da
woman can so you can have a you
c a nh a v eam o t h e ra n daf a t h e rthat that still don‚Äôt do right with
the kid and you can have to men
t h a tc a ns oa sl o n ga st h el o v eeach other as long as they love
each other it doesn‚Äôt mattertwo // two women can look out
a f t e rak i ds ob a da sam a na n da
woman can // so you can have a
// you can have a mother and afather that // that still don‚Äôt do
right with the kid and you can
have to men that can // so aslong as the love each other // as
long as they love each other it
doesn‚Äôt matter//
Stolcke and Shriberg [ 26] considered a set of linguistic structures as segments
including the following list:
‚Äì Complete sentences
‚Äì Stand-alone sentences‚Äì DisÔ¨Çuent sentences aborted in mid-utterance‚Äì Interjections‚Äì Back-channel responses.
In [17], Meteer and Iyer divided speaker utterances into segments, consisting
each of a single independent clause. A segment was considered to begin either
at the beginning of an utterance, or after the end of the preceding segment. Any
dysÔ¨Çuency between the end of the previous segments and the begging of currentone was considered part of the current segments.
Rott and ÀáCerva [ 23] aimed to summarize news delivered orally segmenting the
transcripts into ‚Äúsomething that is similar to sentences‚Äù . They used a syntactic
analyzer to identify the phrases within the text.
A wide study focused in unbalanced data for the SBD task was performed
by Liu et al.[15]. During this study they followed the segmentation scheme pro-
posed by the Linguistic Data Consortium
2on the Simple Metadata Annotation
SpeciÔ¨Åcation V5.0 guideline (SimpleMDE V5.0) [ 27], dividing the transcripts in
Semantic Units.
2https://www.ldc.upenn.edu/ .
A Semantic Unit (SU) is considered to be an atomic element of the transcript
that manages to express a complete thought or idea on the part of the speaker[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text,
but other times (the most part of them) a SU corresponds to a phrase or a single
word.
SUs seem to be an inclusive conception of a segment, they embrace diÔ¨Äerent
previous segment deÔ¨Ånitions and are Ô¨Çexible enough to deal with the majority
of spoken language troubles. For these reasons we will adopt SUs as our segmentdeÔ¨Ånition.
2.1 Sentence Boundary Evaluation
SBD research has been focused on two diÔ¨Äerent aspects; features and methods.
Regarding the features, some work focused on acoustic elements like pauses
duration, fundamental frequencies, energy, rate of speech, volume change andspeaker turn [ 10,12,14].
The other kind of features used in SBD are textual or lexical features. They
rely on the transcript content to extract features like bag-of-word, POS tags orword embeddings [ 7,12,16,18,23,26,30]. Mixture of acoustic and lexical features
have also been explored [ 1,13,14,32], which is advantageous when both audio
signal and transcript are available.
With respect to the methods used for SBD, they mostly rely on statisti-
cal/neural machine translation [ 12,22], language models [ 8,15,18,26], conditional
random Ô¨Åelds [ 16,30] and deep neural networks [ 3,7,29].
Despite their diÔ¨Äerences in features and/or methodology, almost all previous
cited research share a common element; the evaluation methodology. Metrics as
Precision, Recall, F1-score, ClassiÔ¨Åcation Error Rate and Slot Error Rate (SER)are used to evaluate the proposed system against one reference. As discussed
in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial
to have a good segmentation. But comparing the output of a system against a
unique reference will provide a reliable score to decide if the system is good or
bad?
Bohac et al. [1] compared the human ability to punctuate recognized spon-
taneous speech. They asked 10 people (correctors) to punctuate about 30 min of
ASR transcripts in Czech. For an average of 3,962 words, the punctuation marksplaced by correctors varied between 557 and 801; this means a diÔ¨Äerence of 244
segments for the same transcript. Over all correctors, the absolute consensus for
period (.) was only 4.6% caused by the replacement of other punctuation marksas semicolons (;) and exclamation marks (!). These results are understandable if
we consider the diÔ¨Éculties presented previously in this section.
To our knowledge, the amount of studies that have tried to target the sentence
boundary evaluation with a multi-reference approach is very small. In [ 1], Bohac
et al. evaluated the overall punctuation accuracy for Czech in a straightforward
multi-reference framework. They considered a period (.) valid if at least Ô¨Åve oftheir 10 correctors agreed on its position.
Kol√°Àár and Lamel [ 13] considered two independent references to evaluate their
system and proposed two approaches. The Ô¨Åst one was to calculate the SER foreach of one the two available references and then compute their mean. They
found this approach to be very strict because for those boundaries where no
agreement between references existed, the system was going to be partially wrongeven the fact that it has correctly predicted the boundary. Their second app-
roach tried to moderate the number of unjust penalizations. For this case, a
classiÔ¨Åcation was considered incorrect only if it didn‚Äôt match either of the tworeferences.
These two examples exemplify the real need and some straightforward solu-
tions for multi-reference evaluation metrics. However, we think that it is possibleto consider in a more inclusive approach the similarities and diÔ¨Äerences that mul-
tiple references could provide into a sentence boundary evaluation protocol.
3 Window-Based Sentence Boundary Evaluation
Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic
multi-reference sentence boundary evaluation protocol which considers the per-formance of a candidate segmentation over a set of segmentation references and
the agreement between those references.
LetR={R
1,R2,...,R m}be the set of all available references given a tran-
script T={t1,t2,...,t n}, where tjis the jthword in the transcript; a reference
Riis deÔ¨Åned as a binary vector in terms of the existent SU boundaries in T.
Ri={b1,b2,...,b n} (1)
where
bj=/braceleftbigg
1i ftjis a boundary
0 otherwise
Given a transcript T, the candidate segmentation CTis deÔ¨Åned similar to Ri.
CT={b1,b2,...,b n} (2)
where
bj=/braceleftbigg
1i ftjis a boundary
0 otherwise
3.1 General Reference and Agreement Ratio
A General Reference ( RG) is then constructed to calculate the agreement ratio
between all references in. It is deÔ¨Åned by the boundary frequencies of each ref-erence R
i‚ààR.
RG={d1,d2,...,d n} (3)
where
dj=m/summationdisplay
i=1tij‚àÄtj‚ààT, d j=[ 0,m] (4)
The Agreement Ratio ( RGAR) is needed to get a numerical value of the dis-
tribution of SU boundaries over R. A value of RGARclose to 0 means a low
agreement between references in R, while RGAR= 1 means a perfect agreement
(‚àÄRi‚ààR,Ri=Ri+1|i=1,...,m ‚àí1) in R.
RGAR=RGPB
RGHA(5)
In the equation above, RGPBcorresponds to the ponderated common boundaries
ofRGandRGHAto its hypothetical maximum agreement.
RGPB=n/summationdisplay
j=1dj[dj‚â•2] (6)
RGHA=m√ó/summationdisplay
dj‚ààRG1[dj/negationslash= 0] (7)
3.2 Window-Boundaries Reference
In Sect. 2we discussed about how disÔ¨Çuencies complicate SU segmentation. In a
multi-reference environment this causes disagreement between references around
a same SU boundary. The way WiSeBE handle disagreements produced by dis-Ô¨Çuencies is with a Window-boundaries Reference ( R
W) deÔ¨Åned as:
RW={w1,w2,...,w p} (8)
where each window wkconsiders one or more boundaries djfromRGwith a
window separation limit equal to RW l.
wk={dj,dj+1,dj+2,...} (9)
3.3 Wi S e B E
WiSeBE is a normalized score dependent of (1) the performance of CToverRW
and (2) the agreement between all references in R. It is deÔ¨Åned as:
WiSeBE =F1RW√óRGARWiSeBE =[ 0,1] (10)
where F1RWcorresponds to the harmonic mean of precision and recall of CT
with respect to RW(Eq.11), while RGARis the agreement ratio deÔ¨Åned in ( 5).
RGARcan be interpreted as a scaling factor; a low value will penalize the overall
WiSeBE score given the low agreement between references. By contrast, for a
high agreement in R(RGAR‚âà1),WiSeBE ‚âàF1RW.
F1RW=2√óprecision RW√órecall RW
precision RW+recall RW(11)
precision RW=/summationtext
bj‚ààCT1[bj=1,bj‚ààw‚àÄw‚ààRW]
/summationtext
bj‚ààCT1[bj=1 ](12)
recall RW=/summationtext
wk‚ààRW1[wk/ownerb‚àÄb‚ààCT]
p(13)
Equations 12and13describe precision and recall of CTwith respect to RW.
Precision is the number of boundaries bjinside any window wkfromRWdivided
by the total number of boundaries bjinCT. Recall corresponds to the number
of windows wwith at least one boundary bdivided by the number of windows
winRW.
4 Evaluating with Wi S e B E
To exemplify the WiSeBE score we evaluated and compared the performance
of two diÔ¨Äerent SBD systems over a set of YouTube videos in a multi-reference
environment. The Ô¨Årst system (S1) employs a Convolutional Neural Network to
determine if the middle word of a sliding window corresponds to a SU bound-ary or not [ 6]. The second approach (S2) by contrast, introduces a bidirectional
Recurrent Neural Network model with attention mechanism for boundary detec-
tion [28].
In a Ô¨Årst glance we performed the evaluation of the systems against each
one of the references independently. Then, we implemented a multi-reference
evaluation with WiSeBE .
4.1 Dataset
We focused evaluation over a small but diversiÔ¨Åed dataset composed by 10
YouTube videos in the English language in the news context. The selected videos
cover diÔ¨Äerent topics like technology, human rights, terrorism and politics witha length variation between 2 and 10 min. To encourage the diversity of content
format we included newscasts, interviews, reports and round tables.
During the transcription phase we opted for a manual transcription process
because we observed that using transcripts from an ASR system will diÔ¨Écult
in a large degree the manual segmentation process. The number of words pertranscript oscilate between 271 and 1,602 with a total number of 8,080.
We gave clear instructions to three evaluators ( ref
1,ref 2,ref 3)o fh o ws e g -
mentation was needed to be perform, including the SU concept and how punctu-ation marks were going to be taken into account. Periods (.), question marks (?),
exclamation marks (!) and semicolons (;) were considered SU delimiters (bound-
aries) while colons (:) and commas (,) were considered as internal SU marks.The number of segments per transcript and reference can be seen in Table 2.A n
interesting remark is that ref
3assigns about 43% less boundaries than the mean
of the other two references.
Table 2. Manual dataset segmentation
Reference v1v2v3v4v5v6v7 v8v9v10Total
ref 1 384217115587109725516502
ref 2 33421614549892655120485
ref 3 232010639397630299281
4.2 Evaluation
We ran both systems (S1 &amp; S2) over the manually transcribed videos obtaining
the number of boundaries shown in Table 3. In general, it can be seen that S1
predicts 27% more segments than S2. This diÔ¨Äerence can aÔ¨Äect the performanceof S1, increasing its probabilities of false positives.
Table 3. Automatic dataset segmentation
System v1v2v3v4v5v6 v7 v8v9v10Total
S1 5338151354108106707111539
S2 38371211369286465313424
Table 4condenses the performance of both systems evaluated against each
one of the references independently. If we focus on F1 scores, performance of both
systems varies depending of the reference. For ref 1, S1 was better in 5 occasions
with respect of S2; S1 was better in 2 occasions only for ref 2;S 1o v e r p e r f o r m e d
S2 in 3 occasions concerning ref 3and in 4 occasions for mean (bold).
Also from Table 4we can observe that ref 1has a bigger similarity to S1 in
5 occasions compared to other two references, while ref 2is more similar to S2
in 7 transcripts (underline ).
After computing the mean F1 scores over the transcripts, it can be concluded
that in average S2 had a better performance segmenting the dataset compared
to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of
the dataset? Regardless all references have been considered, nor agreement or
disagreement between them has been taken into account.
All values related to the WiSeBE score are displayed in Table 5. The Agree-
ment Ratio ( RGAR) between references oscillates between 0.525 for v8and 0.767
forv5. The lower the RGAR, the bigger the penalization WiSeBE will give to
the Ô¨Ånal score. A good example is S2 for transcript v4where F1RWreaches a
value of 0.800, but after considering RGARtheWiSeBE score falls to 0.462.
It is feasible to think that if all references are taken into account at the same
time during evaluation ( F1RW), the score will be bigger compared to an average
of independent evaluations ( F1mean); however this is not always true. That is
the case of S1 in v10, which present a slight decrease for F1RWcompared to
F1mean.
Table 4. Independent multi-reference evaluation
Transcript System ref 1 ref 2 ref 3 Mean
P R F1 P R F1 P R F1 P R F1
v1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432
S2 0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480
v2 S1 0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578
S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549
v3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270
S2 0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325
v4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505
S2 0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735
v5 S1 0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592
S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499
v6 S1 0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443
S2 0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457
v7 S1 0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518
S2 0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539
v8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429
S2 0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487
v9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459
S2 0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541
v10 S1 0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582
S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487
Mean scores S1 ‚Äî 0.520 ‚Äî 0.510 ‚Äî 0.404 ‚Äî 0.481
S2 ‚Äî 0.543 ‚Äî 0.554 ‚Äî 0.433 ‚Äî 0.510
An important remark is the behavior of S1 and S2 concerning v6. If evalu-
ated without considering any (dis)agreement between references ( F1mean), S2
overperforms S1; this is inverted once the systems are evaluated with WiSeBE .</corps>
  <conclusion>In this paper we presented WiSeBE, a semi-automatic multi-reference sentence
boundary evaluation protocol based on the necessity of having a more reliable
way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric
which not only evaluates the performance of a system against all references, but
also takes into account the agreement between them. According to your point
of view, this inclusivity is very important given the diÔ¨Éculties that are presentwhen working with spoken language and the possible disagreements that a task
like SBD could provoke.
WiSeBE shows to be correlated with standard SBD metrics, however we
want to measure its correlation with extrinsic evaluations techniques like auto-
matic summarization and machine translation.</conclusion>
  <discussion>5.1 RG ARand Fleiss‚Äô Kappa correlation
In Sect. 3we described the WiSeBE score and how it relies on the RGARvalue
to scale the performance of CToverRW.RGARcan intuitively be consider an
agreement value over all elements of R. To test this hypothesis, we computed
the Pearson correlation coeÔ¨Écient ( PCC)[21] between RGARand the Fleiss‚Äô
Kappa [ 4] of each video in the dataset ( Œ∫R).
A linear correlation between RGARandŒ∫Rc a nb eo b s e r v e di nT a b l e 6. This
is conÔ¨Årmed by a PCC value equal to 0 .890, which means a very strong positive
linear correlation between them.
5.2 F1mean vs. Wi S e B E
Results form Table 5may give an idea that WiSeBE is just an scaled F1mean.
While it is true that they show a linear correlation, WiSeBE may produce a
Table 5. Wi Se B E evaluation
Transcript System F1mean F1RW RGAR Wi Se B E
v1 S1 0.432 0.495 0.691 0.342
S2 0.480 0.513 0.354
v2 S1 0.578 0.659 0.688 0.453
S2 0.549 0.595 0.409
v3 S1 0.270 0.303 0.684 0.207
S2 0.325 0.400 0.274
v4 S1 0.505 0.593 0.578 0.342
S2 0.735 0.800 0.462
v5 S1 0.592 0.614 0.767 0.471
S2 0.499 0.500 0.383
v6 S1 0.443 0.550 0.541 0.298
S2 0.457 0.535 0.289
v7 S1 0.518 0.592 0.617 0.366
S2 0.539 0.606 0.374
v8 S1 0.429 0.494 0.525 0.259
S2 0.487 0.508 0.267
v9 S1 0.459 0.569 0.604 0.344
S2 0.541 0.667 0.403
v10 S1 0.582 0.581 0.619 0.359
S2 0.487 0.545 0.338
Mean scores S1 0.481 0.545 0.631 0.344
S2 0.510 0.567 0.355
Table 6. Agreement within dataset
Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10
RGAR0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619
Œ∫R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718
diÔ¨Äerent system ranking than F1meangiven the integral multi-reference principle
it follows. However, what we consider the most proÔ¨Åtable about WiSeBE is the
twofold inclusion of all available references it performs. First, the construction of
RWto provide a more inclusive reference against to whom be evaluated and then,
the computation of RGAR, which scales the result depending of the agreement
between references.</discussion>
  <bibliographie>1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog-
nized speech for web presentation of large audio archive. In: 2012 35th International
Conference on Telecommunications and Signal Processing (TSP), pp. 441‚Äì445.IEEE (2012)
2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over
a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134‚Äì138. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-41552-9
14
3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented
transcript based on word vector. In: LREC (2016)
4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.
76(5), 378 (1971)
5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net-
works. In: IEEE International Conference on Information Systems and Economic
Intelligence (2017)
6. Gonz√°lez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran-
scripts informativeness study: an approach based on automatic summarization. In:
Conf√©rence en Recherche d‚ÄôInformation et Applications (CORIA), Rennes, France,
May (2018)
7. Gonz√°lez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for
French with subword-level information vectors and convolutional neural networks.arXiv preprint arXiv:1802.04559 (2018)
8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.
In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium
ISCA Tutorial and Research Workshop (ITRW) (2000)
9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni-
tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),
82‚Äì97 (2012)
10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech
recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308‚Äì318
(2015)
11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com-
put. Linguist. 32(4), 485‚Äì525 (2006)
12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts
using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology
Workshop (SLT), pp. 433‚Äì440. IEEE (2016)
13. Kol√°Àár, J., Lamel, L.: Development and evaluation of automatic punctuation for
French and english speech-to-text. In: Thirteenth Annual Conference of the Inter-
national Speech Communication Association (2012)
14. Kol√°Àár, J.,ÀáSvec, J., Psutka, J.: Automatic punctuation annotation in Czech broad-
cast news speech. In: SPECOM 2004 (2004)
15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine
learning from imbalanced data for sentence boundary detection in speech. Comput.
Speech Lang. 20(4), 468‚Äì494 (2006)
16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran-
dom Ô¨Åelds. In: Proceedings of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing. pp. 177‚Äì186. Association for Computational Linguistics
(2010)
17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:
Conference on Empirical Methods in Natural Language Processing (1996)
18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg-
mentation of speech for automatic summarization. In: 2006 IEEE International
Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.
IEEE (2006)
19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro-
ceedings of the Fourth Conference on Applied Natural Language Processing, pp.
78‚Äì83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,USA (1994)
20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua-
tion. Comput. Linguist. 23(2), 241‚Äì267 (1997)
21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.
R. Soc. Lond. 58, 240‚Äì242 (1895)
22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical
phrase-based translation. In: Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014)
23. Rott, M., ÀáCerva, P.: Speech-to-text summarization using automatic phrase extrac-
tion from recognized text. In: Sojka, P., Hor√°k, A., KopeÀá cek, I., Pala, K. (eds.) TSD
2016. LNCS (LNAI), vol. 9924, pp. 101‚Äì108. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-45510-5
12
24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based
study. In: Proceedings of the Fourth International Conference on Spoken Language,1996. ICSLP 1996, vol. 3, pp. 1868‚Äì1871. IEEE (1996)
25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:
Proceedings of the sixth conference on Applied natural language processing, pp.84‚Äì89. Association for Computational Linguistics (2000)
26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational
speech. In: Proceedings of the Fourth International Conference on Spoken Lan-
guage, 1996. ICSLP 1996, vol. 2, pp. 1005‚Äì1008. IEEE (1996)
27. Strassel, S.: Simple metadata annotation speciÔ¨Åcation v5. 0, linguistic data consor-
tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE
V5.0.pdf
28. Tilk, O., Alum¬® ae, T.: Bidirectional recurrent neural network with attention mech-
anism for punctuation restoration. In: Interspeech 2016 (2016)
29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen-
tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704
(2017)
30. UeÔ¨Éng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation
prediction for spoken and written text. In: Interspeech, pp. 3097‚Äì3101 (2013)
31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disÔ¨Çuency removal for
improving spoken language translation. In: 2010 IEEE International Conference on
Acoustics Speech and Signal Processing (ICASSP), pp. 5214‚Äì5217. IEEE (2010)
32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network
approach for sentence boundary detection in broadcast news. In: Fifteenth Annual
Conference of the International Speech Communication Association (2014)
33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://
doi.org/10.1007/978-1-4471-5779</bibliographie>

  <preamble>Iria_Juan-Manuel_Gerardo.pdf</preamble>
  <titre>On the Development of the RST Spanish Treebank </titre>
  <auteurs>
    <auteur>
      <name>Iria da Cunha</name>
      <mail>iria.dacunha@upf.edu</mail>
      <affiliation>Institute for Applied
Linguistics (UPF), Spain Laboratoire Informatique
d‚ÄôAvignon (UAPV), France Instituto de Ingenier√≠a (UNAM),
Mexico
Instituto de Ingenier√≠a
(UNAM), Mexico Instituto de Ingenier√≠a (UNAM),</affiliation>
    </auteur>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>Institute for Applied
Linguistics (UPF), Spain Laboratoire Informatique
d‚ÄôAvignon (UAPV), France Instituto de Ingenier√≠a (UNAM),
Mexico
Instituto de Ingenier√≠a
(UNAM), Mexico Instituto de Ingenier√≠a (UNAM),</affiliation>
    </auteur>
    <auteur>
      <name>Gerardo Sierra</name>
      <mail>gsierram@iingen.unam</mail>
      <affiliation>Institute for Applied
Linguistics (UPF), Spain Laboratoire Informatique
d‚ÄôAvignon (UAPV), France Instituto de Ingenier√≠a (UNAM),
Mexico
Instituto de Ingenier√≠a
(UNAM), Mexico Instituto de Ingenier√≠a (UNAM),</affiliation>
    </auteur>
  </auteurs>
  <abstract>In this article we present the RST Spanish
Treebank, the first corpus annotated with
rhetorical relations for this language. We
describe the characteristics of the corpus,
the annotation criteria, the annotation
procedure, the inter-annotator agreement,
and other related aspects. Moreover, we
show the interface that we have developed
to carry out searches over the corpus‚Äô
annotated texts.</abstract>
  <introduction>
The Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988) is a language independent theory
based on the idea that a text can be segmented into
Elementary Discourse Units (EDUs) linked by
means of nucleus-satellite or multinuclear
rhetorical relations. In the first case, the satell ite
gives additional information about the other one,
the nucleus, on which it depends (ex. Result,
Condition, Elaboration or Concession). In the
second case, several elements, all nuclei, are
connected at the same level, that is, there are no
elements dependent on others and they all have the
same importance with regard to the intentions of
the author of the text (ex. Contrast, List, Joint o r
Sequence). The rhetorical analysis of a text by
means of RST includes 3 phases: segmentation,
detection of relations and building of hierarchical
rhetorical trees. For more information about RST
we recommend the original article of Mann and Thompson (1988), the web site of RST 1 and the
RST review by Taboada and Mann (2006a).
RST has been used to develop several
applications, like automatic summarization,
information extraction (IE), text generation,
question-answering, automatic translation, etc.
(Taboada and Mann, 2006b). Nevertheless, most of
these works have been developed for English,
German or Portuguese. This is due to the fact that
at present corpora annotated with RST relations are
available only for these languages (for English:
Carlson et al., 2002, Taboada and Renkema, 2008;
for German: Stede, 2004; for Portuguese: Pardo et
al., 2008) and there are automatic RST parsers for
two of them (for English: Marcu, 2000; for
Portuguese: Pardo et al., 2008) or automatic RST
segmenters (for English: Tofiloski et al., 2009).
Scientific community working on RST applied to
Spanish is very small. For example, Bouayad-Agha
et al. (2006) apply RST to text generation in
several languages, Spanish among them. Da Cunha
et al. (2007) develop a summarization system for
medical texts in Spanish based on RST. Da Cunha
and Iruskieta (2010) perform a contrastive analysis
of Spanish and Basque texts. Romera (2004)
analyzes coherence relations by means of RST in
spoken Spanish. Taboada (2004) applies RST to
analyze the resources used by speakers to elaborate
conversations in English and Spanish.
We consider that it is necessary to build a
Spanish corpus annotated by means of RST. This
corpus should be useful for the development of a
rhetorical parser for this language and several oth er
applications related to computational linguistics,
like those developed for other languages

1 http://www.sfu.ca/rst/index.html (automatic translation, automatic summarization,
IE, etc.). And that is what we pretend to achieve
with our work. We present the development of the
RST Spanish Treebank, the first Spanish corpus
annotated by means of RST.
In Section 2, we present the state of the art
about RST annotated corpora. In Section 3, we
explain the characteristics of the RST Spanish
Treebank. In Section 4, we show the search
interface we have developed. In Section 5, we
establish some conclusions and future work.
</introduction>
  <corps>The most known RST corpus is the RST Discourse
Treebank, for English (Carlson et al., 2002a,
2002b). It includes 385 texts of the journalistic
domain, extracted from the Penn Treebank
(Marcus et al., 1993), such as cultural reviews,
editorials, economy articles, etc. 347 texts are us ed
as a learning corpus and 38 texts are used as a tes t
corpus. It contains 176,389 words and 21,789
EDUs. 13.8% of the texts (that is, 53) were
annotated by two people with a list of 78 relations .
For annotation, the annotation tool RSTtool2
(O'Donnell, 2000) was used, with some
adaptations. The principal advantages of this
corpus stand on the high number of annotated texts
(for the moment it is the biggest RST corpus) and
the clarity of the annotation method (specified in
the annotation manual by Carlson and Marcu,
2001). However, some drawbacks remain. The
corpus is not free, it is not on-line and it only
includes texts of one domain (journalistic).
For English there is also the Discourse
Relations Reference Corpus (Taboada and
Renkema, 2008). This corpus includes 65 texts
(each one tagged by one annotator) of several types
and from several sources: 21 articles from the Wall
Street Journal extracted from the RST Discourse
Treebank, 30 movies and books‚Äô reviews extracted
from the epinions.com website, and 14 diverse
texts, including letters, webs, magazine articles,
newspaper editorials, etc. The tool used for
annotation was also the RSTtool. The advantages
of this corpus are that it is free and on-line, and  it
includes texts of several types and domains. The
disadvantages are that the amount of texts is not
very high, the annotation methodology is not

2 http://www.wagsoft.com/RSTTool/ specified and it does not include texts annotated b y
several people.
Another well-known corpus is the Potsdam
Commentary Corpus, for German (Stede, 2004;
Reitter and Stede, 2003). This corpus includes 173
texts on politics from the on-line newspaper
M√§rkische Allgemeine Zeitung. It contains 32,962
words and 2,195 sentences. It is annotated with
several data: morphology, syntax, rhetorical
structure, connectors, correference and informative
structure. Nevertheless, only a part of this corpus
(10 texts), which the authors name "core corpus",
is annotated with all this information. The texts
were annotated with the RSTtool. This corpus has
several advantages: it is annotated at different
levels (the annotation of connectors is especially
interesting); all the texts were annotated by two
people (with a previous RST training phase); it is
free for research purposes, and there is a tool for
searching over the corpus (although it is not
available on-line). The disadvantages are: the
genre and domain of all the texts are the same, the
methodology of annotation was quite intuitive
(without a manual or specific criteria) and the
inter-annotator agreement is not given.
For Portuguese, there are 2 corpora, built in
order to develop a rhetorical parser (Pardo et al.,
2008). The first one, the CorpusTCC (Pardo et al.,
2008), was used as learning corpus for detection of
linguistic patterns indicating rhetorical relations . It
contains 100 introduction sections of computer
science theses (53,000 words and 1,350 sentences).
To annotate the corpus a list of 32 rhetorical
relations was used. The annotation manual by
Carlson and Marcu (2001) was adapted to
Portuguese. The annotation tool was the ISI RST
Annotation Tool3, an extension of the RSTtool.
The advantages of this corpus are: it is free, it
contains an acceptable number of texts and words
and it follows a specific annotation methodology.
The disadvantage is: it only includes texts of one
genre and domain, only annotated by one person.
The second one, Rhetalho (Pardo and Seno,
2005), was used as reference corpus for the parser
evaluation. It contains 50 texts: 20 introduction
sections and 10 conclusion sections from computer
science scientific articles, and 20 texts from the on-
line newspaper Folha de S√£o Paulo (7 from the
Daily section, 7 from the World section and 6 from

3 http://www.isi.edu/~marcu/discourse/  the Science section). It includes approximately
5,000 words. The relations and the annotation tool
are the same as those used in the CorpusTCC. The
advantages of this corpus are that it is free, it w as
annotated by 2 people (they both were RST experts
and followed an annotation manual) and it contains
texts of several genres and domains. The main
disadvantage is the scarce amount of texts.
The Penn Discourse Treebank (Rashmi et al.,
2008)f for English includes texts annotated with
information related to discourse structure and
semantics (without a specific theoretical approach) .
Its advantages are: its big size (it contains 40,60 0
annotated discourse relations) allows to apply
machine learning, and the discourse annotations
are aligned with the syntactic constituency
annotations of the Penn Treebank. Its limitations
are: dependencies across relations are not marked,
it only includes texts of the journalistic domain,
and it is not free. Although there are several
corpora annotated with discourse relations, there i s
not a corpus of this type for Spanish.
3 The RST Spanish Treebank
As Sierra (2008) states, a corpus consists of a
compilation of a set of written and/or spoken texts
sharing some characteristics, created for certain
investigation purposes. According to Hovy (2010),
we use 7 core questions in corpus design, detailed
in the next subsections.
3.1 Selecting a Corpus
For the RST Spanish Treebank, we wanted to
include short texts (finally, the average is 197
words by text; the longest containing 1,051 words
and the shortest, 25) in order to get a best on-lin e
visualization of the RST trees. Moreover, in the
first stage of the project, we preferred to select
specialized texts of very different areas, although
in the future we plan to include also non-
specialized texts (ex. blogs, news, websites) in
order to guarantee the representativity of the
corpus. We did not find a pre-existing Spanish
corpus with these characteristics, so we decided to
build our own corpus. Following Cabr√© (1999), we
consider that a text is specialized if it is writte n by
a professional in a given domain. According to this
work, specialized texts can be divided in three
levels: high (both the author and the potential
reader of the text are specialists), average (the author of the text is a specialist, and the potenti al
reader of that text is a student or someone
interested in or possessing some prior knowledge
about the subject) and low (the author of the text is
a specialist, and the potential reader is the gener al
public). The RST Spanish Treebank includes
specialized texts of the three mentioned levels:
high (scientific articles, conference proceedings,
doctoral theses, etc.), average (textbooks) and low
(articles and reports from popular magazines,
associations‚Äô websites, etc.). The texts have been
divided in 9 domains (some of them including
subdivisions): Astrophysics, Earthquake
Engineering, Economy, Law, Linguistics (Applied
Linguistics, Language Acquisition, PLN,
Terminology), Mathematics (Primary Education,
Secondary Education, Scientific Articles),
Medicine (Administration of Health Services,
Oncology, Orthopedy), Psychology and Sexuality
(Clinical Perspective, Psychological Perspective).
The size of a corpus is also a polemic question.
If the corpus is developed for machine learning, it s
size will be enough when the application we want
to develop obtains acceptable percentages of
precision and recall (in the context of that
application). Nevertheless, if the corpus is built
with descriptive purposes, it is difficult to
determine the corpus size. In the case of a corpus
annotated with rhetorical relations, it is even mor e
difficult, because there are various factors
involved: EDUs, SPANs (that is, a group of related
EDUs), nuclearity and relations. In addition,
relations are multiple (we use 28). As Hovy (2010:
13) mentions, one of the most difficult phenomena
to annotate is the discourse structure. Our corpus
contains 52,746 words and 267 texts. Table 1
includes RST Spanish Treebank statistics in terms
of texts, words, sentences and EDUs.

 Texts Words Sentences EDUs
Learning corpus 183 41,555 1,759 2,655
Test corpus  84 11,191 497 694
Total corpus  267 52,746 2,256 3,349

Table 1: RST Spanish Treebank statistics

To increase the linear performance of a
statistical method, it is necessary that the traini ng
corpus size grows exponentially (Zhao et al.,
2010). However, the RST Spanish Treebank is not
designed only to use statistical methods; we think
it will be useful to employ symbolic or hybrid algorithms (combining symbolic and statistical
methods). Moreover, this corpus will be dynamic,
so we expect to have a bigger corpus in the future,
useful to apply machine learning methods.
If we measure the corpus size in terms of words
or texts, we can take as a reference the other RST
corpora. Nevertheless, as Sierra states (2008), it is
‚Äúabsurd‚Äù to try to build an exhaustive corpus
covering all the aspects of a language. On the
contrary, the linguist looks for the
representativeness of the texts, that is, tries to
create a sample of the studied language, selecting
examples which represent the linguistic reality, in
order to analyze them in a pertinent way. In this
sense and in the frame of this work, we consider
that the size will be adequate if the rhetorical tr ees
of the corpus include a representative number of
examples of rhetorical relations, at least 20
examples of each one (taking into account that the
corpus contains 3115 relations, we consider that
this quantity is acceptable; however, we expect to
have even more examples when the corpus grows).
Table 2 shows the number of examples of each
relation currently included into the RST Spanish
Treebank (N-S: nucleus-satellite relation; N-N:
multinuclear relation). As it can be observed, it
contains more than 20 examples of most  of the
relations. The exceptions are the nucleus-satellite
relations of Enablement, Evaluation, Summary,
Otherwise and  Unless, and the multinuclear
relations of Conjunction and Disjunction, because
it is not so usual to find these rhetorical relatio ns in
the language, in comparison with others. Hovy
(2010: 128) states that, given the lack of examples
in the corpus, there are 2 possible strategies: a) to
leave the corpus as it is, with few or no examples
of some cases (but the problem will be the lack of
training examples for machine learning systems),
or b) to add low-frequency examples artificially to
‚Äúenrich‚Äù the corpus (but the problem will be the
distortion of the native frequency distribution and
perhaps the confusion of machine learning
systems). In the current state of our project, we
have chosen the first option. We think that,
including specialized texts in a second stage, we
will get more examples of these less common
relations. If we carry out a more granulated
segmentation maybe we could obtain more
examples; however, we wanted to employ the
segmentation criteria used to develop the Spanish
RST discourse segmenter (da Cunha et al., 2011).
Quantity Relation Type
N¬∫ %
Elaboration N-S 765 24.56
Preparation N-S 475 15.25
Background N-S 204 6.55
Result N-S 193 6.20
Means N-S 175 5.62
List N-N 172 5.52
Joint N-N 160 5.14
Circumstance N-S 140 4.49
Purpose N-S 122 3.92
Interpretation N-S 88 2.83
Antithesis N-S 80 2.57
Cause N-S 77 2.47
Sequency N-N 74  2.38
Evidence N-S 59 1.89
Contrast N-N 58 1.86
Condition N-S 53 1.70
Concession N-S 50 1.61
Justification N-S 39 1.25
Solution N-S 32 1.03
Motivation N-S 28 0.90
Reformulation N-S 22 0.71
Otherwise N-S 3 0.10
Conjunction N-N 11 0.35
Evaluation N-S 11 0.35
Disjunction N-N 9 0.29
Summary N-S 8 0.26
Enablement  N-S 5 0.16
Unless N-S 2 0.06

Table 2: Rhetorical relations in RST Spanish Treeba nk

3.2 Instantiating the Theory
Our segmentation and annotation criteria are very
similar to the original ones used by Mann and
Thompson (1988) for English, and by da Cunha
and Iruskieta (2010) for Spanish. We also explore
the annotation manual for English by Carlon and
Marcu (2001). Though we use some of their
postulates, we think that their analysis is too
meticulous in some aspects. Because of this, we
consider that it is not adjusted to our interest,
which is the finding of the simplest and most
objective annotation method, orientated to the future development of a rhetorical parser for
Spanish. To sum up, our segmentation criteria are:

a) All the sentences of the text are segmented as
EDUs (we consider that a sentence is a textual
passage between a period and another period, a
semicolon, a question mark or an exclamation
point; texts‚Äô titles are also segmented). Exs.4

[√âstas son las razones fundamentales que motivaron
este trabajo.]
      [These are the fundamental reasons which motivated this
work.]
[Estudio de caso √∫nico sobre violencia conyugal]
      [Study of a case on conjugal violence]

b) Intra-sentence EDUs are segmented, using the
following criteria:

b1) An intra-sentence EDU has to include a finite
verb, an infinitive or a gerund. Ex.

[Siendo  una variante de la eliminaci√≥n Gaussiana,]
[posee  caracter√≠sticas did√°cticas ventajosas.]
      [Being  a variant of Gaussian elimination,] [it possesses
didactic profitable characteristics.]

b2) Subject/object subordinate clauses or
substantive sentences are not segmented. Ex.

[Se muestra que el modelo discreto en diferencias f initas
es convergente y que su realizaci√≥n se reduce a res olver
una sucesi√≥n de sistemas lineales tridiagonales .]
      [It appears that the discreet model in finite diffe rences is
convergent and that its accomplishment is to solve a
succession of tridiagonal linear systems .]

b3) Subordinate relative clauses are not segmented.
Ex.

[Durante el proceso, que utiliza solo aritm√©tica en tera ,
se obtiene el determinante de la matriz de coeficie ntes
del sistema, sin necesidad de c√°lculos adicionales. ]
       [During the process, which only uses entire arithmetic , the
determinant of the system coefficient matrix is obt ained,
without  additional calculations.]

b4) Elements in parentheses are only segmented if
they follow the criterion b1. Ex.
[Este a√±o se cumple el bicentenario del nacimiento de
Niels (Nicol√°s, en nuestro idioma) Henrik Abel.]
       [This year is the bicentenary of Niels's bir th (Nicol√°s, in
our language) Henrik Abel.]
b5) Embedded units are segmented by means of
the non-relation Same-Unit proposed by Carlon
and Marcu (2001). Figure 1 shows this structure.

[En d√©cadas precedentes se ha puesto de manifiesto,] [y
as√≠ lo han atestiguado muchos investigadores de la

4 Spanish examples were extracted from the corpus. E nglish
translations are ours. terminolog√≠a cient√≠fica serbia,] [una tendencia a
importar pr√©stamos del ingl√©s.]
        [In previous decades it has been shown,] [a nd it has been
testified by many researchers of the scientific Ser bian
terminology,] [a trend to import loanwords from Eng lish.]


Figure 1: Example of the non-relation Same-Unit
3.3 Designing the Interface
The annotation tool used in this work is the
RSTtool, since it is free and easy to use. Therefor e,
we preferred to use it instead of designing a new
one. Nevertheless, we have designed an on-line
interface to include the corpus and to carry out
searches over it (see Section 4).
3.4 Selecting and Training the Annotators
With regard to the corpus annotators, we have a
team of 10 people (last year Bachelor‚Äôs degree
students, Master‚Äôs degree students and PhDs)5.
Before the annotation, they took a RST course of 6
months (100 hours), where the segmentation and
annotation methodology used for the development
of the RST Spanish Treebank was explained.6 We
called this period "training phase". The course had
a theoretical and a practical part. In the theoreti cal
part, some criteria with regard to the 3 phases of
rhetorical analysis (segmentation, detection of
relations, and rhetorical trees building) were give n
to annotators. In the practical part, firstly, it w as
explained how to use the RSTtool. Secondly,
annotators extracted several texts from the web,
following their personal interests, as for example,
music, video games, cookery or art webs. They
segmented those texts, using the established
segmentation criteria. Once segmented, all the
doubts and problematic examples were discussed,
and they tried to get an agreement on the most
complicated cases. Thirdly, the relations were

5 We thank annotators (Adriana Valerio, Brenda Castr o,
Daniel Rodr√≠guez, Ita Cruz, Jessica M√©ndez, Josu√© C areaga,
Luis Cabrera, Marina Fomicheva and Paulina De La Ve ga)
and interface developers (Luis Cabrera and Juan Rol land).
6 This course was given in the framework of a last-y ear subject
in the Spanish Linguistics Degree at UNAM (Mexico C ity).  analyzed (using a given relations list) and, once
again, annotators discussed the difficult cases.
After the discussion, texts were re-annotated to
verify if the difficulties were solved. This proces s
was doubly interesting, since it helped to create
common criteria for the annotation of the final
corpus and to define the annotation criteria more
clearly and consensually, in order to include them
in the RST Spanish Treebank annotation manual.
Once annotators agreed on the most difficult cases,
we consider that the training phase finished.
3.5 Designing and Managing the Annotation
Procedure
We start from the following annotation definition:

Annotation (‚Äòtagging‚Äô) is the process of adding new
information into source material by humans
(annotators) or suitably trained machines. [...]. T he
addition process usually requires some sort of
mental decision that depends both on the source
material and on some theory or knowledge that the
annotator has internalized earlier. (Hovy, 2010: 6)

Exactly, after our annotators internalized the
theory and annotation criteria during the training
phase, the "annotation phase" of the final texts
included in the RST Spanish Treebank started. In
this phase, the annotation tasks were assigned to
annotators (the number of texts assigned to each
annotator was different, depending on their
availability). They were asked to carry out the
annotation individually and without questions
among them. We calculated that the average time
to carry out the annotation of one text was between
15 minutes and 1 hour. This time difference is due
to the fact that the corpus includes both short and
long texts. The annotation process is the following :
once a text is segmented, rhetorical relations
between EDUs are annotated. First, EDUs inside
the same sentence are annotated in a binary way.
Second, sentences inside the same paragraph are
linked. Finally, paragraphs are linked.
Hovy (2010) states that it is difficult to
determine if, for the same money (we add ‚Äúfor the
same time‚Äù), it is better to double-annotate less, or
to single-annotate more. As he explains, Dligach et
al. (2010) made an experiment with OntoNotes
(Pradhan et al., 2007) verb sense annotation. The
result was that, assuming the annotation is stable
(that is, inter-annotator agreement is high), it is
better to annotate more, even with only one
annotator. The problem with RST annotation is that there are so many categories to annotate, that
is very difficult to obtain a stable annotation.
Therefore, we consider it is necessary to have at
least some texts double-annotated (or even triple-
annotated), in order to have an adequate discourse
corpus. This is the reason why, following the RST
Discourse Treebank methodology, we use some
texts as learning corpus and some others (from the
Mathematics, Psychology and Sexuality domains)
as test corpus: 69% (183 texts) and 31% (84 texts),
respectively. The texts of the learning corpus were
annotated by 1 person, whereas the texts of the tes t
corpus were annotated by 2 people.
3.6 Validating Results
Da Cunha and Iruskieta (2010) measure inter-
annotator agreement by using the RST trees
comparison methodology by Marcu (2000). This
methodology evaluates the agreement on 4
elements (EDUs, SPANs, Nuclearity and
Relations), by means of precision and recall
measures (an annotation with regard to the other
one). Following this methodology, we have
measured inter-annotator agreement over the test
corpus. We employ an on-line automatic tool for
RST trees comparison, RSTeval (Mazeiro and
Pardo, 2009), where Marcu‚Äôs methodology has
been implemented (for 4 languages: English,
Portuguese, Spanish and Basque). We know that
there are some other ways to measure agreement,
such as Cohen's kappa (Cohen, 1960) or Fleiss's
kappa (Fleiss, 1971), for example. Nevertheless,
we consider that Marcu's methodology (2000) is
suitable to compare adequately 2 annotations of the
same original text, because it has been designed
specifically for this task.
For each trees pair from the test corpus,
precision and recall were measured separately.
Afterwards, all those individual results were put
together to obtain general results. Table 3 shows
global results for the 4 categories. The category
with more agreement was EDUs (recall: 91.04% /
precision: 87.20%), that is, segmentation. This
result was expected, since the segmentation criteri a
given to the annotators were quite precise and the
possibility of mistake was low. The lowest
agreement was obtained for the category Relations
(recall: 78.48% / precision: 76.81%). This result i s
lower than the other, but we think it is acceptable .
In the RST Discourse Treebank the trend was
similar to the one detected in our corpus: the highest agreement is obtained at the segmentation
level and the lowest at the relations level.


Category Precision Recall
EDUs 87.20% 91.04%
SPANs 86% 87.31%
Nuclearity 82.46% 84.66%
Relations 76.81% 78.48%

Table 3: Inter-annotator agreement


Precision and recall have not been calculated
with respect to a gold standard because it does not
exist for Spanish. Our future aim is to reach a
consensus on the annotation of the test corpus
(using an external "judge"), in order to establish a
set of texts considered as a preliminary gold
standard for this language. We consider that the
annotations have quality at present, because inter-
annotator agreement is quite high; however, this
consensus could solve the typical annotation
mistakes we have detected or some ambiguities.
We have analyzed the main discrepancy reasons
between annotators. With regard to the
segmentation, the main one was human mistake;
ex. segmenting EDUs without a verb (one
annotator segmented the following passage into 2
EDUs because she detected a Means relation, but
the second EDU does not include any verb):

[Adem√°s estudiamos el desarrollo de criterios para
determinar si un semigrupo dado tiene dicha propied ad ]
[mediante el estudio de desigualdades de curvatura-
dimensi√≥n. ]
      [We also study the development of tests in or der to
determine if a given semi group has this property] [by means
of curvature-dimension inequalities.]

The second reason was that in the manual some
aspects were not explained in detail. For example,
if a substantive sentence or a direct/object clause
(which must not be segmented, according to the
point b2) includes two coordinated clauses, these
must not be segmented either. Thus, we found
some erroneous segmentations. For example:

[Los hombres adultos tienen miedo de fracasar] [y n o
cumplir con el rol masculino de ser proveedores del
hogar y de proteger a su familia.]
      [Adult men are scared to fail] [and not to fu lfill the
masculine role of being the suppliers of the home a nd to
protect their family.]

This kind of mistakes allowed us to refine our
segmentation manual  a posteriori. In the future, we
will ask the test corpus annotators to make a new annotation of the texts, using the refined manual, in
order to check if the agreement increases, in the
same way as the RST Discourse Treebank.
With regard to rhetorical annotations, we
detected 2 main reasons of inter-annotator
disagreement. The first one was the ambiguity of
some relations and their corresponding connectors;
for example, Justification-Reason, Antithesis-
Concession or Circumstance-Means relations, like
in the following passage (in Spanish, ‚Äúal‚Äù may
indicate time or manner):

[Los ni√±os aprenden matem√°ticas] [al resolver
problemas.]
      [Children learn mathematics] [when solving pr oblems.]

The second one is due to differences between
annotators when determining nuclearity. For
example, in the following passage, one annotator
marked Background and the other one Elaboration:

[Qued√≥ un hueco en la pared de 60 x
1.20cm.]S_Background [Norma y Andr√©s quieren
colocar en el hueco una pecera. ]N_Background

[Qued√≥ un hueco en la pared de 60 x
1.20cm.]N_Elaboration [Norma y Andr√©s quieren
colocar en el hueco una pecera. ]S_Elaboration
      [A hole of 60 x 1.20 cm remained in the wall. ] [Norma and
Andr√©s want to place a fish tank in the hole.]

It is easier to solve segmentation disagreement
than relations disagreement, since in this case
annotator subjectivity is more evident; we must
consider how to refine our manual in this sense.
3.7 Delivering and Maintaining the Product
Hovy (2010) mentions some technical issues
regarding these points: licensing, distribution,
maintenance and updates. With regard to licensing
and distribution, the RST Spanish Treebank will be
free for research purposes. We have a data
manager responsible for maintenance and updates.
The description of the annotated corpus is also
a very important issue (Ide and Pustejovsky, 2010).
It is important to provide a high level description
of the corpus, including the theoretical framework,
the methodology (annotators, annotation manual
and tool, agreement, etc.), the means for resource
maintenance, the technical aspects, the project
leader, the contact, the team, etc. The RST Spanish
Treebank includes all this detailed information.
XML (with a DTD) has been used, in order the
corpus can be reused for several aplications. In th e
future, we plan to use the standard XCES. To know more about resources development,
linguistic annotation or inter-annotator agreement,
we recommend: Palmer et al. (on-line), Palmer and
Xue (2010), and Artstein and Poesio (2008).
4 The Search Interface of the RST
Spanish Treebank
The RST Spanish Treebank interface is freely
available on-line 7. It allows the visualization and
downloading of all the texts in txt format, with
their corresponding annotated trees in RSTtool
format (rs3), as well as in image format (jpg). Eac h
text includes its title, its reference, its web lin k (if
it is an on-line text) and its number of words. The
interface shows texts by areas and allows the user
to select a subcorpus (including individual files o r
folders containing several files). The selected
subcorpus can be saved on local disk (generating a
xml file) for future analyses.
The interface includes a statistical tool which
allows obtaining statistics of rhetorical relations  in
a subcorpus selected by the user. The RSTtool also
offers this option but it can be only used for one
text. We consider that it is more useful for the us er
to obtain statistics from various texts, in order t o
get significant statistical results. As the RSTtool ,
our tool allows to count the multinuclear relations
in two ways: a) one unit for each detected
multinuclear relation, and b) one unit for each
detected nucleus. If we use b), the statistics of t he
multinuclear relations of Table 2 are higher: List
(864), Joint (537), Sequence (289), Contrast (153),
Conjunction (28) and Disjunction (24).
We are developing another tool, aimed to
extract information from the annotated texts, which
we will soon include into the interface. This tool
will allow to the user to select a subcorpus and to
extract from it the EDUs corresponding to the
rhetorical relations selected, like a multidocument
specialized summarizer guided by user's interests.
The RST Spanish Treebank interface also
includes a screen which permits the users to send
their own annotated texts. Our aim is for the RST
Spanish Treebank to become a dynamic corpus, in
constant evolution, being increased with texts
annotated by users. This has a double advantage
since, on the one hand, the corpus will grow and,
on the other hand, users will profit from the

7 http://www.corpus.unam.mx/rst/ interface's applications, using their own
subcorpora. The only requirement is to use the
relations and the segmentation and annotation
criteria of our project. Once the texts are sent, t he
RST Spanish Treebank data manager will verify if
the annotation corresponds to these criteria.</corps>
  <conclusion>We think that this work means an important step
for the RST research in Spanish, and that the RST
Spanish Treebank will be useful to carry out
diverse researches about RST in this language,
from a descriptive point of view (ex. analysis of
texts from different domains or genres) and an
applied point of view (development of discourse
parsers and NLP applications, like automatic
summarization, automatic translation, IE, etc.).
For the moment the corpus' size is acceptable
and, though the percentage of double-annotated
texts is not very high, we think that having 10
annotators (using the same annotation manual)
avoids the bias of only one annotator. In addition,
the corpus includes texts of diverse domains and
genres, which provides us with a heterogeneous
Spanish corpus. Moreover, the corpus interface
that we have designed allows the user to select a
subcorpus and to analyze it statistically. In
addition, we think that it is essential to release a
free corpus, on-line and dynamic, that is, in
continuous growth. Nevertheless, we are conscious
that our work still has certain limitations, which we
will try to solve in the future. In the short term,  we
have 5 aims:

a) To add one more annotator for the test corpus
and to measure inter-annotator agreement.
b) To use more agreement measures, like kappa.
c) To reach a consensus on the annotation of the
test corpus, in order to establish a set of texts
considered as a preliminary gold standard.
d) To finish and to evaluate the IE tool.
e) To analyze the corpus to extract linguistic
patterns for the automatic relations detection.

In the long term, we consider other aims:

f) To increase the corpus, by adding non-
specialized texts, and new domains and genres.
g) To annotate all the texts by 3 people, to get a
representative gold-standard for Spanish (this aim
will depend on the funding of the project).</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>
Ron Artstein, and Massimo Poesio. 2008. Survey
Article: Inter-Coder Agreement for Computational
Linguistics. Computational Linguistics, 34(4):555-
596.
Nadjet Bouayad-Agha, Leo Wanner, and Daniel
Nicklass. 2006. Discourse structuring of dynamic
content. Procesamiento del lenguaje natural, 37:207 -
213.
M. Teresa Cabr√© (1999). La terminolog√≠a:
representaci√≥n y comunicaci√≥n. Barcelona: IULA-
UPF.
Lynn Carlson and Daniel Marcu. 2001. Discourse
Tagging Reference Manual. ISI Technical Report
ISITR-545. Los √Ångeles: University of Southern
California.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002a. RST Discourse Treebank.
Pennsylvania: Linguistic Data Consortium.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002b. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of the 2nd SIGDIAL
Workshop on Discourse and Dialogue, Eurospeech
2001.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 20(1):37-46
Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-
Moreno, Marina Lloberes, and Irene Castell√≥n. 2010.
Discourse Segmentation for Spanish based on
Shallow Parsing. Lecture Notes in Computer
Science, 6437:13-23.
Iria da Cunha, and Mikel Iruskieta. 2010. Comparing
rhetorical structures of different languages: The
influence of translation strategies. Discourse Stud ies,
12(5):563-598.
Iria da Cunha, Leo Wanner, and M. Teresa Cabr√©. 200 7.
Summarization of specialized discourse: The case of
medical articles in Spanish. Terminology, 13(2):249 -
286.
Dmitriy Dligach, Rodney D. Nielsen, and Martha
Palmer. 2010. To Annotate More Accurately or to
Annotate More. In Proceedings of the 4th Linguistic
Annotation Workshop (LAW-IV). 48th Annual
Meeting of the Association for Computational
Linguistics. Joseph L. Fleis. 1971. Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378-382.
Eduard Hovy. 2010. Annotation. A Tutorial. Presente d
at the 48th Annual Meeting of the Association for
Computational Linguistics.
Nancy Ide and Pustejovsky, J. (2010). What Does
Interoperability Mean, anyway? Toward an
Operational Definition of Interoperability. In
Proceedings of the Second International Conference
on Global Interoperability for Language Resources
(ICGL 2010).
William C. Mann, and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional
theory of text organization. Text, 8(3):243-281.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing Summarization. Massachusetts:
Institute of Technology.
Mitchell P. Marcus, Beatrice Santorini, Mary A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treenbank.
Computational Linguistics, 19(2):313-330.
Michael O‚ÄôDonnell. 2000. RSTTOOL 2.4 ‚Äì A markup
tool for rhetorical structure theory. In Proceeding s of
the International Natural Language Generation
Conference. 253-256.
Martha Palmer, and Nianwen Xue. 2010. Linguistic
Annotation. Handbook of Computational Linguistics
and Natural Language Processing.
Martha Palmer, Randee Tangi, Stephanie Strassel,
Christiane Fellbaum, and Eduard Hovy (on-line).
Historical Development and Future Directions in
Data Resource Development. MINDS report.
http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, Ralph Weischedel. 2007.
OntoNotes: A Unified Relational Semantic
Representation. In Proceedings of the First IEEE
International Conference on Semantic Computing
(ICSC-07).
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni
Miltsakaki, Livio Robaldo, Aravind Joshi, and
Bonnie Webber. 2008. The Penn Discourse Treebank
2.0. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008).
David Reitter, and Mandred Stede. 2003. Step by ste p:
underspecified markup in incremental rhetorical
analysis. In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora
(LINC-03).
Magdalena Romera. 2004. Discourse Functional Units:
The Expression of Coherence Relations in Spoken
Spanish. Munich: LINCOM.
Thiago Alexandre Salgueiro Pardo, and Lucia Helena
Machado Rino. 2001. A summary planner based on a
three-level discourse model. In Proceedings of
Natural Language Processing Pacific Rim
Symposium. 533-538.
Thiago Alexandre Salgueiro Pardo, Maria das Gra√ßas
Volpe Nunes, and Lucia Helena Machado Rino.
2008. DiZer: An Automatic Discourse Analyzer for
Brazilian Portuguese. Lecture Notes in Artificial
Intelligence, 3171:224-234.
Thiago Alexandre Salgueiro Pardo, and Eloize Rossi
Marques Seno. 2005. Rhetalho: um corpus de
refer√™ncia anotado retoricamente. In Anais do V
Encontro de Corpora. S√£o Carlos-SP, Brasil.
Gerardo Sierra. 2008. Dise√±o de corpus textuales pa ra
fines ling√º√≠sticos. In Proceedings of the IX Encuen tro
Internacional de Ling√º√≠stica en el Noroeste 2. 445-
462.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the Workshop on Discourse
Annotation, 42 nd  Meeting of the Association for
Computational Linguistics.
Maite Taboada. 2004. Building Coherence and
Cohesion: Task-Oriented Dialogue in English and
Spanish. Amsterdam/Philadelphia: John Benjamins.
Maite Taboada, and Jan Renkema. 2008. Discourse
Relations Reference Corpus [Corpus]. Simon Fraser
University and Tilburg University.
http://www.sfu.ca/rst/06tools/discourse_relations_c or
pus.html.
Maite Taboada, and William C. Mann. 2006a.
Rhetorical Structure Theory: Looking Back and
Moving Ahead. Discourse Studies, 8(3):423-459.
Maite Taboada, and William C. Mann. 2006b.
Applications of Rhetorical Structure Theory.
Discourse Studies, 8(4):567-588.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A Syntactic and Lexical-Based Discourse
Segmenter. In Proceedings of the 47th Annual
Meeting of the Association for Computational
Linguistics.
Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large
a Corpus Do We Need: Statistical Method Versus
Rule-based Method. In Proceedings of the Seventh conference on International Language Resources and
Evaluation (LREC'10).
 </bibliographie>

  <preamble>jing-cutepaste.pdf</preamble>
  <titre>Cut and Paste Based Text Summarization </titre>
  <auteurs>
    <auteur>
      <name>Hongyan Jing</name>
      <mail>hjing@cs.columbia.edu</mail>
      <affiliation>Department of Computer Science
Columbia University
New York, NY 10027, USA</affiliation>
    </auteur>
    <auteur>
      <name>Kathleen R. McKeown</name>
      <mail>kathy@cs.columbia.edu</mail>
      <affiliation>Department of Computer Science
Columbia University
New York, NY 10027, USA</affiliation>
    </auteur>
  </auteurs>
  <abstract>We present a cut and paste based text summa-
rizer, which uses operations derived from an anal-
ysis of human written abstracts. The summarizer
edits extracted sentences, using reduction to remove
inessential phrases and combination to merge re-
suiting phrases together as coherent sentences. Our
work includes a statistically based sentence decom-
position program that identifies where the phrases of
a summary originate in the original document, pro-
ducing an aligned corpus of summaries and articles
which we used to develop the summarizer.</abstract>
  <introduction>
There is a big gap between the summaries produced
by current automatic summarizers and the abstracts
written by human professionals. Certainly one fac-
tor contributing to this gap is that automatic sys-
tems can not always correctly identify the important
topics of an article. Another factor, however, which
has received little attention, is that automatic sum-
marizers have poor text generation techniques. Most
automatic summarizers rely on extracting key sen-
tences or paragraphs from an article to produce a
summary. Since the extracted sentences are discon-
nected in the original article, when they are strung
together, the resulting summary can be inconcise,
incoherent, and sometimes even misleading.
We present a cut and paste based text sum-
marization technique, aimed at reducing the gap
between automatically generated summaries and
human-written abstracts. Rather than focusing
on how to identify key sentences, as do other re-
searchers, we study how to generate the text of a
summary once key sentences have been extracted.
The main idea of cut and paste summarization
is to reuse the text in an article to generate the
summary. However, instead of simply extracting
sentences as current summarizers do, the cut and
paste system will "smooth" the extracted sentences
by editing them. Such edits mainly involve cutting
phrases and pasting them together in novel ways.
The key features of this work are:
(1) The identification of cutting and past- ing operations. We identified six operations that
can be used alone or together to transform extracted
sentences into sentences in human-written abstracts.
The operations were identified based on manual and
automatic comparison of human-written abstracts
and the original articles. Examples include sentence
reduction, sentence combination, syntactic transfor-
mation, and lexical paraphrasing.
(2) Development of an automatic system to
perform cut and paste operations. Two opera-
tions - sentence reduction and sentence combination
- are most effective in transforming extracted sen-
tences into summary sentences that are as concise
and coherent as in human-written abstracts. We
implemented a sentence reduction module that re-
moves extraneous phrases from extracted sentences,
and a sentence combination module that merges the
extracted sentences or the reduced forms resulting
from sentence reduction. Our sentence reduction
model determines what to cut based on multiple
sources of information, including syntactic knowl-
edge, context, and statistics learned from corpus
analysis. It improves the conciseness of extracted
sentences, making them concise and on target. Our
sentence combination module implements combina-
tion rules that were identified by observing examples
written by human professionals. It improves the co-
herence of extracted sentences.
(3) Decomposing human-wrltten summary
sentences. The cut and paste technique we propose
here is a new computational model which we based
on analysis of human-written abstracts. To do this
analysis, we developed an automatic system that can
match a phrase in a human-written abstract to the
corresponding phrase in the article, identifying its
most likely location. This decomposition program
allows us to analyze the construction of sentences
in a human-written abstract. Its results have been
used to train and test the sentence reduction and
sentence combination module.
In Section 2, we discuss the cut and paste tech-
nique in general, from both a professional and com-
putational perspective. We also describe the six cut
and paste operations. In Section 3, we describe the
system architecture. The major components of the
system, including sentence reduction, sentence com-
bination, decomposition, and sentence selection, are
described in Section 4. The evaluation results are
shown in Section 5. Related work is discussed in
Section 6. Finally, we conclude and discuss future
work. Document sentence: When it arrives some-
time next year in new TV sets, the V-chip will
give parents a new and potentially revolution-
ary device to block out programs they don't
want their children to see.
Summary sentence: The V-chip will give par-
ents a device to block out programs they don't
want their children to see.
</introduction>
  <corps>2.1 Related work in professional
summarizing
Professionals take two opposite positions on whether
a summary should be produced by cutting and past-
ing the original text. One school of scholars is
opposed; "(use) your own words... Do not keep
too close to the words before you", states an early
book on abstracting for American high school stu-
dents (Thurber, 1924). Another study, however,
shows that professional abstractors actually rely on
cutting and pasting to produce summaries: "Their
professional role tells abstractors to avoid inventing
anything. They follow the author as closely as pos-
sible and reintegrate the most important points of
a document in a shorter text" (Endres-Niggemeyer
et al., 1998). Some studies are somewhere in be-
tween: "summary language may or may not follow
that of author's" (Fidel, 1986). Other guidelines or
books on abstracting (ANSI, 1997; Cremmins, 1982)
do not discuss the issue.
Our cut and paste based summarization is a com-
putational model; we make no claim that humans
use the same cut and paste operations.
2.2 Cut and paste operations
We manually analyzed 30 articles and their corre-
sponding human-written summaries; the articles and
their summaries come from different domains ( 15
general news reports, 5 from the medical domain,
10 from the legal domain) and the summaries were
written by professionals from different organizations.
We found that reusing article text for summarization
is almost universal in the corpus we studied. We de-
fined six operations that can be used alone, sequen-
tially, or simultaneously to transform selected sen-
tences from an article into the corresponding sum-
mary sentences in its human-written abstract:
(1) sentence reduction
Remove extraneous phrases from a selected sen-
tence, as in the following example 1:
1 All the examples in this section were produced by human
professionals The deleted material can be at any granularity: a
word, a phrase, or a clause. Multiple components
can be removed.
(2) sentence combination
Merge material from several sentences. It can be
used together with sentence reduction, as illustrated
in the following example, which also uses paraphras-
ing:
Text Sentence 1: But it also raises serious
questions about the privacy of such highly
personal information wafting about the digital
world.
Text Sentence 2: The issue thus fits squarely
into the broader debate about privacy and se-
curity on the internet, whether it involves pro-
tecting credit card number or keeping children
from offensive information.
Summary sentence: But it also raises the is-
sue of privacy of such personal information
and this issue hits the head on the nail in the
broader debate about privacy and security on
the internet.
(3) syntactic transformation
In both sentence reduction and combination, syn-
tactic transformations may be involved. For exam-
ple, the position of the subject in a sentence may be
moved from the end to the front.
(4) lexical paraphrasing
Replace phrases with their paraphrases. For in-
stance, the summaries substituted point out with
note, and fits squarely into with a more picturesque
description hits the head on the nail in the previous
examples.
(5) generalization or specification
Replace phrases or clauses with more general or
specific descriptions. Examples of generalization
and specification include:
Generalization: "a proposed new law that
would require Web publishers to obtain
parental consent before collecting personal in-
formation from children" --+ "legislation to
protect children's privacy on-line"
Specification: "the White House's top drug
official" ~ "Gen. Barry R. McCaffrey, the
White House's top drug official"
p .....
,_e_ yr _', -
I
, Co-reference ~, I ......... I
,]WordNet'l
√±ed ie~ -~ Input~icle . I~
I Sentenc i extracti¬∞nl )
extracteikey sentenc~
Cut and paste based generation
[ Sentence reduction ]
I Sentence combinatio~
Output summary
Figure 1: System architecture
(6) reordering
Change the order of extracted sentences. For in-
stance, place an ending sentence in an article at the
beginning of an abstract.
In human-written abstracts, there are, of course,
sentences that are not based on cut and paste, but
completely written from scratch. We used our de-
composition program to automatically analyze 300
human-written abstracts, and found that 19% of sen-
tences in the abstracts were written from scratch.
There are also other cut and paste operations not
listed here due to their infrequent occurrence.
3 System architecture
The architecture of our cut and paste based text
summarization system is shown in Figure 1. Input
to the system is a single document from any domain.
In the first stage, extraction, key sentences in the ar-
ticle are identified, as in most current summarizers.
In the second stage, cut and paste based generation, a
sentence reduction module and a sentence combina-
tion module implement the operations we observed
in human-written abstracts.
The cut and paste based component receives as
input not only the extracted key sentences, but also
the original article. This component can be ported
to other single-document summarizers to serve as
the generation component, since most current sum-
marizers extract key sentences - exactly what the
extraction module in our system does.
Other resources and tools in the summarization
system include a corpus of articles and their human- written abstracts, the automatic decomposition pro-
gram, a syntactic parser, a co-reference resolution
system, the WordNet lexical database, and a large-
scale lexicon we combined from multiple resources.
The components in dotted lines are existing tools or
resources; all the others were developed by ourselves.
4 Major components
The main focus of our work is on decomposition of
summaries, sentence reduction, and sentence com-
bination. We also describe the sentence extraction
module, although it is not the main focus of our
work.
4.1 Decomposition of human-written
summary sentences
The decomposition program, see (Jing and McKe-
own, 1999) for details, is used to analyze the con-
struction of sentences in human-written abstracts.
The results from decomposition are used to build
the training and testing corpora for sentence reduc-
tion and sentence combination.
The decomposition program answers three ques-
tions about a sentence in a human-written abstract:
(1) Is the sentence constructed by cutting and past-
ing phrases from the input article? (2) If so, what
phrases in the sentence come from the original arti-
cle? (3) Where in the article do these phrases come
from?
We used a Hidden Markov Model (Baum, 1972)
solution to the decomposition problem. We first
mathematically formulated the problem, reducing it
to a problem of finding, for each word in a summary
Summary sentence:
(F0:S1 arthur b sackler vice president for law and public policy of time warner inc )
(FI:S-1 and) (F2:S0 a member of the direct marketing association told ) (F3:$2 the com-
munications subcommittee of the senate commerce committee ) (F4:S-1 that legislation )
(F5:Slto protect ) (F6:$4 children' s ) (F7:$4 privacy ) (F8:$4 online ) (F9:S0 could destroy
the spontaneous nature that makes the internet unique )
Source document sentences:
Sentence 0: a proposed new law that would require web publishers to obtain parental consent before
collecting personal information from children (F9 could destroy the spontaneous nature that
makes the internet unique ) (F2 a member of the direct marketing association told) a
senate panel thursday
Sentence 1:(F0 arthur b sackler vice president for law and public policy of time warner
inc ) said the association supported efforts (F5 to protect ) children online but he urged lawmakers
to find some middle ground that also allows for interactivity on the internet
Sentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such
as updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online
magazine sackler said in testimony to (F3 the communications subcommittee of the senate
commerce committee )
Sentence 4: the subcommittee is considering the (F6 children's ) (F8 online ) (F7 privacy )
protection act which was drafted on the recommendation of the federal trade commission
Figure 2: Sample output of the decomposition program
sentence, a document position that it most likely
comes from. The position of a word in a document
is uniquely identified by the position of the sentence
where the word appears, and the position of the word
within the sentence. Based on the observation of cut
and paste practice by humans, we produced a set of
general heuristic rules. Sample heuristic rules in-
clude: two adjacent words in a summary sentence
are most likely to come from two adjacent words in
the original document; adjacent words in a summary
sentence are not very likely to come from sentences
that are far apart in the original document. We
use these heuristic rules to create a Hidden Markov
Model. The Viterbi algorithm (Viterbi, 1967) is used
to efficiently find the most likely document position
for each word in the summary sentence.
Figure 2 shows sample output of the program.
For the given summary sentence, the program cor-
rectly identified that the sentence was combined
from four sentences in the input article. It also di-
vided the summary sentence into phrases and pin-
pointed the exact document origin of each phrase.
A phrase in the summary sentence is annotated as
(FNUM:SNUM actual-text), where FNUM is the se-
quential number of the phrase and SNUM is the
number of the document sentence where the phrase
comes from. SNUM = -1 means that the compo-
nent does not come from the original document. The
phrases in the document sentences are annotated as
(FNUM actual-text).
4.2 Sentence reduction
The task of the sentence reduction module, de-
scribed in detail in (Jing, 2000), is to remove extra-
neous phrases from extracted sentences. The goal of reduction is to "reduce without major loss"; that is,
we want to remove as many extraneous phrases as
possible from an extracted sentence so that it can be
concise, but without detracting from the main idea
that the sentence conveys. Ideally, we want to re-
move a phrase from an extracted sentence only if it
is irrelavant to the main topic.
Our reduction module makes decisions based on
multiple sources of knowledge:
(1) Grammar checking. In this step, we mark
which components of a sentence or a phrase are
obligatory to keep it grammatically correct. To do
this, we traverse the sentence parse tree, produced
by the English Slot Grammar(ESG) parser devel-
oped at IBM (McCord, 1990), in top-down order
and mark for each node in the parse tree, which
of its children are obligatory. The main source of
knowledge the system relies on in this step is a
large-scale, reusable lexicon we combined from mul-
tiple resources (Jing and McKeown, 1998). The lexi-
con contains subcategorizations for over 5,000 verbs.
This information is used to mark the obligatory ar-
guments of verb phrases.
(2) Context information. We use an extracted
sentence's local context in the article to decide which
components in the sentence are likely to be most
relevant to the main topic. We link the words in the
extracted sentence with words in its local context,
if they are repetitions, morphologically related, or
linked with each other in WordNet through certain
type of lexical relation, such as synonymy, antonymy,
or meronymy. Each word in the extracted sentence
gets an importance score, based on the number of
links it has with other words and the types of links.
Each phrase in the sentence is then assigned a score
Example 1:
Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give
parents a new and potentially revolutionary device to block out programs they don't
want their children to see.
Reduction program: The V-chip will give parents a new and potentially revolutionary device to
block out programs they don't want their children to see.
Professionals : The V-chip will give parents a device to block out programs they don't want
their children to see.
Example 2:
Original sentence : Sore and Hoffman's creation would allow broadcasters to insert
multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave
unexceptional portions o.f a show alone.
Reduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat-
ings into a show.
Professionals : Som and Hoffman's creation would allow broadcasters to insert multiple rat-
ings into a show.
Figure 3: Sample output of the
by adding up the scores of its children nodes in the
parse tree. This score indicates how important the
phrase is to the main topic in discussion.
(3) Corpus evidence. The program uses a cor-
pus of input articles and their corresponding reduced
forms in human-written abstracts to learn which
components of a sentence or a phrase can be re-
moved and how likely they are to be removed by
professionals. This corpus was created using the de-
composition program. We compute three types of
probabilities from this corpus: the probability that
a phrase is removed; the probability that a phrase is
reduced (i.e., the phrase is not removed as a whole,
but some components in the phrase are removed);
and the probability that a phrase is unchanged at
all (i.e., neither removed nor reduced). These cor-
pus probabilities help us capture human practice.
(4) Final decision. The final reduction decision
is based on the results from all the earlier steps. A
phrase is removed only if it is not grammatically
obligatory, not the focus of the local context (indi-
cated by a low context importance score), and has a
reasonable probability of being removed by humans.
The phrases we remove from an extracted sentence
include clauses, prepositional phrases, gerunds, and
to-infinitives.
The result of sentence reduction is a shortened
version of an extracted sentence 2. This shortened
text can be used directly as a summary, or it can
be fed to the sentence combination module to be
merged with other sentences.
Figure 3 shows two examples produced by the re-
duction program. The corresponding sentences in
human-written abstracts are also provided for com-
parison.
2It is actually also possible that the reduction program
decides no phrase in a sentence should be removed, thus the
result of reduction is the same as the input. sentence reduction program
4.3 Sentence combination
To build the combination module, we first manu-
ally analyzed a corpus of combination examples pro-
duced by human professionals, automatically cre-
ated by the decomposition program, and identified
a list of combination operations. Table 1 shows the
combination operations.
To implement a combination operation, we need
to do two things: decide when to use which com-
bination operation, and implement the combining
actions. To decide when to use which operation, we
analyzed examples by humans and manually wrote
a set of rules. Two simple rules are shown in Fig-
ure 4. Sample outputs using these two simple rules
are shown in Figure 5. We are currently exploring
using machine learning techniques to learn the com-
bination rules from our corpus.
The implementation of the combining actions in-
volves joining two parse trees, substituting a subtree
with another, or adding additional nodes. We im-
plemented these actions using a formalism based on
Tree Adjoining Grammar (Joshi, 1987).
4.4 Extraction Module
The extraction module is the front end of the sum-
marization system and its role is to extract key sen-
tences. Our method is primarily based on lexical re-
lations. First, we link words in a sentence with other
words in the article through repetitions, morpholog-
ical relations, or one of the lexical relations encoded
in WordNet, similar to step 2 in sentence reduction.
An importance score is computed for each word in a
sentence based on the number of lexical links it has
with other words, the type of links, and the direc-
tions of the links.
After assigning a score to each word in a sentence,
we then compute a score for a sentence by adding up
the scores for each word. This score is then normal-
Categories Combination Operations
Add descriptions or names for people or organizations
Aggregations
Substitute incoherent phrases
Substitute phrases with more general or specific information add description (see Figure 5)
add name
extract common subjects or objects (see Figure 5)
change one sentence to a clause
add connectives (e.g., and or while)
add punctuations (e.g., ";")
substitute dangling anaphora
substitute dangling noun phrases
substitute adverbs (e.g., here)
remove connectives
substitute with more general information
substitute with more specific information
Mixed operations combination of any of above operations (see Figure 2)
Table 1: Combination operations
Rule 1:
IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip-
tion of the person or the organization exists somewhere in the original article but is missing in the
summary))
THEN" replace the phrase with the full name plus the full description
Rule 2:
IF: ((two sentences are close to each other in the original article) and (their subjects refer to the
same entity) and (at least one of the sentences is the reduced form resulting from sentence reduc-
tion))
THEN: merge the two sentences by removing the subject in the second sentence, and then com-
bining it with the first sentence using connective "and".
Figure 4: Sample sentence combination rules
ized over the number of words a sentence contains.
The sentences with high scores are considered im-
portant.
The extraction system selects sentences based on
the importance computed as above, as well as other
indicators, including sentence positions, cue phrases,
and tf*idf scores.
5 Evaluation
Our evaluation includes separate evaluations of each
module and the final evaluations of the overall sys-
tem.
We evaluated the decomposition program by two
experiments, described in (Jing and McKeown,
1999). In the first experiment, we selected 50
human-written abstracts, consisting of 305 sentences
in total. A human subject then read the decomposi-
tion results of these sentences to judge whether they
are correct. 93.8% of the sentences were correctly
decomposed. In the second experiment, we tested
the system in a summary alignment task. We ran
the decomposition program to identify the source
document sentences that were used to construct the
sentences in human-written abstracts. Human sub-
jects were also asked to select the document sen-
tences that are semantlc-equivalent to the sentences in the abstracts. We compared the set of sentences
identified by the program with the set of sentences
selected by the majority of human subjects, which is
used as the gold standard in the computation of pre-
cision and recall. The program achieved an average
81.5% precision, 78.5% recall, and 79.1% f-measure
for 10 documents. The average performance of 14
human judges is 88.8% precision, 84.4% recall, and
85.7% f-measure. Recently, we have also tested the
system on legal documents (the headnotes used by
Westlaw company), and the program works well on
those documents too.
The evaluation of sentence reduction (see (Jing,
2000) for details) used a corpus of 500 sentences and
their reduced forms in human-written abstracts. 400
sentences were used to compute corpus probabili-
ties and 100 sentences were used for testing. The
results show that 81.3% of the reduction decisions
made by the system agreed with those of humans.
The humans reduced the length of the 500 sentences
by 44.2% on average, and the system reduced the
length of the 100 test sentences by 32.7%.
The evaluation of sentence combination module
is not as straightforward as that of decomposition
or reduction since combination happens later in the
pipeline and it depends on the output from prior
Example 1: add descriptions or names for people or organization
Original document sentences:
"We're trying to prove that there are big benefits to the patients by involving them more deeply in
their treatment", said Paul Clayton, Chairman of the Department dealing with comput-
erized medical information at Columbia.
"The economic payoff from breaking into health care records is a lot less than for
banks", said Clayton at Columbia.
Combined sentence:
"The economic payoff from breaking into health care records is a lot less than for banks", said Paul
Clayton, Chairman of the Department dealing with computerized medical information at Columbia.
Professional: (the same)
Example 2: extract common subjects
Original document sentences:
The new measure is an echo of the original bad idea, blurred just enough to cloud prospects
both for enforcement and for court review.
Unlike the 1996 act, this one applies only to commercial Web sites - thus sidestepping
1996 objections to the burden such regulations would pose for museums, libraries and freewheeling
conversation deemed "indecent" by somebody somewhere.
The new version also replaces the vague "indecency" standard, to which the court objected,
with the better-defined one of material ruled "harmful to minors."
Combined sentences:
The new measure is an echo of the original bad idea.
The new version applies only to commercial web sites and replaces the vague "indecency" standard
with the better-defined one of material ruled "harmful to minors."
Professional:
While the new law replaces the "indecency" standard with "harmful to minors" and now only
applies to commercial Web sites, the "new measure is an echo of the original bad idea."
Figure 5: Sample output of the sentence combination program
modules. To evaluate just the combination compo-
nent, we assume that the system makes the same
reduction decision as humans and the co-reference
system has a perfect performance. This involves
manual tagging of some examples to prepare for the
evaluation; this preparation is in progress. The eval-
uation of sentence combination will focus on the ac-
cessment of combination rules.
The overM1 system evMuation includes both in-
trinsic and extrinsic evaluation. In the intrinsic evM-
uation, we asked human subjects to compare the
quality of extraction-based summaries and their re-
vised versions produced by our sentence reduction
and combination modules. We selected 20 docu-
ments; three different automatic summarizers were
used to generate a summary for each document, pro-
ducing 60 summaries in total. These summaries
are all extraction-based. We then ran our sentence
reduction and sentence combination system to re-
vise the summaries, producing a revised version for
each summary. We presented human subjects with
the full documents, the extraction-based summaries,
and their revised versions, and asked them to com-
pare the extraction-based summaries and their re-
vised versions. The human subjects were asked to
score the conciseness of the summaries (extraction-
based or revised) based on a scale from 0 to 10 -
the higher the score, the more concise a summary is. They were also asked to score the coherence of the
summaries based on a scale from 0 to 10. On aver-
age, the extraction-based summaries have a score of
4.2 for conciseness, while the revised summaries have
a score of 7.9 (an improvement of 88%). The average
improvement for the three systems are 78%, 105%,
and 88% respectively. The revised summaries are
on average 41% shorter than the original extraction-
based summaries. For summary coherence, the aver-
age score for the extraction-based summaries is 3.9,
while the average score for the revised summaries is
6.1 (an improvement of 56%). The average improve-
ment for the three systems are 69%, 57%, and 53%
respectively.
We are preparing a task-based evaluation, in
which we will use the data from the Summariza-
tion EvMuation Conference (Mani et al., 1998) and
compare how our revised summaries can influence
humans' performance in tasks like text categoriza-
tion and ad-hoc retrieval.
6 Related work
(Mani et al., 1999) addressed the problem of revising
summaries to improve their quality. They suggested
three types of operations: elimination, aggregation,
and smoothing. The goal of the elimination opera-
tion is similar to that of the sentence reduction op-
eration in our system. The difference is that while
elimination always removes parentheticals, sentence-
initial PPs and certain adverbial phrases for every
extracted sentence, our sentence reduction module
aims to make reduction decisions according to each
case and removes a sentence component only if it
considers it appropriate to do so. The goal of the
aggregation operation and the smoothing operation
is similar to that of the sentence combination op-
eration in our system. However, the combination
operations and combination rules that we derived
from corpus analysis are significantly different from
those used in the above system, which mostly came
from operations in traditional natural language gen-
eration.</corps>
  <conclusion>This paper presents a novel architecture for text
summarization using cut and paste techniques ob-
served in human-written abstracts. In order to auto-
matically analyze a large quantity of human-written
abstracts, we developed a decomposition program.
The automatic decomposition allows us to build
large corpora for studying sentence reduction and
sentence combination, which are two effective op-
erations in cut and paste. We developed a sentence
reduction module that makes reduction decisions us-
ing multiple sources of knowledge. We also investi-
gated possible sentence combination operations and
implemented the combination module. A sentence
extraction module was developed and used as the
front end of the summarization system.
We are preparing the task-based evaluation of the
overall system. We also plan to evaluate the porta-
bility of the system by testing it on another corpus.
We will also extend the system to query-based sum-
marization and investigate whether the system can
be modified for multiple document summarization.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>
ANSI. 1997. Guidelines for abstracts. Technical Re-
port Z39.14-1997, NISO Press, Bethesda, Mary-
land.
L. Baum. 1972. An inequality and associated max-
imization technique in statistical estimation of probabilistic functions of a markov process. In-
equalities, (3):1-8.
Edward T. Cremmins. 1982. The Art of Abstracting.
ISI Press, Philadelphia.
Brigitte Endres-Niggemeyer, Kai Haseloh, Jens
Mfiller, Simone Peist, Irene Santini de Sigel,
Alexander Sigel, Elisabeth Wansorra, Jan
Wheeler, and Brfinja Wollny. 1998. Summarizing
Information. Springer, Berlin.
Raya Fidel. 1986. Writing abstracts for free-text
searching. Journal of Documentation, 42(1):11-
21, March.
Hongyan Jing and Kathleen R. McKeown. 1998.
Combining multiple, large-scale resources in a
reusable lexicon for natural language generation.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and the
17th International Conference on Computational
Linguistics, volume 1, pages 607-613, Universit6
de Montreal, Quebec, Canada, August.
Hongyan Jing and Kathleen R. McKeown. 1999.
The decomposition of human-written summary
sentences. In Proceedings of the P2nd In-
ternational ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval(SIGIR'99), pages 129-136, University of
Berkeley, CA, August.
Hongyan Jing. 2000. Sentence reduction for au-
tomatic text summarization. In Proceedings of
ANLP 2000.
Aravind.K. Joshi. 1987. Introduction to tree-
adjoining grammars. In A. Manaster-Ramis, ed-
itor, Mathematics of Language. John Benjamins,
Amsterdam.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Obrst, Therese Firmin, Michael
Chrzanowski, and Beth Sundheim. 1998. The
TIPSTER SUMMAC text summarization eval-
uation final report. Technical Report MTR
98W0000138, The MITRE Corporation.
Inderjeet Mani, Barbara Gates, and Erie Bloedorn.
1999. Improving summaries by revising them. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics(A CL '99),
pages 558-565, University of Maryland, Mary-
land, June.
Michael MeCord, 1990. English Slot Grammar.
IBM.
Samuel Thurber, editor. 1924. Prgcis Writing for
American Schools. The Atlantic Monthly Press,
INC., Boston.
A.J. Viterbi. 1967. Error bounds for convolution
codes and an asymptotically optimal decoding al-
gorithm. IEEE Transactions on Information The-
ory, 13:260-269.</bibliographie>

  <preamble>kessler94715.pdf</preamble>
  <titre>Extraction of terminology in the Ô¨Åeld of
construction</titre>
  <auteurs>
    <auteur>
      <name>R√©my Kessler</name>
      <mail>remy.kessler@univ-ubs.fr</mail>
      <affiliation>Universit√© Bretagne Sud
CNRS 6074A
56017 Vannes,France</affiliation>
    </auteur>
    <auteur>
      <name>Nicolas B√©chet</name>
      <mail>nicolas.bechet@irisa.fr</mail>
      <affiliation>Universit√© Bretagne Sud
CNRS 6074A
56017 Vannes,France</affiliation>
    </auteur>
    <auteur>
      <name>Giuseppe Berio</name>
      <mail>giuseppe.berio@univ-ubs.fr</mail>
      <affiliation>Universit√© Bretagne Sud
CNRS 6074A
56017 Vannes,France</affiliation>
    </auteur>
  </auteurs>
  <abstract>We describe a corpus analysis method to extract
terminology from a collection of technical speciÔ¨Åcations in the
Ô¨Åeld of construction. Using statistics and word n-grams analysis,
we extract the terminology of the domain and then perform
pruning steps with linguistic patterns and internet queries to
improve the quality of the Ô¨Ånal terminology. Results are evaluated
by using a manual evaluation carried out by 6 experts in the Ô¨Åeld.</abstract>
  <introduction>
The current era is increasingly inÔ¨Çuenced by the prominence
of smart data and mobile applications. The work presented
in this paper has been carried out in one industrial project
(VOCAGEN) aiming at automating the production of struc-
tured data from human machine dialogues. SpeciÔ¨Åcally, the
targeted application drives dialogues with people working in
a construction area for populating a database reporting key
data extracted from those dialogues. This application requires
complex processing for both transcripting speeches but also for
driving dialogues. The Ô¨Årst process is required for good speech
recognition in a noisy environment. The second processing is
required because the database needs to be populated with both
right and complete data; indeed, people tend to apply a broad
(colloquial) vocabulary and the transcripted words need to be
used for Ô¨Ålling in the corresponding data. Additionally, if some
data populate the database, additional data may be required
for completeness, thus the dialogue should enable to get those
additional data (e.g. if the word ‚Äùroom‚Äù is recognised and used
to populate the database, the location of the room must also
be got; this can be done by driving the dialogue).
The application provides people with ‚Äúhand-free‚Äù device,
enabling a complete, quick and standardized reporting. First
usages of this application will be oriented to reporting failures
and problems in constructions.
The two processing steps mentioned above require on the
one side a ‚Äúlanguage model‚Äù (for transcripting the sentences)
and on the other side a ‚Äúknowledge model‚Äù for driving thedialogue and correctly understanding the meaning of the word.
The knowledge model is mainly an ontology of the domain (in
this case, the construction domain) providing the standardized
concepts and their relationships. As well-known, building such
knowledge models needs time and is costly; one of the earlier
questions raised by our industrial partners has been about
‚Äúhow to build, as automaticaly as possible, such a knowledge
model‚Äù. This question is closely related to the interest of
quickly adapting the application to other domains (than the
construction one) for reaching new markets. We developed a
complete methodology and system for partially answering the
question, focusing on how to extract a relevant terminology
from a collection of technical speciÔ¨Åcations.
The rest of the paper is organized as follow. Section II
present context of the project. Related work are reviewed in
Section III. Section IV presents collected resources and some
statistics about them. Section V describes the methodology de-
veloped for extracting relevant terms from collected resources.
The details about the evaluation are presented in Section VI-A
and results obtained, are given in Section VI-B.
</introduction>
  <corps>Figure 1 presents the context of this work in VOCA-
GEN project. Our industrial partner Script&amp;Go1develop an
application for the construction management dedicated to
touch devices and wishes to set up an oral dialogue module
to facilitate on construction site seizure. The second industrial
partner (Tykomz) develops a vocal recognition suite based on
toolkit sphynx 4 [1]. This toolkit includes hierarchical agglom-
erative clustering methods using well-known measures such as
BIC and CLR and provides elementary tools, such as segment
and cluster generators, decoder and model trainers. Fitting
those elementary tools together is an easy way of developing
a speciÔ¨Åc diarization system. To work, it is necessary to build
a model of knowledge, i.e. a model describing the expressions
that must be recognized by the program. To improve the
performance of the system, this knowledge model must be
1http://www.scriptandgo.com/en/
Fig. 1. Ô¨Ågure describing the context of the project
powered by a domain-speciÔ¨Åc vocabulary. For example, in the
sentence ‚Äùthere is a stain of paint in the kitchen‚Äù, the system
must understand that it is a stain of paint and that the kitchen is
a room. To our knowledge, there is no ontology or taxonomy
speciÔ¨Åc to the construction industry in French. A version is
under development by [2] but ontology is in English and very
generic. We therefore choose to extract useful knowledge from
textual data, and then, in a second step, to organize them.
III. R ELATED WORKS
The goal of ontology learning (OL) is to build knowledge
models from text. OL use NLP knowledge extraction tools
to extract terminology (concepts) and links between them
(relationships). The main approaches found in the literature are
rule-based systems, statistical or learning based approaches.
The reference in the Ô¨Åeld of rule-based systems was de-
veloped by [3]. General Architecture for Text Engineering
(GATE) is a Java collection of tools initially developed at the
University of ShefÔ¨Åeld since 1995. An alternative is offered by
the existing semi-automatic ontology learning text2onto [4].
More recently, [5] developed UIMA, a system that can be
positioned as an alternative to GATE. Amongst other things,
UIMA makes possible to generate rules from a collection of
annotated documents. Exit, introduced by [6] is an iterative
approach that Ô¨Ånds the terms in an incremental way.
[7] with TERMINAE is certainly the oldest statistic ap-
proach. Developed for French and based on lexical frequen-
cies, it requires pre-processing with TermoStat [8] and ANNIE
(GATE). [9] presents a method for extracting terminology spe-
ciÔ¨Åc to a domain from a corpus of domain-speciÔ¨Åc text, where
no external general domain reference corpus is required. They
present an adaptation of the classic tf-idf as ranking measure
and use different Ô¨Ålters to produce a speciÔ¨Åc terminology.
More recently, the efÔ¨Åciency of ranking measure like mutual
information developed for statistical approach is discussed in
[10] and [11]. [12] proposes Termolator a terminology extrac-
tion system using a chunking procedure, and using internet
queries for ranking candidate terms. Approach is interesting
but the authors emphasize the fact that the runtime for each
queries is a limiting factor to produce a relevant ranking.
Closer to our work, [13] presents an approach combining
linguistic pattern and Z-score to extract terminology in theÔ¨Åeld of nanotechnology. [14] propose TAXI, which combines
statistics and learning approach. TAXI is a system for building
a taxonomy using 2 corpora, a generic, the other speciÔ¨Åc.
It ranks the relevance of candidates by measure (frequency-
based), and by learning with SVM. [15], [16] present TexSIS,
a bilingual terminology extraction system with chunk-based
alignment method for the generation of candidate terms. After
the corpus alignment step, they use an approach combining
log likelihood measure and Mutual Expectation measure [17]
to rank candidate terms in each language. In the same or-
der, [18], [19] present an approach to extract grammatical
terminology from linguistic corpora. They compare a series
of well-established statistical measures that have been used
in similar automatic term extraction tasks and conclude that
corpus-comparing methods perform better than metrics that
are not based on corpus comparison. [20] and [21] present
methods with word embeddings. With a small data set for
learning phase, they improve the term extraction results in
n-gram terms. However, these papers involve labelled data
sets for learning phase, which is the main difference with
our proposed approach. The originality of our approach is to
combine a lexico-syntactical and a statistical approach while
using external resources.
IV. R ESOURCES AND STATISTICS
First experiments were carried out using technical reports
collected from some customers from our industrial partners
who will be called as NC collection thereafter. Each document
contains all the non-compliance that was found on one work
site and describe solutions to resolve it. However heterogeneity
of the formats as well as the artiÔ¨Åcial repetition of the informa-
tion between two reports found in the same construction site
made the term extraction quite difÔ¨Åcult. An insightful analysis
of those reports reveals vocabulary richness, however, difÔ¨Åcult
to exploit given numerous misspellings, typing shortcuts, very
‚Äùtelegraphic‚Äù style with verbs in the inÔ¨Ånitive, little punctu-
ation, few determinants, etc. As a consequence, we used a
collection of technical speciÔ¨Åcations called CCTP2. CCTPs are
available online on public sector websites3. Several thousand
documents were collected by our industrial partner using an
automatic web collecting process. Figure 2 presents some key
descriptive statistics of theses collections.
Collection NC CCTP
Total number of documents 58 402 3665
Without pre-processing
Total number of words 130 309 230 962 734
Total number of different words 93 000 20 6264
Average words/document 125.3 63 018.48
Fig. 2. statistics of the collection.
2The technical speciÔ¨Åcations book (CCTP in French) is a contractual
document that gathers the technical clauses of a public contract in the Ô¨Åeld
of construction.
3For example, https://www.marches-publics.gouv.fr/ or
http://marchespublics.normandie.fr/.  rankingCCTPCR 1 06/02
Lot A :Siphon ind√©pendant sous √©vierCR 2 15/02
Lot A :- Siphon ind√©pendant sous √©vierLot B :- Pose des plinthes sur la cloisonCR 3 22/02
Lot A :- Siphon ind√©pendant sous √©vierLot B :- suppression de la cloison entre wc et sdbwords ngrams
extraction
pruningpre-processing
citerneaudevantentree
marchescheztechniplan
coulagepardefaut
propositionsd'auge
typologiedesciterneaux
espaced'activite
visiondepuisascenseur
vaissellesousbar
miseenplace
coulisseaupour douchette
terminology‚ë†‚ûÅ
‚ûÇ‚ë£Fig. 3. System overview
V. M ETHODOLOGY
A. System Overview
Figure 3 presents an overview of the system designed and
implemented: steps are further explained in Sections V-B to
V-E. In Step 1 pre-processing of raw information extracted
from CCTP collection takes place; this is required for nor-
malizing the entire set of documents. In Step 2 n-grams are
extracted (by using measures). 1,2,3 grams are extracted. In
Step 3, n-grams are Ô¨Åltered by using linguistic patterns and
Internet queries. Finally, in Step 4 a ranking is applied to the
Ô¨Åltered n-grams.
B. Normalization, pre-processing and word ngrams extraction
In Step 1, a text normalization is performed to improve the
quality of the process. We remove special characters such as
‚Äú/‚Äù or ‚Äú()‚Äù. Different pretreatments are done to reduce noise
in the model: we remove numbers (numeric and/or textual),
special symbols. ‚Äú.‚Äù are tagged with a special character to
not create artiÔ¨Åcial n-grams. SpeciÔ¨Åc words (including named
entities) like company names, dates, etc. are normalized and
will be removed in the next module. We do not include a
stop list to keep n-grams with prepositions, for the purpose
described in the remainder. Then, we tokenize the entire
collection before using TreeTagger [22] to get the part-of-
speech tags and lemmas of each word. After this step we
transform all vocabulary from the CCTP collection into 1-
grams, 2-grams and 3-grams. Special characters or normalized
words resulting from the previous processing are discarded. N-
grams with a very low frequency (2) are also discarded.
C. Linguistic patterns module
We use grammatical labels generated in the previous step
(section V-B) and linguistic patterns to retrieve collocations
such as NOUN-NOUN, NOUN-PREP-NOUN. These patterns
are frequently found in the literature [6] to capture speciÔ¨Åc
words in French like ‚Äùcarte de cr√©dit‚Äù( credit card ) and discard
3-gram like ‚Äôcr√©diter sa carte‚Äô ( credit his card ) with the
pattern VERB-PREP-NOUN. Among frequent patterns found
in literature, those patterns have been selected according to the
statistics obtained from a knowledge model of another Ô¨Åeld
(agriculture), given by one of our industrial partners. Figure4 presents main patterns we selected using this knowledge
model. The sum of the percentages is less than 100% because
Number Percentage
1-grams 1360 65.24%
NOUN 1037 76.25%
VERB 194 14.26%
ADJ 120 8.82%
2-grams 390 19.57%
NOUN-NOUN 346 88.72%
ADJ-NOUN 11 2.82%
PREP-NOUN 7 1,79%
VERB-NOUN 5 1,28%
3-grams 188 9.43%
NOUN-NOUN-NOUN 150 79.79%
PREP-NOUN-NOUN 15 7.98%
NOUN-PREP-NOUN 6 3,19%
VERB-NOUN-NOUN 6 3,19%
Fig. 4. Distribution of linguistic patterns according to the knowledge model.
patterns with a very low frequency are not included in the
table. We observed that the noun based patterns are the most
frequent patterns, whatever the size of the n-gram. The other
selected patterns also contain nouns, but they are n-grams
with verbs, adjectives or prepositions. Therefore, we have
conÔ¨Ågured our system to keep only the ngrams corresponding
to these patterns.
D. Pruning step
This step uses the Internet to prune n-grams for which no
information is returned after querying Bing4search engine.
We count the number of links in the result pages that contain
exactly the ngram. We save the number of exact matches
between the ngram and the title and snippet of each result.
We keep only the n-grams whose number of matches exceeds
a deÔ¨Åned threshold. We varied this threshold between 1 and
50 and results presented in Section VI-B have been obtained
with a threshold of 10.
E. Ranking step
We tested several measures as provided in [6], [16] like
maximum likelihood estimation or mutual information in
order to rank selected n-grams by quality but results were
disappointing. We Ô¨Ånally use classical Z score [23] with
twenty years of the French newspaper Le Monde5as generic
collection. This metric considers word frequencies weighted
over two different corpora, in order to assign high values to
words having much higher or lower frequencies than expected
in a generic collection. We deÔ¨Åned it as follows :
p1=a0=b0 (1)
p2=a1=b1 (2)
4https://www.bing.com/
5http://www.islrn.org/resources/421-401-527-366-2/p= (a0+a1)=(b0+b1) (3)
ZScore =p1 p2q
(p(1 p)(1
b0+1
b1)(4)
Where ais the lexical unit considered (1-gram, 2-gram or
3-gram), a0the frequency of ain the CCTP collection, b0the
total size in words of CCTP collection, b1the frequency of a
in the collection Le Monde.
VI. E XPERIMENTS AND RESULTS
A. Experimental protocol
We have made a manual evaluation on all 3 grams retained
by the system. Manual evaluation was realized by 6 specialists
in the Ô¨Åeld of construction. Each specialist evaluating one third
of the results. 5144 3-grams were evaluated with this method
and each n-gram was evaluated by 2 different specialists.
For each n-grams, the specialist can choose between three
possibilities:
0 3-gram is irrelevant
1 3-gram is relevant but does not belong to the domain
2 3-gram is relevant and belongs to the domain
Evaluation was done in two steps and we use Kappa
measure6[24] and inter-annotator agreement at the end of the
Ô¨Årst step to show the difÔ¨Åculty of the task. At the end of the
Ô¨Årst step, we obtained a Kappa score of 0.62 and a global
inter-annotator agreement of 0.74, which is quite good as
explained in [25]. The difÔ¨Åculty of the task was to distinguish
the domain-speciÔ¨Åc vocabulary from the generic vocabulary
used in the Ô¨Åeld of construction. Each disagreement was re-
evaluated in the second step by a pair of experts. Figure 5
shows the Ô¨Ånal results of the evaluation.
B. Results
In this section, we present the results obtained during the
manual evaluation of the 3-grams retained by the system. We
only compute the accuracy and the error rate, because we are
not able to compute the recall for this collection7. We have
merged the assessments of each expert using two different
evaluation rules:
a strict evaluation where a n-gram is considered correct
if both experts have rated it relevant and in the domain .
a Ô¨Çexible evaluation where a n-gram is considered correct
if both experts consider it relevant and at least one of the
experts consider it in the domain.
strict evaluation Ô¨Çexible evaluation
accuracy 0.77 0.91
error rate 0.23 0.09
Fig. 5. Results of manual evaluation on the 3-grams.
6We use general formula as follows : =A0 Ae
1 AewhereA0=observed
agreement and Ae=expected (chance) agreement.
7Indeed, we do not know every the relevant terms existing in the corpus,
so we cannot estimate the recall for the collection of terms we automatically
extract.Strict evaluation shows good quality results (0.77). Anal-
ysis of the results shows that the main error is related to
‚Äùincomplete n-grams‚Äù. For example, the 3-gram ‚Äúpersonne
√† mobilit√©‚Äù (person with mobility) is not relevant while the
4-gram ‚Äúpersonne√† mobilit√© r√©duite‚Äù (person with reduced
mobility ) can belong to the Ô¨Åeld of construction. Some errors
can also be traced back to the CCTP documents. For example,
‚Äúengin de guerre‚Äù (war machine) is a term which does not
belong to the Ô¨Åeld but a law relating to the presence of war
machine on the building sites is reported in every CCTP. The
Ô¨Çexible evaluation shows very good results (0.91) and the
difÔ¨Åculty of assessing class of some terms such as ‚Äúabsence de
remise‚Äù which has 2 distinct meanings in French (no outhouse
and no discount). The Ô¨Årst meaning is relevant in the Ô¨Åeld of
construction but not the second.</corps>
  <conclusion>The paper reports our experiments and results for building
a precise and large terminology for the construction domain.
Collecting terminology is indeed the Ô¨Årst step towards a
complete knowledge model containing both concepts and
relationships. During our work we were faced to several
problems: Ô¨Ånding resources and selecting them for building
an appropriate corpus, thinking and developing pre-processing
for cleaning those resources, experimenting distinct measures
for n-grams and selecting the most appropriate, improving
results by adding linguistic patterns and Internet queries. The
current results are quite promising according to the evaluation
of the extracted terminology carried out by 6 experts in the
Ô¨Åeld. As a perspective, we will develop generic modules and
guidelines for adapting these pre-processing modules to other
languages. Most importantly, the results of our work are useful
for extracting taxonomical and non-taxonomical relationships.
For the both purposes, we are currently working on SemEval
collection [26]. Applying our method to other domain corpora
and datasets is another future direction for this research.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>[1] S. Meignier and T. Merlin, ‚ÄúLium spkdiarization: an open source toolkit
for diarization,‚Äù in in CMU SPUD Workshop , 2010.
[2] P. Pauwels and W. Terkaj, ‚ÄúExpress to owl for construction industry:
Towards a recommendable and usable ifcowl ontology,‚Äù Automation in
Construction , vol. 63, pp. 100‚Äì133, 03 2016.
[3] H. Cunningham, ‚ÄúGate, a general architecture for text engineering,‚Äù in
Computers and the Humanities , vol. 36, 2002, pp. 223‚Äì254.
[4] P. Cimiano and J. V ¬®olker, ‚Äútext2onto,‚Äù in International conference on
application of natural language to information systems . Springer, 2005,
pp. 227‚Äì238.
[5] P. Kluegl, M. Toepfer, P.-D. Beck, G. Fette, and F. Puppe, ‚ÄúUima ruta:
Rapid development of rule-based information extraction applications,‚Äù
Natural Language Engineering , vol. 22, no. 1, p. 1‚Äì40, 2016.
[6] M. Roche and Y . Kodratoff, ‚ÄúExit: Un syst√®me it√©ratif pour l‚Äôextraction
de la terminologie du domaine√† partir de corpus sp√©cialis√©s,‚Äù in
Proceedings of JADT 4 , 2004, pp. 946‚Äì956.
[7] B. Bi√©bow, S. Szulman, and A. J. B. Cl√©ment, ‚ÄúTerminae: A linguistics-
based tool for the building of a domain ontology,‚Äù in Knowledge
Acquisition, Modeling and Management , D. Fensel and R. Studer, Eds.,
1999, pp. 49‚Äì66.
[8] P. Drouin, ‚ÄúTerm extraction using non technical corpora as point of lever-
age,‚Äù in John Benjamins Publishing Company: Amsterdam/Philadelphia ,
n. Terminology, vol. 9, Ed., 2003, pp. 99‚Äì115.[9] M. F. M. Chowdhury, A. M. Gliozzo, and S. M. Trewin, ‚ÄúDomain-
speciÔ¨Åc terminology extraction by boosting frequency metrics,‚Äù Sep. 27
2018, uS Patent App. 15/469,766.
[10] G. Bouma, ‚ÄúNormalized (pointwise) mutual information in collocation
extraction,‚Äù Proceedings of GSCL , 2009.
[11] Y . Bestgen, ‚ÄúEvaluation de mesures d‚Äôassociation pour les bigrammes et
les trigrammes au moyen du test exact de Ô¨Åsher,‚Äù Proceedings of TALN
2017 , pp. 10‚Äì19, 2017.
[12] A. L. Meyers, Y . He, Z. Glass, J. Ortega, S. Liao, A. Grieve-Smith,
R. Grishman, and O. Babko-Malaya, ‚ÄúThe termolator: Terminology
recognition based on chunking, statistical and search-based scores,‚Äù
Frontiers in Research Metrics and Analytics , vol. 3, p. 19, 2018.
[13] L. Gillam, M. Tariq, and K. Ahmad, ‚ÄúTerminology and the construction
of ontology,‚Äù TERMINOLOGY , vol. 11, pp. 55‚Äì81, 2005.
[14] A. Panchenko, S. Faralli, E. Ruppert, S. Remus, H. Naets, C. Fairon, S. P.
Ponzetto, and C. Biemann, ‚ÄúTAXI at semeval-2016 task 13: a taxonomy
induction method based on lexico-syntactic patterns, substrings and
focused crawling,‚Äù in Proceedings of the 10th International Workshop
on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA,
USA, June 16-17, 2016 , 2016, pp. 1320‚Äì1327.
[15] E. Lefever, L. Macken, and V . Hoste, ‚ÄúLanguage-independent
bilingual terminology extraction from a multilingual parallel corpus,‚Äù
inProceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics , ser. EACL ‚Äô09.
Stroudsburg, PA, USA: Association for Computational Linguistics,
2009, pp. 496‚Äì504. [Online]. Available: http://dl.acm.org/citation.cfm?
id=1609067.1609122
[16] L. Macken, E. Lefever, and V . Hoste, ‚ÄúTexsis: bilingual terminology
extraction from parallel corpora using chunk-based alignment,‚Äù
Terminology , vol. 19, no. 1, pp. 1‚Äì30, 2013. [Online]. Available:
http://dx.doi.org/10.1075/term.19.1.01mac
[17] G. Dias and H.-J. Kaalep, ‚ÄúAutomatic extraction of multiword units
for estonian : Phrasal verbs,‚Äù in Languages in Development , 2003, p.
41:81‚Äì91.
[18] B. Daille, ‚ÄúStudy and implementation of combined techniques for
automatic extraction of terminology,‚Äù The Balancing Act: Combining
Symbolic and Statistical Approaches to Language , 12 2002.
[19] C. Lang, R. Schneider, and K. Suchowolec, ‚ÄúExtracting specialized
terminology from linguistic corpora,‚Äù GRAMMAR AND CORPORA , p.
425, 2018.
[20] E. Amjadian, D. Inkpen, T. S. Paribakht, and F. Faez, ‚ÄúLocal-global
vectors to improve unigram terminology extraction,‚Äù in Proceedings of
the 5th International Workshop on Computational Terminology , 2016.
[21] G. Wohlgenannt and F. Minic, ‚ÄúUsing word2vec to build a simple
ontology learning system.‚Äù in International Semantic Web Conference
(Posters &amp; Demos) , 2016.
[22] H. Schmid, ‚ÄúProbabilistic part-of-speech tagging using decision trees,‚Äù
1994.
[23] E. Altman, ‚ÄúFinancial ratios, discriminant analysis and the predic-
tion of corporate bankruptcy.‚Äù in The Journal of Finance, 23(4).
doi:10.2307/2978933 , 1968, pp. 589‚Äì609.
[24] J. Cohen, ‚ÄúA CoefÔ¨Åcient of Agreement for Nominal Scales,‚Äù Educational
and Psychological Measurement , vol. 20, no. 1, p. 37, 1960.
[25] M. L. McHugh, ‚ÄúInterrater reliability: the kappa statistic,‚Äù in Biochemia
medica , 2012.
[26] M. Apidianaki, S. M. Mohammad, J. May, E. Shutova, S. Bethard,
and M. Carpuat, ‚ÄúProceedings of the 12th international workshop on
semantic evaluation,‚Äù in Proceedings of The 12th International Workshop
on Semantic Evaluation . Association for Computational Linguistics,
2018. [Online]. Available: http://aclweb.org/anthology/S18-1000</bibliographie>

  <preamble>kesslerMETICS-ICDIM2019.pdf</preamble>
  <titre>A word embedding approach to explore a collection
of discussions of people in psychological distress</titre>
  <auteurs>
    <auteur>
      <name>R√©my Kessler</name>
      <mail>remy.kessler@univ-ubs.fr</mail>
      <affiliation>Universit√© Bretagne Sud
CNRS 6074A
56017 Vannes,France</affiliation>
    </auteur>
    <auteur>
      <name>Nicolas B√©chet</name>
      <mail>nicolas.bechet@irisa.fr</mail>
      <affiliation>Universit√© Bretagne Sud
CNRS 6074A
56017 Vannes,France</affiliation>
    </auteur>
    <auteur>
      <name>Gudrun Ledegen</name>
      <mail>gudrun.ledegen@univ-rennes2.fr</mail>
      <affiliation>Universit√© Rennes II
PREFics, EA 4246
5043 Rennes, France</affiliation>
    </auteur>
    <auteur>
      <name>Frederic Pugni√®re-Saavedra</name>
      <mail>frederic.pugniere-saavedra@univ-ubs.fr</mail>
      <affiliation>Universit√© Bretagne Sud
PREFics, EA 4246
56017 Vannes, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>In order to better adapt to society, an association
has developed a web chat application that allows anyone to
express and share their concerns and anguishes. Several thousand
anonymous conversations have been gathered and form a new
corpus of stories about human distress and social violence. We
present a method of corpus analysis combining unsupervised
learning and word embedding in order to bring out the themes
of this particular collection. We compare this approach with a
standard algorithm of the literature on a labeled corpus and
obtain very good results. An interpretation of the obtained
clusters collection conÔ¨Årms the interest of the method.</abstract>
  <introduction>
Since the nineties, social suffering has been a theme that has
received much attention from public and associative action.
Among the consequences, there is an explosion of listening
places or socio-technical devices of communication whose
objectives consist in moderating the various forms of suffering
by the liberation of the speech for a therapeutic purpose [1]
[2]. As part of the METICS project, a suicide prevention
association developed an application of web chat to meet
this need. The web chat is an area that allows anyone to
express and share with a volunteer listener their concerns and
anguishes. The main speciÔ¨Åcity of this device is its anonymous
nature. Protected by a pseudonym, the writers are invited
to discuss with a volunteer the problematic aspects of their
existence. Several thousand anonymous conversations have
been gathered and form a corpus of unpublished stories about
human distress. The purpose of the METICS project is to make
visible the ordinary forms of suffering usually removed from
common spaces and to grasp both its modes of enunciation and
digital support. In this study, we want to automatically identify
the reason for coming on the web chat for each participant.
Indeed, even if the association provided us with the theme
of all the conversations (work, loneliness, violence, racism,
addictions, family, etc.), the original reason has not been
preserved. In what follows, we Ô¨Årst review some of the relatedwork in Section II. Section III presents the resources used and
gives some statistics about the collection. An overview of the
system and the strategy for identify the reason for coming
on the web chat is given in Section IV. Section V presents
the experimental protocol, an evaluation of our system and an
interpretation of the Ô¨Ånal results on the collection of human
distress.
</introduction>
  <corps>The main characteristic of the approach presented in this
paper is to only have to provide the labels of the classes to
be predicted. This method does not need to have a tagged
data set to predict the different classes, so it is closer to an
unsupervised (clustering) or semi-supervised learning method
than a supervised. The main idea of clustering is to group
untagged data into a number of clusters, such that similar ex-
amples are grouped together and different ones are separated.
In clustering, the number of classes and the distribution of
instances between classes are unknown and the goal is to Ô¨Ånd
meaningful clusters.
One kind of clustering methods is the partitioning-based
one. The k-means algorithm [3] is one of the most popu-
lar partitioning-based algorithms because it provides a good
compromise between the quality of the solution obtained and
its computational complexity [4]. K-means aims to Ô¨Ånd k
centroids, one for each cluster, minimizing the sum of the
distances of each instance of data from its respective centroid.
We can cite other partitioning-based algorithms such as k-
medoids or PAM (Partition Around Medoids), which is an
evolution of k-means [5]. Hierarchical approaches produce
clusters by recursively partitioning data backwards or upwards.
For example, in a hierarchical ascending classiÔ¨Åcation or
CAH [6], each example from the initial dataset represents a
cluster. Then, the clusters are merged, according to a similarity
measure, until the desired tree structure is obtained. The result
of this clustering method is called a dendrogram. Density-
based methods like the EM algorithm [7] assume that the data
belonging to each cluster is derived from a speciÔ¨Åc probabilitydistribution [8]. The idea is to grow a cluster as the density in
the neighborhood of the cluster exceeds a predeÔ¨Åned threshold.
Model-based classiÔ¨Åcation methods like self-organizing
map - SOM [9] are focus on Ô¨Ånding features to represent each
cluster. The most used methods are decision trees and neural
networks. Approaches based on semi-supervised learning such
as label propagation algorithm [10] are similar to the method
proposed in this paper because they consist in using a learning
dataset consisting of a few labelled data points to build a
model for labelling a larger number of unlabelled data. Closer
to the theme of our collection, [11] and [12] use supervised
approaches to automatically detect suicidal people in social
networks. They extract speciÔ¨Åc features like word distribution
statistics or sentiments to train different machine-learning clas-
siÔ¨Åers and compare performance of machine-learning models
against the judgments of psychiatric trainees and mental health
professionals. More recently, CLEF challenge in 2018 consists
of performing a task on early risk detection of depression on
texts written in Social Media1. However, these papers and this
task involve tagged data sets, which is the main difference
with our proposed approach (we do not have tagged data set).
III. R ESOURCES AND STATISTICS
The association provided a collection of conversations be-
tween volunteers and callers between 2005 and 2015, which
is called ‚ÄúMETICS collection‚Äù henceforth.
To reduce noise in the collection, we removed all the
discussions containing fewer than 15 exchanges between a
caller and a person from the association, these exchanges are
generally unrepresentative (connection problem, request for
information, etc.). We observe particular linguistic phenomena
like emoticons2, acronyms, mistakes (spelling, typography,
glued words) and an explosive lexical creativity [13]. These
phenomena have their origin in the mode of communication
(direct or semi-direct), the speed of the composition of the
message or in the technological constraints of input imposed
by the material (mobile terminal, tablet, etc.). In addition, we
used a subset of the collection of the French newspaper, Le
Monde to validate our method on a tagged corpus. We only
keep articles on television, politics, art, science or economics.
Figure 1 presents some descriptive statistics of these two
collections.
IV. M ETHODOLOGY
A. System Overview
Figure 2 presents an overview of the system, each step will
be detailed in the rest of the section. In the Ô¨Årst step (mod-
ule¬¨), we apply different linguistic pre-processing to each
discussion. The next module ( ¬≠) creates a word embedding
model with these discussions while the third module ( ¬Æ) uses
this model to create speciÔ¨Åc vectors. The last module ( ¬Ø)
performs a prediction for each discussion before separating
the collection into clusters based on the predicted class.
1http://early.irlab.org/
2Symbols used in written messages to express emotions, e.g. smile or
sadnessCollection METICS Le-Monde
Total number of documents 17 594 205 661
Without pre-processing
Total number of words 12 276 973 87 122 002
Total number of different words 158 361 419 579
Average words/document 698 424
With pre-processing
Total number of words 4 529 793 41 425 938
Total number of different words 120 684 419 006
Average words/document 257 201
Fig. 1. Statistics of both collections.
  Conversations
or documentsWord2vec
model
PredictionClass 1
pre-processingSpecific
vectors
creation
Model 1
‚ë†‚ûÅ‚ûÇ
‚ë£Class 2
Model 2Class n
Model n
Cluster 1Cluster 2 Cluster n
Fig. 2. System overview
B. Normalization and pre-processing
We Ô¨Årst extract the textual content of each discussion. In
step¬¨, a text normalization is performed to improve the
quality of the process. We remove accents, special characters
such as ‚Äú-‚Äù,‚Äú/‚Äù or ‚Äú()‚Äù. Different linguistic processes are used
to reduce noise in the model: we remove numbers (numeric
and/or textual), special symbols and terms contained in a stop-
list adapted to our problem. A lemmatization process was
incorporated during the Ô¨Årst experiments but it was inefÔ¨Åcient
considering the typographical variations described in Section
III.
C. word2vec model
In the next step we build a word embedding model using
word2vec [14]. We project each word of our corpus in a vector
space in order to obtain a semantic representation of these.
In this way, words appearing in similar contexts will have a
relatively close vector representation. In addition to semantic
information, one advantage of such modeling is the production
of vector representations of words, depending on the context
in which they are encountered. Some words close to a term tin
a model learned from a corpus c1may be very different from
those from a model learned from a corpus c2. For example,
we observe in Ô¨Ågure 3 that the Ô¨Årst eight words close to the
term ‚Äúteen‚Äù vary according to the corpus used. This example
also shows that the use of a generic model like Le Monde in
French or Wikipedia is irrelevant in our case, since the corpuscorpus words
METICS teenager, young, 15years, kid,
school, problem , spoiled, teen,
Le-Monde sitcom, radio, compote, hearing
boy, styx, scamp, rebel
Fig. 3. Eight words closest to the term ‚Äúteenager‚Äù according to the type of
corpus in learning.
of the association is noisy and contains a number of apocopes,
abbreviations or acronyms. Different parameters were tested
and the conÔ¨Åguration with the best results was kept3.
D. SpeciÔ¨Åc vectors creation and cluster predictions
In this step, we build vectors containing terms that are
selected using the word2vec model described in step IV-C.
For each theme in the collection, we build a speciÔ¨Åc linguistic
model by performing a word embedding to reconstruct the
linguistic context of each theme. We observe, for example,
that the terms closest to the thematic ‚Äúwork‚Äù are: ‚Äúunemploy-
ment‚Äù, ‚Äújob‚Äù, ‚Äústress‚Äù. Similarly, for the ‚Äúaddiction‚Äù theme,
we observe the terms: ‚Äúcannabis‚Äù, ‚Äúalcoholism‚Äù, ‚Äúdrugs‚Äù and
‚Äúheroin‚Äù. We used this context subsequently to construct a
vector, containing the distance dist c(i)between each term i
and the theme c. Each of these models is independent, so
the same term can appear in several models. In this way,
we observed that the word ‚Äústress‚Äù is present in the vector
‚Äúsuicide‚Äù and in that of ‚Äúwork‚Äù, however, the associated weight
is different. We varied the size of these vectors between 20
and 1000 and the best results were obtained with a size of 400.
In the last step ¬Ø, the system computes an Scscore for each
discussion and for each cluster according to each linguistic
model such as:
Sc(d) =nX
i=1tf(i)dist c(i) (1)
with ithe considered term, tf(i)frequency of iin the
collection, and dist c(i)is the distance between the term iand
the thematic c. In the end, the class with the highest score is
chosen.
V. E XPERIMENTS AND RESULTS
A. Experimental protocol
To evaluate the quality of the obtained clusters, we used a
subset of the texts of the Le-Monde newspaper, described in
Section III, each article having a label according to the theme.
For these experiments, we conÔ¨Ågured the speciÔ¨Åc vectors (SV)
approach with the optimal parameters, as deÔ¨Åned in Sections
IV-C and IV-D. We also tested the speciÔ¨Åc vectors without
weighting to test the particular inÔ¨Çuence of this parameter. To
highlight the difÔ¨Åculty of the task, we compare our system
with a baseline which consists in a random draw, and with
3The best results were obtained with the following parameter values: vector
size: 700, sliding window size: 5, minimum frequency: 10, vectorization
method: skip grams, and a soft hierarchical max function for the model
learning.the k-means algorithm [3], commonly used in the literature,
as mentioned in Section II. To feed the k-means algorithm, we
transformed our initial collection into a bag of words matrix
[15] where each conversation is described by the frequency
of its words. Each of the experiments was evaluated using the
classic measures of Precision, Recall and F-measure, averaged
over all classes (with beta = 1 in order not to privilege
precision or recall [16]). Since the k-means algorithm does not
associate a tag with the Ô¨Ånal clusters, we have exhaustively
calculated the set of solutions to keep only the one yielding
the highest F-score.
B. Results
Prec. Recall F-score
Without pre-processing
Baseline 0.18 0.16 0.17
k-means 0.23 0.20 0.22
Without weighting 0.54 0.50 0.52
SpeciÔ¨Åc Vectors 0.53 0.54 0.53
With pre-processing
k-means 0.30 0.21 0.25
Without weighting 0.55 0.51 0.53
SpeciÔ¨Åc Vectors 0.54 0.54 0.54
Fig. 4. Results obtained by each system.
Figure 4 presents a summary of the results obtained with
each systems. We Ô¨Årst observe that baseline scores are very
low, but remain relatively close to the theoretical random (0.2)
given by the number of classes. Linguistic pre-treatments are
not very efÔ¨Åcient individually, but improve overall the results
of other experiments. The k-means algorithm obtains slightly
better results in terms of F-score, but remains weak. SpeciÔ¨Åc
vectors get excellent results that outperform other systems with
an F-score of 0.54. The execution without weighting improve
slightly the recall.
C. Cluster Analysis
Initial objective of this work was the exploration of the
METICS collection, we apply the whole process with the
speciÔ¨Åc vectors approach to automatically categorize all the
conversations. We use the Latent Dirichlet Allocation [17] in
order to obtain the main topic of each cluster. Figure 5 presents
average weight of each thematic keywords according to each
clusters.
In Ô¨Ågure 5, fear, shrink and trust are present designations
for each cluster with a largely signiÔ¨Åcant rank; yet, does
the writer still express fear when he writes, ‚ÄùI‚Äôm afraid of
being sick‚Äù? Do these designations not participate in opening
and constructing spheres of meanings around these pivotal
words? Conversely, with a lower rank, but also signiÔ¨Åcant, the
designations of thing, difÔ¨Åcult, and problem are more vague,
but more reformulating to take up the elements involved in
writing what is wrong.Fig. 5. Distribution of discursive routines by cluster.</corps>
  <conclusion>In this article, we presented an unsupervised approach to
exploring a collection of stories about human distress. This
approach uses a word embedding model to build vectors
containing only vocabulary from the linguistic context of the
model. We evaluated the quality of the approach on a col-
lection labeled with classical measures. The detailed analysis
showed very good results (average Fscore of 0.54) compared
to the other systems tested. This method of analysis has also
made it possible to highlight semantic universes and thematic
groupings. We Ô¨Årst intend to study in more detail the inÔ¨Çuence
of each of the parameters on the results obtained. We are also
planning to be able to assign several tags to each discussion,
which would allow thematic overlaps to be taken into account.
The analysis reinforces the cluster approach to highlight the
deÔ¨Åning features of this type of speech production and to
reveal its inner workings. This entry by the discursive routines
is only one example which will then make it possible to
approach other explorations with a particular focus on the
argumentative forms and on the forms of intensity.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>[1] D. Fassin, ‚ÄúEt la souffrance devint sociale,‚Äù in Critique . 680(1), 2004,
pp. 16‚Äì29.
[2] ‚Äî‚Äî, ‚ÄúSouffrir par le social, gouverner par l‚Äô√©coute,‚Äù in Politix . 73(1),
2006, pp. 137‚Äì157.
[3] MacQueen, J., ‚ÄúSome methods for classiÔ¨Åcation and analysis of multi-
variate observations,‚Äù in Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, Vol. 1: Statistics . USA:
University of California Press, 1967, pp. 281‚Äì297.
[4] D. Arthur and S. Vassilvitskii, ‚ÄúK-means++: The advantages of careful
seeding,‚Äù Proceedings of the Eighteenth Annual ACM-SIAM Symposium
on Discrete Algorithms , pp. 1027‚Äì1035, 2007.
[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids .
Delft University of Technology : reports of the Faculty of
Technical Mathematics and Informatics, 1987. [Online]. Available:
https://books.google.fr/books?id=HK-4GwAACAAJ
[6] G. N. Lance and W. T. Williams, ‚ÄúA general theory of classiÔ¨Åcatory
sorting strategies1. hierarchical systems,‚Äù The Computer Journal 4 , pp.
373‚Äì380, 1967.[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, ‚ÄúMaximum likelihood
from incomplete data via the em algorithm,‚Äù in Journal of the royal
society, series B , 1977, pp. 1‚Äì38.
[8] J. D. BanÔ¨Åeld and A. E. Raftery, ‚ÄúModel-based gaussian and non-
gaussian clustering,‚Äù in Biometrics , vol. 49, 1993, pp. 803‚Äì821.
[9] T. Kohonen, ‚ÄúSelf-organized formation of topologically correct feature
maps,‚Äù Biological Cybernetics , pp. 59‚Äì69, Jan 1982.
[10] U. N. Raghavan, R. Albert, and S. Kumara, ‚ÄúNear linear time algorithm
to detect community structures in large-scale networks.‚Äù Physical review.
E, Statistical, nonlinear, and soft matter physics , p. 036106, 2007.
[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,
J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, ‚ÄúSentiment analysis
of suicide notes: A shared task,‚Äù Biomedical Informatics Insights , pp.
3‚Äì16, 2012.
[12] A. Abboute, Y . Boudjeriou, G. Entringer, J. Az√©, S. Bringay, and
P. Poncelet, ‚ÄúMining twitter for suicide prevention,‚Äù in Natural Language
Processing and Information Systems: 19th International Conference on
Applications of Natural Language to Information Systems, NLDB 2014,
Montpellier, France, June 18-20, 2014. Proceedings . Springer, 2014,
pp. 250‚Äì253.
[13] R. Kessler, J.-M. Torres, and M. El-B√®ze, ‚ÄúClassiÔ¨Åcation th√©matique de
courriel par des m√©thodes hybrides,‚Äù Journ√©e ATALA sur les nouvelles
formes de communication√©crite , 2004.
[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, ‚ÄúDistributed
representations of words and phrases and their compositionality,‚Äù in
Proceedings of NIPS‚Äô13 . USA: Curran Associates Inc., 2013,
pp. 3111‚Äì3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=
2999792.2999959
[15] C. D. Manning and H. Sch√ºtze, Foundations of Statistical Natural
Language Processing . Cambridge, MA, USA: MIT Press, 1999.
[16] C. Goutte and E. Gaussier, ‚Äú A Probabilistic Interpretation of Precision,
Recall and F-Score, with Implication for Evaluation,‚Äù ECIR 2005 , pp.
345‚Äì359, 2005.
[17] M. Hoffman, F. R. Bach, and D. M. Blei, ‚ÄúOnline learning for latent
dirichlet allocation,‚Äù in Advances in Neural Information Processing
Systems , J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
and A. Culotta, Eds. 23, 2010, pp. 856‚Äì86</bibliographie>

  <preamble>mikheev J02-3002.pdf</preamble>
  <titre>Periods, Capitalized Words, etc.</titre>
  <auteurs>
    <auteur>
      <name>Andrei Mikheev</name>
      <mail>mikheev@cogsci.ed.ac.uk</mail>
      <affiliation>University of Edinburgh</affiliation>
    </auteur>
  </auteurs>
  <abstract>In this article we present an approach for tackling three important aspects of text normaliza-
tion: sentence boundary disambiguation, disambiguation of capitalized words in positions wherecapitalization is expected, and identiÔ¨Åcation of abbreviations. As opposed to the two dominanttechniques of computing statistics or writing specialized grammars, our document-centered ap-proach works by considering suggestive local contexts and repetitions of individual words withina document. This approach proved to be robust to domain shifts and new lexica and produced per-formance on the level with the highest reported results. When incorporated into a part-of-speechtagger, it helped reduce the error rate signiÔ¨Åcantly on capitalized words and sentence boundaries.We also investigated the portability to other languages and obtained encouraging results.</abstract>
  <introduction>
Disambiguation of sentence boundaries and normalization of capitalized words, as
well as identiÔ¨Åcation of abbreviations, however small in comparison to other tasksof text processing, are of primary importance in the developing of practical text-processing applications. These tasks are usually performed before actual ‚Äúintelligent‚Äùtext processing starts, and errors made at this stage are very likely to cause more errorsat later stages and are therefore very dangerous.
Disambiguation of capitalized words in mixed-case texts has received little atten-
tion in the natural language processing and information retrieval communities, but infact it plays an important role in many tasks. In mixed-case texts capitalized wordsusually denote proper names (names of organizations, locations, people, artifacts, etc.),but there are special positions in the text where capitalization is expected. Such manda-tory positions include the Ô¨Årst word in a sentence, words in titles with all signiÔ¨Åcantwords capitalized or table entries, a capitalized word after a colon or open quote, andthe Ô¨Årst word in a list entry, among others. Capitalized words in these and some otherpositions present a case of ambiguity: they can stand for proper names, as in White
later said ..., or they can be just capitalized common words, as in White elephants are
.... The disambiguation of capitalized words in ambiguous positions leads to the
identiÔ¨Åcation of proper names (or their derivatives), and in this article we will use
these two terms and the term case normalization interchangeably.
Church (1995, p. 294) studied, among other simple text normalization techniques,
the effect of case normalization for different words and showed that ‚Äúsometimes casevariants refer to the same thing ( hurricane and Hurricane ), sometimes they refer to
different things ( continental and Continental ) and sometimes they don‚Äôt refer to much
of anything (e.g., anytime and Anytime ).‚Äù Obviously these differences arise because
some capitalized words stand for proper names (such as Continental , the name of an
airline) and some do not.
‚àóInstitute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place,
Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk
Proper names are the main concern of the named-entity recognition subtask (Chin-
chor 1998) of information extraction. The main objective of this subtask is the identi-Ô¨Åcation of proper names and also their classiÔ¨Åcation into semantic categories (person,organization, location, etc.).
1There the disambiguation of the Ô¨Årst word in a sentence
(and in other ambiguous positions) is one of the central problems: about 20% of namedentities occur in ambiguous positions. For instance, the word Black in the sentence-
initial position can stand for a person‚Äôs surname but can also refer to the color. Evenin multiword capitalized phrases, the Ô¨Årst word can belong to the rest of the phraseor can be just an external modiÔ¨Åer. In the sentence Daily, Mason and Partners lost their
court case , it is clear that Daily, Mason and Partners is the name of a company. In the
sentence Unfortunately, Mason and Partners lost their court case , the name of the company
does not include the word Unfortunately , but the word Daily is just as common a word
asUnfortunately .
IdentiÔ¨Åcation of proper names is also important in machine translation, because
usually proper names are transliterated (i.e., phonetically translated) rather than prop-erly (semantically) translated. In conÔ¨Ådential texts, such as medical records, propernames must be identiÔ¨Åed and removed before making such texts available to peopleunauthorized to have access to personally identiÔ¨Åable information. And in general,most tasks that involve text analysis will beneÔ¨Åt from the robust disambiguation ofcapitalized words into proper names and common words.
Another important task of text normalization is sentence boundary disambigua-
tion (SBD) or sentence splitting. Segmenting text into sentences is an important aspectin developing many applications: syntactic parsing, information extraction, machinetranslation, question answering, text alignment, document summarization, etc. Sen-tence splitting in most cases is a simple matter: a period, an exclamation mark, or aquestion mark usually signals a sentence boundary. In certain cases, however, a perioddenotes a decimal point or is a part of an abbreviation, and thus it does not necessarilysignal a sentence boundary. Furthermore, an abbreviation itself can be the last tokenin a sentence in which case its period acts at the same time as part of this abbreviationand as the end-of-sentence indicator (fullstop). A detailed introduction to the SBDproblem can be found in Palmer and Hearst (1997).
The disambiguation of capitalized words and sentence boundaries presents a
chicken-and-egg problem. If we know that a capitalized word that follows a period isa common word, we can safely assign such period as sentence terminal. On the otherhand, if we know that a period is not sentence terminal, then we can conclude thatthe following capitalized word is a proper name.
Another frequent source of ambiguity in end-of-sentence marking is introduced by
abbreviations: if we know that the word that precedes a period is notan abbreviation,
then almost certainly this period denotes a sentence boundary. If, however, this wordis an abbreviation, then it is not that easy to make a clear decision. This problem isexacerbated by the fact that abbreviations do not form a closed set; that is, one can-not list all possible abbreviations. Moreover, abbreviations can coincide with regularwords; for example, ‚Äúin‚Äù can denote an abbreviation for ‚Äúinches,‚Äù ‚Äúno‚Äù can denote anabbreviation for ‚Äúnumber,‚Äù and ‚Äúbus‚Äù can denote an abbreviation for ‚Äúbusiness.‚Äù
In this article we present a method that tackles sentence boundaries, capitalized
words, and abbreviations in a uniform way through a document-centered approach.As opposed to the two dominant techniques of computing statistics about the wordsthat surround potential sentence boundaries or writing specialized grammars, our ap-
1 In this article we are concerned only with the identiÔ¨Åcation of proper names.
proach disambiguates capitalized words and abbreviations by considering suggestive
local contexts and repetitions of individual words within a document. It then appliesthis information to identify sentence boundaries using a small set of rules.
</introduction>
  <corps>A standard practice for measuring the performance of a system for the class of tasks
with which we are concerned in this article is to calculate its error rate :
error
rate=incorrectly assigned
allassigned bysystem
This single measure gives enough information, provided that the system does not
leave unassigned word tokens that it is intended to handle. Obviously, we want thesystem to handle all cases as accurately as possible. Sometimes, however, it is beneÔ¨Åcialto assign only cases in which the system is conÔ¨Ådent enough, leaving the rest to behandled by other methods. In this case apart from the error rate (which correspondsto precision or accuracy as 1 ‚àíerror rate) we also measure the system‚Äôs coverage or
recall
coverage =correctly
assigned
alltobeassigned
2.1 Corpora for Evaluation
There are two corpora normally used for evaluation in a number of text-processingtasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ)
corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993).The Brown corpus represents general English. It contains over one million word tokensand is composed from 15 subcorpora that belong to different genres and domains,ranging from news wire texts and scientiÔ¨Åc papers to Ô¨Åction and transcribed speech.The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, andungrammatical sentences with complex internal structure. Altogether there are about500 documents in the Brown corpus, with an average length of 2,300 word tokens.
The WSJ corpus represents journalistic news wire style. Its size is also over a
million word tokens, and the documents it contains are rich in abbreviations andproper names, but they are much shorter than those in the Brown corpus. Altogetherthere are about 2,500 documents in the WSJ corpus, with an average length of about500 word tokens.
Documents in the Penn Treebank are segmented into paragraphs and sentences.
Sentences are further segmented into word tokens annotated with part-of-speech (POS)information. POS information can be used to distinguish between proper names andcommon words. We considered proper nouns ( NNP), plural proper nouns ( NNPS ), and
proper adjectives
2(JJP) to signal proper names, and all other categories were consid-
ered to signal common words or punctuation. Since proper adjectives are not includedin the Penn Treebank tag set, we had to identify and retag them ourselves with thehelp of a gazetteer.
Abbreviations in the Penn Treebank are tokenized together with their trailing pe-
riods, whereas fullstops and other sentence boundary punctuation are tokenized asseparate tokens. This gives all necessary information for the evaluation in all our three
2 These are adjectives derived from proper nouns (e.g. ‚ÄúAmerican‚Äù).
tasks: the sentence boundary disambiguation task, the capitalized word disambigua-
tion task, and the abbreviation identiÔ¨Åcation task.
2.2 Tokenization Convention and Corpora Markup
For easier handling of potential sentence boundary punctuation, we developed a newtokenization convention for periods. In the traditional Penn Treebank schema, abbrevi-ations are tokenized together with their trailing periods, and thus stand-alone periods
unambiguously signal the end of a sentence. We decided to treat periods and all otherpotential sentence termination punctuation as ‚ÄúÔ¨Årst-class citizens‚Äù and adopted a con-vention always to tokenize a period (and other punctuation) as a separate token whenit is followed by a white space, line break, or punctuation. In the original Penn Tree-bank format, periods are unambiguous, whereas in our new convention they can takeon one of the three tags: fullstop ( .), part of abbreviation ( A) or both ( *).
To generate the new format from the Penn Treebank, we had to split Ô¨Ånal periods
from abbreviations, mark them as separate tokens and assign them with Aor*tags
according to whether or not the abbreviation was the last token in a sentence. Weapplied a similar tokenization convention to the case in which several (usually three)periods signal ellipsis in a sentence. Again, sometimes such constructions occur withina sentence and sometimes at a sentence break. We decided to treat such constructionssimilarly to abbreviations, tokenize all periods but the last together in a single token,and tokenize the last period separately and tag it with Aor*according to whether
or not the ellipsis was the last token in a sentence. We treated periods in numbers(e.g., 14.534 ) or inside acronyms (e.g., Y.M.C.A. ) as part of tokens rather than separate
periods.
In all our experiments we treated embedded sentence boundaries in the same way
as normal sentence boundaries. An embedded sentence boundary occurs when thereis a sentence inside a sentence. This can be a quoted direct-speech subsentence inside asentence, a subsentence embedded in brackets, etc. We considered closing punctuationof such sentences equal to closing punctuation of normal sentences.
We also specially marked word tokens in positions where they were ambiguously
capitalized if such word tokens occurred in one of the following contexts:
‚Ä¢the Ô¨Årst token in a sentence
‚Ä¢following a separately tokenized period, question mark, exclamation
mark, semicolon, colon, opening quote, closing quote, opening bracket,or closed bracket
‚Ä¢occurring in a sentence with all words capitalized
All described transformations were performed automatically by applying a simple
Perl script. We found quite a few infelicities in the original tokenization and tagging,
however, which we had to correct by hand. We also converted both our corpora fromtheir original Penn Treebank format into an XML format where each word token isrepresented as an XML element ( W) with the attribute Cholding its POS information
and attribute Aset toYfor ambiguously capitalized words. An example of such a
markup is displayed in Figure 1.
3. Our Approach to Sentence Boundary DisambiguationIf we had at our disposal entirely correct information on whether or not each word
preceding a period was an abbreviation and whether or not each capitalized word
...&lt;W C=RB&gt;soon&lt;/W&gt;&lt;W C=‚Äô.‚Äô&gt;.&lt;/W&gt; &lt;W A=Y C=NNP&gt;Mr&lt;/W&gt;&lt;W C=A&gt;.&lt;/W&gt;...
...&lt;W C=VBN&gt;said&lt;/W&gt; &lt;W C=NNP&gt;Mr&lt;/W&gt;&lt;W C=A&gt;.&lt;/W&gt;
&lt;W A=Y C=NNP&gt;Brown&lt;/W&gt;...
...&lt;W C=‚Äô,‚Äô&gt;,&lt;/W&gt; &lt;W C=NNP&gt;Tex&lt;/W&gt;&lt;W C=‚Äô*‚Äô&gt;.&lt;/W&gt;
&lt;W A=Y C=JJP&gt;American&lt;/W&gt;...
Figure 1
Example of the new tokenization and markup generated from the Penn Treebank format.Tokens are represented as XML elements W, where the attribute Cholds POS information.
Proper names are tagged as NNP,NNPS andJJP. Periods are tagged as .(fullstop), A(part of
abbreviation), *(a fullstop and part of abbreviation at the same time). Ambiguously
capitalized words are marked with A=Y .
that follows a period was a proper name, we could apply a very simple set of rules
to disambiguate sentence boundaries:
‚Ä¢If a period follows a nonabbreviation, it is sentence terminal ( .).
‚Ä¢If a period follows an abbreviation and is the last token in a text passage
(paragraph, document, etc.), it is sentence terminal and part of theabbreviation ( *).
‚Ä¢If a period follows an abbreviation and is not followed by a capitalized
word, it is part of the abbreviation and is not sentence terminal ( A).
‚Ä¢If a period follows an abbreviation and is followed by a capitalized word
that is not a proper name, it is sentence terminal and part of theabbreviation ( *).
It is a trivial matter to extend these rules to allow for brackets and quotation marks
between the period and the following word. To handle other sentence terminationpunctuation such as question and exclamation marks and semicolons, this rule setalso needs to include corresponding rules. The entire rule set for sentence boundarydisambiguation that was used in our experiments is listed in Appendix A.
3.1 Ideal Case: Upper Bound for Our SBD Approach
The estimates from the Brown corpus and the WSJ corpus (section 3) show that theapplication of the SBD rule set described above together with the information onabbreviations and proper names marked up in the corpora produces very accurateresults (error rate less than 0.0001%), but it leaves unassigned the outcome of the casein which an abbreviation is followed by a proper name. This is a truly ambiguous case,and to deal with this situation in general, one should encode detailed informationabout the words participating in such contexts. For instance, honoriÔ¨Åc abbreviationssuch as Mr. orDr.when followed by a proper name almost certainly do not end a
sentence, whereas the abbreviations of U.S. states such as Mo.,Cal., and Ore., when
followed directly by a proper name, most likely end a sentence. Obviously encodingthis kind of information into the system requires detailed analysis of the domain lexica,is not robust to unseen abbreviations, and is labor intensive.
To make our method robust to unseen words, we opted for a crude but simple
solution. If such ambiguous cases are always resolved as ‚Äúnot sentence boundary‚Äù ( A),
this produces, by our measure, an error rate of less than 3%. Estimates from the Brown
Table 1
Estimates of the upper and lower bound error rates on the SBD task for our method. Threeestimated categories are sentence boundaries, ambiguously capitalized words, andabbreviations.
Brown Corpus WSJ Corpus
SBD Amb. Cap. Abbreviations SBD Amb. Cap. Abbreviations
Number of 59,539 58,957 4,657 53,043 54,537 16,317resolved instances
A Upper Bound: 0.01% 0.0% 0.0% 0.13% 0.0% 0.0%
All correct propernamesAll correct abbrs.
B Lower Bound: 2.00% 7.4% 10.8% 4.10% 15.0% 9.6%
Lookup propernamesGuessed abbrs.
C Lookup proper 1.20% 7.4% 0.0% 2.34% 15.0% 0.0%
namesAll correct abbrs.
D All correct proper 0.45% 0.0% 10.8% 1.96% 0.0% 9.6%
namesGuessed abbrs.
corpus and the WSJ corpus showed that such ambiguous cases constitute only 5‚Äì7%
of all potential sentence boundaries. This translates into a relatively small impact ofthe crude strategy on the overall error rate on sentence boundaries. This impact wasmeasured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presentedin row A of Table 1. Although this overly simplistic strategy extracts a small penaltyfrom the performance, we decided to use it because it is very general and independentof domain-speciÔ¨Åc knowledge.
The SBD handling strategy described above is simple, robust, and well perform-
ing, but it relies on the assumption that we have entirely correct information aboutabbreviations and proper names, as can be seen in row A of the table. The main dif-Ô¨Åculty is that when dealing with real-world texts, we have to identify abbreviationsand proper names ourselves. Thus estimates based on the application of our methodwhen using 100% correctly disambiguated capitalized words and abbreviations can beconsidered as the upper bound for the SBD approach, that is, the top performance wecan achieve.
3.2 Worst Case: Lower Bound for Our SBD Approach
We can also estimate the lower bound for this approach applying very simple strategiesto the identiÔ¨Åcation of proper names and abbreviations.
The simplest strategy for deciding whether or not a capitalized word in an ambigu-
ous position is a proper name is to apply a lexical-lookup strategy (possibly enhancedwith a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, wordsnot listed as known common words for a language are usually marked as propernames. The application of this strategy produced a 7.4% error rate on the Browncorpus and a 15% error rate on the WSJ corpus. The difference in error rates can beexplained by the observation that the WSJ corpus contains a higher percentage of orga-nization names and person names, which often coincide with common English words,
and it contains more words in titles with all important words capitalized, which we
also consider as ambiguously capitalized.
The simplest strategy for deciding whether a word that is followed by a period
is an abbreviation or a regular word is to apply well-known heuristics based on theobservation that single-word abbreviations are short and normally do not includevowels ( Mr.,Dr.,kg.). Thus a word without vowels can be guessed to be an abbreviation
unless it is written in all capital letters and can stand for an acronym or a proper name(e.g., BBC ). A span of single letters separated by periods forms an abbreviation too
(e.g., Y.M.C.A. ). A single letter followed by a period is also a very likely abbreviation.
There is also an additional heuristic that classiÔ¨Åes as abbreviations short words (withlength less than Ô¨Åve characters) that are followed by a period and then by a comma, alower-cased word, or a number. All other words are considered to be nonabbreviations.
These heuristics are reasonably accurate. On the WSJ corpus they misrecognized
as abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate wassigniÔ¨Åcantly higher: 1.6%. The major source for these errors were single letters thatstand for mathematical symbols in the scientiÔ¨Åc subcorpora of the Brown Corpus (e.g.,point T ortriangle F ). The major shortcoming of these abbreviation-guessing heuristics,
however, comes from the fact that they failed to identify about 9.5% of abbreviations.This brings the overall error rate of the abbreviation-guessing heuristics to about 10%.
Combining the information produced by the lexical-lookup approach to proper
name identiÔ¨Åcation with the abbreviation-guessing heuristics feeding the SBD rule setgave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on theSBD task. This can be interpreted as the lower bound to our SBD approach. Here wesee how errors in the identiÔ¨Åcation of proper names and abbreviations propagatedthemselves into errors on sentence boundaries. Row B of Table 1 displays a summaryfor the lower-bound results.
3.3 Major Findings
We also measured the importance of each of the two knowledge sources (abbreviationsand proper names) separately. First, we applied the SBD rule set when all abbreviationswere correctly identiÔ¨Åed (using the information presented in the corpus) but applyingthe lexical lookup strategy to proper-name identiÔ¨Åcation (row C of Table 1). Then, weapplied the SBD rule set when all proper names were correctly identiÔ¨Åed (using theinformation presented in the corpus) but applying the guessing heuristics to handleabbreviations (row D of the table). In general, when a knowledge source returned100% accurate information this signiÔ¨Åcantly improved performance on the SBD taskmeasured against the lower-bound error rate. We also see that proper names have ahigher impact on the SBD task than abbreviations.
Since the upper bound of our SBD approach is high and the lower bound is far
from being acceptable, our main strategy for sentence boundary disambiguation will be to
invest in the disambiguation of capitalized words and abbreviations that then feed our SBDrule set .
4. Document-Centered Approach to Proper Name and Abbreviation Handling
As we discussed above, virtually any common word can potentially act as a proper
name or part of a multiword proper name. The same applies to abbreviations: there isno Ô¨Åxed list of abbreviations, and almost any short word can be used as an abbrevia-tion. Fortunately, there is a mitigating factor too: important words are typically usedin a document more than once and in different contexts. Some of these contexts create
ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually
unambiguously introduced at least once in the text unless they are part of commonknowledge presupposed to be possessed by the readers.
This observation can be applied to a broader class of tasks. For example, people
are often referred to by their surnames (e.g., Black ) but are usually introduced at least
once in the text either with their Ô¨Årst name ( John Black ) or with their title/profession
afÔ¨Åliation ( Mr. Black ,President Bush ), and it is only when their names are common
knowledge that they do not need an introduction (e.g., Castro ,Gorbachev ). Thus our
suggestion is to look at the unambiguous usages of the words in question in the entire document .
In the case of proper name identiÔ¨Åcation, we are not concerned with the semantic
class of a name (e.g., whether it is a person‚Äôs name or a location), but rather we simplywant to distinguish whether a capitalized word in a particular occurrence acts as aproper name (or part of a multiword proper name). If we restrict our scope to a singlesentence, we might Ô¨Ånd that there is just not enough information to make a reliabledecision. For instance, Riders in the sentence Riders rode all over the green is equally likely
to be a proper noun, a plural proper noun, or a plural common noun. But if in thesame text we Ô¨Ånd John Riders , this sharply increases the likelihood that the proper noun
interpretation is the correct one, and conversely if we Ô¨Ånd many riders , this suggests
the plural-noun interpretation.
The above reasoning can be summarized as follows: if we detect that a word is
used capitalized in an unambiguous context, this increases the chances that this wordacts as a proper name in ambiguous positions in the same document. And converselyif a word is seen only lower-cased, this increases the chances that it should be treatedas a common word even when used capitalized in ambiguous positions in the samedocument. (This, of course, is only a general principle and will be further elaboratedelsewhere in the article.)
The same logic applies to abbreviations. Although a short word followed by a
period is a potential abbreviation, the same word occurring in the same documentin a different context can be unambiguously classiÔ¨Åed as a regular word if it is usedwithout a trailing period, or it can be unambiguously classiÔ¨Åed as an abbreviation ifit is used with a trailing period and is followed by a lower-cased word or a comma.This information gives us a better chance of assigning these potential abbreviationscorrectly in nonobvious contexts.
We call such style of processing a document-centered approach (DCA), since in-
formation for the disambiguation of an individual word token is derived from theentire document rather than from its immediate local context. Essentially the systemcollects suggestive instances of usage for target words from each document underprocessing and applies this information on the Ô¨Çy to the processing of the document,in a manner similar to instance-based learning. This differentiates DCA from the tradi-tional corpus-based approach, in which learning is applied prior to processing, whichis usually performed with supervision over multiple documents of the training corpus.
5. Building Support Resources
Our method requires only four word lists. Each list is a collection of words that belong
to a single type, but at the same time, a word can belong to multiple lists. Since wehave four lists, we have four types:
‚Ä¢common word (as opposed to proper name)
‚Ä¢common word that is a frequent sentence starter
‚Ä¢frequent proper name
‚Ä¢abbreviation (as opposed to regular word)
These four lists can be acquired completely automatically from raw (unlabeled) texts.
For the development of these lists we used a collection of texts of about 300,000 wordsderived from the New York Times (NYT) corpus that was supplied as training data for
the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used thesetexts because the approach described in this article was initially designed to be partof a named-entity recognition system (Mikheev, Grover, and Moens 1998) developedfor MUC-7. Although the corpus size of 300,000 words can be seen as large, the factthat this corpus does not have to be annotated in any way and that a corpus of similarsize can be easily collected from on-line sources (including the Internet) makes thisresource cheap to obtain.
The Ô¨Årst list on which our method relies is a list of common words . This list
includes common words for a given language, but no supplementary information suchas POS or morphological information is required to be present in this list. A varietyof such lists for many languages are already available (e.g., Burnage 1990). Words insuch lists are usually supplemented with morphological and POS information (whichis not required by our method). We do not have to rely on pre-existing resources,however. A list of common words can be easily obtained automatically from a raw(unannotated in any way) text collection by simply collecting and counting lower-cased words in it. We generated such list from the NYT collection. To account forpotential spelling and capitalization errors, we included in the list of common wordsonly words that occurred lower-cased at least three times in the NYT texts. The listof common words that we developed from the NYT collection contained about 15,000English words.
The second list on which our method relies is a frequent-starters list , a list of
common words most frequently used in sentence-starting positions. This list can alsobe obtained completely automatically from an unannotated corpus by applying thelexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a7‚Äì15% error rate. We applied the list of common words over the NYT text collec-tion to tag capitalized words in sentence-starting positions as common words and asproper names: if a capitalized word was found in the list of common words, it wastagged as a common word: otherwise it was tagged as a proper name. Of course, suchtagging was far from perfect, but it was good enough for our purposes. We included inthe frequent-starters list only the 200 most frequent sentence-starting common words.This was more than a safe threshold to ensure that no wrongly tagged words wereadded to this list. As one might predict, the most frequent sentence-starting commonword was The. This list also included some adverbs, such as However ,Suddenly , and
Once ; some prepositions, such as In,To, and By; and even a few verbs: Let,Have ,Do, etc.
The third list on which our method relies is a list of single-word proper names that
coincide with common words. For instance, the word Japan is much more likely to be
used as a proper name (name of a country) rather than a verb, and therefore it needs tobe included in this list. We included in the proper name list 200 words that were mostfrequently seen in the NYT text collection as single capitalized words in unambiguouspositions and that at the same time were present in the list of common words. Forinstance, the word Thecan frequently be seen capitalized in unambiguous positions,
but it is always followed by another capitalized word, so we do not count it as acandidate. On the other hand the word China is often seen capitalized in unambiguous
positions where it is not preceded or followed by other capitalized words. Since china
Table 2
Error rates for different combinations of the abbreviation identiÔ¨Åcation methods, includingcombinations of guessing heuristics (GH), lexical lookup (LL), and the document-centeredapproach (DCA).
Abbreviation IdentiÔ¨Åcation Method WSJ Brown
A GH 9.6% 10.8%B LL 12.6% 11.9%CG H +LL 1.2% 2.1%
DG H +DCA 6.6% 8.9%
EG H +DCA+LL 0.8% 1.2%
is also listed among common words and is much less frequently used in this way, we
include it in the proper name list.
The fourth list on which our method relies is a list of known abbreviations .
Again, we induced this list completely automatically from an unannotated corpus.We applied the abbreviation-guessing heuristics described in Section 6 to our NYTtext collection and then extracted the 270 most frequent abbreviations: all abbrevi-ations that appeared Ô¨Åve times or more. This list included honoriÔ¨Åc abbreviations(Mr,Dr), corporate designators ( Ltd,Co), month name abbreviations ( Jan,Feb), ab-
breviations of names of U.S. states ( Ala,Cal), measure unit abbreviations ( ft,kg), etc.
Although we described these abbreviations in groups, this information was not en-coded in the list; the only information this list provides is that a word is a knownabbreviation.
Among these four lists the Ô¨Årst three reÔ¨Çect general language regularities and
usually do not require modiÔ¨Åcation for handling texts from a new domain. The ab-breviation list, however, is much more domain dependent and for better performanceneeds to be reinduced for a new domain. Since the compilation of all four lists does notrequire data preannotated in any way, it is very easy to specialize the above-describedlists to a particular domain: we can simply rebuild the lists using a domain-speciÔ¨Åccorpus. This process is completely automatic and does not require any human laborapart from collecting a raw domain-speciÔ¨Åc corpus. Since all cutoff thresholds thatwe applied here were chosen by intuition, however, different domains might requiresome new settings.
6. Recognizing Abbreviations
The answer to the question of whether or not a particular word token is an abbreviation
or a regular word largely solves the sentence boundary problem. In the Brown corpus92% of potential sentence boundaries come after regular words. The WSJ corpus isricher with abbreviations, and only 83% of sentences in that corpus end with a regularword followed by a period. In Section 3 we described the heuristics for abbreviationguessing and pointed out that although these heuristics are reasonably accurate, theyfail to identify about 9.5% of abbreviations. Since unidentiÔ¨Åed abbreviations are thentreated as regular words, the overall error rate of the guessing heuristics was measuredat about 10% (row A of Table 2). Thus, to improve this error rate, we need Ô¨Årst of allto improve the coverage of the abbreviation-handling strategy.
A standard way to do this is to use the guessing heuristics in conjunction with a
list of known abbreviations. We decided to use the list of 270 abbreviations describedin Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e.,
only when a token was found in the list of 270 known abbreviations was it marked
as an abbreviation). This gave us an unexpectedly high error rate of about 12%, asdisplayed in row B of Table 2. When we investigated the reason for the high errorrate, we found that the majority of single letters and spans of single letters sepa-rated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus werenot present in our abbreviation list and therefore were not recognized as abbrevia-tions.
Such cases, however, are handled well by the abbreviation-guessing heuristics.
When we applied the abbreviation list together with the abbreviation-guessing heuris-tics (row C of Table 2), this gave us a very strong performance on the WSJ corpus:an error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This canbe explained by the fact that we collected our abbreviation list from a corpus of newsarticles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con-tained many abbreviations found in that corpus. The Brown corpus, in contrast, rangesacross several different domains and sublanguages, which makes it more difÔ¨Åcult tocompile a list from a single corpus to cover it.
6.1 Unigram DCA
The abbreviation-guessing heuristics supplemented with a list of abbreviations areaccurate, but they still can miss some abbreviations. For instance, if an abbreviationlike secorOkla. is followed by a capitalized word and is not listed in the list of
abbreviations, the guessing heuristics will not uncover them. We also would like toboost the abbreviation handling with a domain-independent method that enables thesystem to function even when the abbreviation list is not much of a help. Thus, inaddition to the list of known abbreviations and the guessing heuristics, we decided toapply the DCA as described below.
Each word of length four characters or less that is followed by a period is treated as
a potential abbreviation. First, the system collects unigrams of potential abbreviationsin unambiguous contexts from the document under processing. If a potential abbre-viation is used elsewhere in the document without a trailing period, we can concludethat it in fact is not an abbreviation but rather a regular word (nonabbreviation). Todecide whether a potential abbreviation is really an abbreviation, we look for contextsin which it is followed by a period and then by a lower-cased word, a number, or acomma.
For instance, the word Kong followed by a period and then by a capitalized word
cannot be safely classiÔ¨Åed as a regular word (nonabbreviation), and therefore it is apotential abbreviation. But if in the same document we detect a context lived in Hong
Kong in 1993 , this indicates that Kong in this document is normally written without
a trailing period and hence is not an abbreviation. Having established that, we canapply this information to nonevident contexts and classify Kong as a regular word
throughout the document. However, if we detect a context such as Kong., said, this
indicates that in this document, Kong is normally written with a trailing period and
hence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation
in all its occurrences within the same document.
6.2 Bigram DCA
The DCA relies on the assumption that there is a consistency in writing within thesame document. Different authors can write MrorDrwith or without a trailing period,
but we assume that the same author (the author of a particular document) writesit in the same way consistently. A situation can arise, however, in which the samepotential abbreviation is used as a regular word and as an abbreviation within the same
document. This is usually the case when an abbreviation coincides with a regular word,
for example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle
this problem, the system can collect from a document not only unigrams of potentialabbreviations in unambiguous contexts but also their bigrams with the precedingword. Of course, as in the case with unigrams, the bigrams are collected on the Ô¨Çyand completely automatically.
For instance, if the system Ô¨Ånds a context vitamin C is , it stores the bigram vita-
min C and the unigram Cwith the information that Cis a regular word. If in the
same document the system also detects a context John C. later said , it stores the bi-
gram John C and the unigram Cwith the information that Cis an abbreviation. Here
we have conÔ¨Çicting information for the word C: it was detected to act as a regu-
lar word and as an abbreviation within the same document, so there is not enoughinformation to resolve ambiguous cases purely using the unigram. Some cases, how-ever, can still be resolved on the basis of the bigrams. The system will assign Cas a
regular word (nonabbreviation) in an ambiguous context such as vitamin C. Research
because of the stored vitamin C bigram. Obviously from such a short context, it is
difÔ¨Åcult even for a human to make a conÔ¨Ådent decision, but the evidence gatheredfrom the entire document can inÔ¨Çuence this decision with a high degree of conÔ¨Å-dence.
6.3 Resulting Approach
When neither unigrams nor bigrams can help to resolve an ambiguous context for apotential abbreviation, the system decides in favor of the more frequent category forthat abbreviation. If the word Inwas detected to act as a regular word (preposition) Ô¨Åve
times in the current document and two times as abbreviation (for the state Indiana ), in
a context in which neither of the bigrams collected from the document can be applied,Inis assigned as a regular word (nonabbreviation). The last-resort strategy is to assign
all nonresolved cases as nonabbreviations.
Row D of Table 2 shows the results when we applied the abbreviation-guessing
heuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rateof the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impactwas somewhat smaller, about 18%. This can be explained by the fact that abbreviationsin the WSJ corpus have a much higher repetition rate, which is very important forthe DCA.
We also applied the DCA together with the lexical lookup and the guessing heuris-
tics. This reduced the error rate on abbreviation identiÔ¨Åcation by about 30% in com-parison with the list and guessing heuristics conÔ¨Åguration, as can be seen in row E ofTable 2.
7. Disambiguating Capitalized Words
The second key task of our approach is the disambiguation of capitalized words that
follow a potential sentence boundary punctuation sign. Apart from being an importantcomponent in the task of text normalization, information about whether or not acapitalized word that follows a period is a common word is crucial for the SBD task,as we showed in Section 3. We tackle capitalized words in a similar fashion as wetackled the abbreviations: through a document-centered approach that analyzes onthe Ô¨Çy the distribution of ambiguously capitalized words in the entire document. Thisis implemented as a cascade of simple strategies, which were brieÔ¨Çy described inMikheev (1999).
7.1 The Sequence Strategy
The Ô¨Årst DCA strategy for the disambiguation of ambiguous capitalized words is toexplore sequences of words extracted from contexts in which the same words are usedunambiguously with respect to their capitalization. We call this the sequence strategy .
The rationale behind this strategy is that if there is a phrase of two or more capitalizedwords starting from an unambiguous position (e.g., following a lower-cased word),the system can be reasonably conÔ¨Ådent that even when the same phrase starts from anunreliable position (e.g., after a period), all its words still have to be grouped togetherand hence are proper nouns. Moreover, this applies not just to the exact replication ofthe capitalized phrase, but to any partial ordering of its words of size two charactersor more preserving their sequence.
For instance, if a phrase Rocket Systems Development Co. is found in a document
starting from an unambiguous position (e.g., after a lower-cased word, a number, or acomma), the system collects it and also generates its partial-order subphrases: Rocket
Systems ,Rocket Systems Co. ,Rocket Co. ,Systems Development , etc. If then in the same
document Rocket Systems is found in an ambiguous position (e.g., after a period), the
system will assign the word Rocket as a proper noun because it is part of a multiword
proper name that was seen in the unambiguous context.
A span of capitalized words can also internally include alpha-numerals, abbrevia-
tions with internal periods, symbols, and lower-cased words of length three charactersor shorter. This enables the system to capture phrases like A&amp;M and The Phantom of
the Opera . Partial orders from such phrases are generated in a similar way, but with
the restriction that every generated subphrase should start and end with a capitalizedword.
The sequence strategy can also be applied to disambiguate common words. Since
in the case of common words the system cannot determine boundaries of a phrase,only bigrams of the lower-cased words with their following words are collected fromthe document. For instance, from a context continental suppliers of Mercury , the sys-
tem collects three bigrams: continental suppliers ,suppliers of , and of Mercury . When the
system encounters the phrase Continental suppliers after a period, it can now use the
information that in the previously stored bigram continental suppliers , the word token
continental was written lower-cased and therefore was unambiguously used as a com-
mon word. On this basis the system can assign the ambiguous capitalized word tokenContinental as a common word.
Row A of Table 3 displays the results obtained in the application of the sequence
strategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremelyuseful when we are dealing with names of organizations, since many of them aremultiword phrases composed of common words. For instance, the words Rocket and
Insurance can be used both as proper names and common words within the same
document. The sequence strategy maintains contexts of the usages of such wordswithin the same document, and thus it can disambiguate such usages in the ambiguouspositions matching surrounding words. And indeed, the error rate of this strategywhen applied to proper names was below 1%, with coverage of about 9‚Äì12%.
For tagging common words the sequence strategy was also very accurate (error
rate less than 0.3%), covering 17% of ambiguous capitalized common words on theWSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpuscan be explained by the fact that the documents in that corpus are in general longerthan those in the WSJ corpus, which enables more word bigrams to be collected froma document.
Dual application of the sequence strategy contributes to its robustness against po-
tential capitalization errors in the document. The negative evidence (not proper name)
Table 3
First part: Error rates of different individual strategies for capitalized-word disambiguation.Second part: Error rates of the overall cascading application of the individual strategies.
Strategy Word Class Error Rate Coverage
WSJ Brown WSJ Brown
A Sequence strategy Proper Names 0.12% 0.97% 12.6% 8.82%
Sequence strategy Common Words 0.28% 0.21% 17.68% 26.5%
B Frequent-list lookup strategy Proper Names 0.49% 0.16% 2.62% 6.54%
Frequent-list lookup strategy Common Words 0.21% 0.14% 64.62% 61.20%
C Single-word assignment strategy Proper Names 3.18% 1.96% 18.77% 34.13%
Single-word assignment strategy Common Words 6.51% 2.87% 3.07% 4.78%
D Cascading DCA Proper/Common 1.10% 0.76% 84.12% 91.76%E Cascading DCA Proper/Common 4.88% 2.83% 100.0% 100.0%
and lexical lookup
is used together with the positive evidence (proper name) and blocks assignment when
conÔ¨Çicts are found. For instance, if the system detects a capitalized phrase The President
in an unambiguous position, then the sequence strategy will treat the word theas part
of the proper name The President even when this phrase follows a period. If in the
same document, however, the system detects alternative evidence (e.g., the President ,
where theis not part of the proper name), it then will block as unsafe the assignment
ofTheas a proper name in ambiguous usages of The President .
7.2 Frequent-List Lookup Strategy
The frequent-list lookup strategy applies lookup of ambiguously capitalized wordsin two word lists. The Ô¨Årst list contains common words that are frequently foundin sentence-starting positions, and the other list contains the most frequent propernames. Both these lists can be compiled completely automatically, as explained insection 5. Thus, if an ambiguous capitalized word is found in the list of frequentsentence-starting common words, it is assigned as a common word, and if it is foundin the list of frequent proper names, it is assigned as a proper name. For instance,the word token Thewhen used after a period will be recognized as a common word,
because Theis a frequent sentence-starting common word. The Word token Japan in a
similar context will be recognized as a proper name, because Japan is a member of the
frequent-proper-name list.
Note, however, that this strategy is applied after the sequence strategy and thus, a
word listed in one of the lists will not necessarily be marked according to its list class.The list lookup assignment is applied only to the ambiguously capitalized words thathave not been handled by the sequence strategy.
Row B of Table 3 displays the results of the application of the frequent-list lookup
strategy to the Brown corpus and the WSJ corpus. The frequent-list lookup strategyproduced an error rate of less than 0.5%. A few wrong assignments came from phraseslike Mr. A and Mrs. Someone and words in titles like I‚Äôve Got a Dog , where A,Someone ,
and Iwere recognized as common words although they were tagged as proper nouns
in the text. The frequent-list lookup strategy is not very effective for proper names,where it covered under 7% of candidates in the Brown corpus and under 3% in theWSJ corpus, but it is extremely effective for common words: it covered over 60% ofambiguous capitalized common words.
7.3 Single-Word Assignment
The sequence strategy is accurate, but it covers only 9‚Äì12% of proper names in ambigu-ous positions. The frequent-list lookup strategy is mostly effective for common words.To boost the coverage on the proper name category, we introduced another DCAstrategy. We call this strategy single-word assignment , and it can be summarized as
follows: if a word in the current document is seen capitalized in an unambiguous po-sition and at the same time it is not used lower-cased anywhere in the document, thisword in this particular document is very likely to stand for a proper name even whenused capitalized in ambiguous positions. And conversely, if a word in the currentdocument is used only lower-cased (except in ambiguous positions), it is extremelyunlikely that this word will act as a proper name in an ambiguous position and thus,such a word can be marked as a common word.
Note that by the time single-word assignment is implemented, the sequence strat-
egy and the frequent-list lookup strategy have been already applied and all high-frequency sentence-initial words have been assigned. This ordering is important, be-cause even if a high-frequency common word is observed in a document only as aproper name (usually as part of a multiword proper name), it is still not safe to markit as a proper name in ambiguous positions.
Row C of Table 3 displays the results of the application of the single-word assign-
ment strategy to the Brown corpus and the WSJ corpus. The single-word assignmentstrategy is useful for proper-name identiÔ¨Åcation: although it is not as accurate as thesequence strategy, it still produces a reasonable error rate at the same time boosting thecoverage considerably (19‚Äì34%). On common words this method is not as effective,with an error rate as high as 6.61% on the WSJ corpus and a coverage below 5%.
The single-word-assignment strategy handles well the so-called unknown-word
problem, which arises when domain-speciÔ¨Åc lexica are missing from a general vocab-ulary. Since our system is not equipped with a general vocabulary but rather builds adocument-speciÔ¨Åc vocabulary on the Ô¨Çy,‚Äù important domain-speciÔ¨Åc words are iden-tiÔ¨Åed and treated similarly to all other words.
A generally difÔ¨Åcult case for the single-word assignment strategy arises when a
word is used both as a proper name and as a common word in the same document, es-pecially when one of these usages occurs only in an ambiguous position. For instance,in a document about steel, the only occurrence of Steel Company happened to start
a sentence. This produced an erroneous assignment of the word Steel as a common
word. Another example: in a document about the Acting Judge , the word acting in a
sentence Acting on behalf ...was wrongly classiÔ¨Åed as a proper name. These difÔ¨Åculties,
however, often are compensated for by the sequence strategy, which is applied priorto the single-word assignment strategy and tackles such cases using n-grams of words.
7.4 Quotes, Brackets, and ‚ÄúAfter Abbr.‚Äù Heuristic
Capitalized words in quotes and brackets do not directly contribute to our primarytask of sentence boundary disambiguation, but they still present a case of ambiguityfor the task of capitalized-word disambiguation. To tackle them we applied two simpleheuristics:
‚Ä¢If a single capitalized word is used in quotes or brackets it is a proper
noun (e.g., John (Cool) Lee ).
‚Ä¢If there is a lowercased word, a number, or a comma that is followed by
an opening bracket and then by a capitalized word, this capitalized wordis a proper noun (e.g., ...happened (Moscow News reported yesterday) but ...).
These heuristics are reasonably accurate: they achieved under 2% error rate on our
two test corpora, but they covered only about 6‚Äì7% of proper names.
When we studied the distribution of capitalized words after capitalized abbrevi-
ations, we uncovered an interesting empirical fact. A capitalized word that followsa capitalized abbreviation is almost certainly a proper name unless it is listed in thelist of frequent sentence-starting common words (i.e., it is not The,However , etc.). The
error rate of this heuristic is about 0.8% and, not surprisingly, in 99.5% of cases theabbreviation and the following proper name belonged to the same sentence. Naturally,the coverage of this ‚Äúafter abbr.‚Äù heuristic depends on the proportion of capitalizedabbreviations in the text. In our two corpora this heuristic disambiguated about 20%of ambiguous capitalized proper names.
7.5 Tagging Proper Names: The Overall Performance
In general, the cascading application of the above-described strategies achieved anerror rate of about 1%, but it left unclassiÔ¨Åed about 9% of ambiguous capitalizedwords in the Brown corpus and 15% of such words in the WSJ corpus. Row D ofTable 3 displays the results of the application of the cascading application of thecapitalized-word disambiguation strategies to the Brown corpus and the WSJ corpus.
For the proper-name category, the most productive strategy was single-word as-
signment, followed by the ‚Äúafter abbr.‚Äù strategy, and then the sequence strategy. Forcommon words, the most productive was the frequent-list lookup strategy, followedby the sequence strategy.
Since our system left unassigned 10‚Äì15% of ambiguous capitalized words, we have
to decide what to do with them. To keep our system simple and domain independent,we opted for the lexical-lookup strategy that we evaluated in Section 3. This strategy,of course, is not very accurate, but it is applied only to the unassigned words. Row Eof Table 3 displays the results of applying the lexical-lookup strategy after the DCAmethods. We see that the error rate went up in comparison to the DCA-only methodby more than three times (2.9% on the Brown corpus and 4.9% on the WSJ corpus),but no unassigned ambiguous capitalized words are left in the text.
8. Putting It All Together: Assigning Sentence Boundaries
After abbreviations have been identiÔ¨Åed and capitalized words have been classiÔ¨Åed
into proper names and common words, the system can carry out the assignmentsof sentence boundaries using the SBD rule set described in Section 3 and listed inAppendix A. This rule set makes use of the observation that if we have at our dis-posal unambiguous (but not necessarily correct) information as to whether a particularword that precedes a period is an abbreviation and whether the word that followsthis period is a proper name, then in mixed-case texts we can easily assign a pe-riod (and other potential sentence termination punctuation) as a sentence break ornot.
The only ambiguous outcome is generated by the conÔ¨Åguration in which an ab-
breviation is followed by a proper name. We decided to handle this case by applyinga crude and simple strategy of always resolving it as ‚Äúnot sentence boundary.‚Äù On onehand, this makes our method simple and robust, but on the other hand, it imposessome penalty on its performance.
Row A of Table 4 summarizes the upper bound for our SBD approach: when we
have entirely correct information on the abbreviations and proper names, as explainedin Section 3.1. There the erroneous assignments come only from the crude treatmentof abbreviations that are followed by proper names.
Table 4
Error rates measured on the SBD, capitalized-word disambiguation, and abbreviationidentiÔ¨Åcation tasks achieved by different methods described in this article.
Method Brown Corpus WSJ Corpus
SBD Capitalized Abbreviations SBD Capitalized Abbreviations
words words
A Upper bound 0.01% 0.0% 0.0% 0.13% 0.0% 0.0%B Lower bound 2.00% 7.40% 10.8% 4.10% 15.0% 9.6%C Best quoted 0.20% 3.15% ‚Äî 0.50% 4.72% ‚ÄîDDCA 0.28% 2.83% 0.8% 0.45% 4.88% 1.2%
E DCA (no abbreviations 0.65% 2.89% 8.9% 1.41% 4.92% 6.6%
lexicon)
F POS tagger 0.25% 3.15% 1.2% 0.39% 4.72% 2.1%G POS tagger + DCA 0.20% 1.87% 0.8% 0.31% 3.22% 1.2%
Row B of Table 4 summarizes the lower-bound results. The lower bound for our
approach was estimated by applying the lexical-lookup strategy for capitalized-worddisambiguation together with the abbreviation-guessing heuristics to feed the SBDrule set, as described in Section 3.2. Here we see a signiÔ¨Åcant impact of the infelicitiesin the disambiguation of capitalized words and abbreviations on the performance ofthe SBD rule set.
Row C of Table 4 summarizes the highest results known to us (for all three tasks)
produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-the-art machine learning and rule-based SBD systems achieve an error rate of 0.8‚Äì1.5%measured on the Brown corpus and the WSJ corpus. The best performance on theWSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The bestperformance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), whotrained a decision tree classiÔ¨Åer on a 25-million-word corpus. In the disambiguation ofcapitalized words, the most widespread method is POS tagging, which achieves abouta 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reportedin Mikheev (2000). We are not aware of any studies devoted to the identiÔ¨Åcation ofabbreviations with comprehensive evaluation on either the Brown corpus or the WSJcorpus.
In row D of Table 4, we summarized our main results: the results obtained by the
application of our SBD rule set, which uses the information provided by the DCA tocapitalized word disambiguation applied together with lexical lookup (as describedin Section 7.5), and the abbreviation-handling strategy, which included the guessingheuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). Ascan be seen in the table, the performance of this system is almost indistinguishablefrom the best previously quoted results. On proper-name disambiguation, it achieveda 2.83% error rate on the Brown corpus and a 4.88% error rate on the WSJ corpus.On the SBD task, it achieved a 0.28% error rate on the Brown corpus and a 0.45%error rate on the WSJ corpus. If we compare these results with the upper boundfor our SBD approach, we can see that the infelicities in proper-name and abbrevia-tion identiÔ¨Åcation introduced an increase of about 0.3% in the error rate on the SBDtask.
To test the adaptability of our approach to a completely new domain, we applied
our system in a conÔ¨Åguration in which it was notequipped with the list of 270 abbre-
viations, since this list is the only domain-sensitive resource in our system. The results
for this conÔ¨Åguration are summarized in row E of Table 4. The error rate increase of5‚Äì7% on the abbreviation handling introduced about a twofold increase in the SBDerror rate on the Brown corpus (a 0.65% error rate) and about a threefold increase onthe WSJ corpus (1.41%). But these results are still comparable to those of the majorityof currently used sentence splitters.
9. Detecting Limits for the DCA
Since our DCA method relies on the assumption that the words it tries to disam-
biguate occur multiple times in a document, its performance clearly should dependon the length of the document: very short documents possibly do not provide enoughdisambiguation clues, whereas very long documents possibly contain too many cluesthat cancel each other.
As noted in Section 2.1, the average length of the documents in the Brown corpus
is about 2,300 words. Also, the documents in that corpus are distributed very denselyaround their mean. Thus not much can be inferred about the dependency of the perfor-mance of the method on document length apart from the observation that documents2,000‚Äì3,000 words long are handled well by our approach. In the WSJ corpus, the aver-age length of the document is about 500 words, and therefore we could investigate theeffect of short documents on the performance. We divided documents into six groupsaccording to their length and plotted the error rate for the SBD and capitalized-worddisambiguation tasks as well as the number of documents in a group, as shown inFigure 2. As can be seen in the Ô¨Ågure, short documents (50 words or less) have thehighest average error rate both for the SBD task (1.63) and for the capitalized-worddisambiguation task (5.25). For documents 50 to 100 words long, the error rate is stilla bit higher than normal, and for longer documents the error rate stabilizes around1.5 for the capitalized-word disambiguation task and 0.3 for the SBD task. The errorrate on documents 2,000 words long and higher is almost identical to that registeredon the Brown corpus on documents of the same length.
Thus here we can conclude that the proposed approach tends not to be very
effective for documents shorter than 50 words (one to three sentences), but it handleswell documents up to 4,000 words long. Since our corpora did not contain documentssigniÔ¨Åcantly longer than that, we could not estimate whether or when the performanceof our method signiÔ¨Åcantly deteriorates on longer documents. We also evaluated theperformance of the method on different subcorpora of the Brown corpus: the mostdifÔ¨Åcult subdomains proved to be scientiÔ¨Åc texts, spoken-language transcripts, andjournalistic texts, whereas Ô¨Åction was the easiest genre for the system.
10. Incorporating DCA into a POS Tagger
To test our hypothesis that DCA can be used as a complement to a local-context
approach, we combined our main conÔ¨Åguration (evaluated in row D of Table 4) witha POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was alsotrained to disambiguate sentence boundaries.
10.1 Training a POS Tagger
In our markup convention (Section 2), periods are tokenized as separate tokens re-gardless of whether they stand for fullstops or belong to abbreviations. Consequentlya POS tagger can naturally treat them similarly to any other ambiguous words. Thereis, however, one difference in the implementation of such a tagger. Normally, a POS
x
x
x
xxxx
o
oo
oo
oocccc
c
c
cNumber of Documents
Cap.Word error rate
SBD error rate
50 100 200 500 1000 2000 3000123200600
Doc. LengthError RateNumber of Docs
400
Figure 2
Distribution of the error rate and the number of documents across the document length(measured in word tokens) in the WSJ corpus.
tagger operates on text spans that form a sentence. This requires resolving sentence
boundaries before tagging. We see no good reason, however, why such text spansshould necessarily be sentences, since the majority of tagging paradigms (e.g., HiddenMarkov Model [HMM] [Kupiec 1992], Brill‚Äôs [Brill 1995a], and MaxEnt [Ratnaparkhi1996]) do not attempt to parse an entire sentence and operate only in the local win-dow of two to three tokens. The only reason why taggers traditionally operate onthe sentence level is that a sentence naturally represents a text span in which POSinformation does not depend on the previous and following history.
This issue can be also addressed by breaking the text into short text spans at
positions where the previous tagging history does not affect current decisions. Forinstance, a bigram tagger operates within a window of two tokens, and thus a se-quence of word tokens can be terminated at an unambiguous word token, since thisunambiguous word token will be the only history used in tagging of the next token.At the same time since this token is unambiguous, it is not affected by the history.A trigram tagger operates within a window of three tokens, and thus a sequence ofword tokens can be terminated when two unambiguous words follow each other.
Using Penn Treebank with our tokenization convention (Section 2), we trained a
trigram HMM POS tagger. Words were clustered into ambiguity classes (Kupiec 1992)according to the sets of POS tags they can take on. The tagger predictions were basedon the ambiguity class of the current word, abbreviation/capitalization information,
and trigrams of POS tags:
P(t1...tnO1...On)=argmaxi=n/productdisplay
i=1P(Oi|ti)‚àóP(ti|ti‚àí1ti‚àí2ai‚àí1)
where tiis a disambiguated POS tag of the ith word, aiis the abbreviation Ô¨Çag of
the ith word, and Oiis the observation at the ith position, which in our case is
the ambiguity class the word belongs to, its capitalization, and its abbreviation Ô¨Çag(AmbClass
i,ai,Cap i). Since the abbreviation Ô¨Çag of the previous word strongly inÔ¨Çu-
ences period disambiguation, it was included in the standard trigram model.
We decided to train the tagger with the minimum of preannotated resources. First,
we used 20,000 tagged words to ‚Äúbootstrap‚Äù the training process, because purely un-supervised techniques, at least for the HMM class of taggers, yield lower precision.We also used our DCA system to assign capitalized words, abbreviations, and sen-tence breaks, retaining only cases assigned by the strategies with an accuracy not lessthan 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch[Baum and Petrie 1966] or Brill‚Äôs [Brill 1995b]) enable regularities to be induced forword classes which contain many entries, exploiting the fact that individual words thatbelong to a POS class occur in different ambiguity patterns. Counting all possible POScombinations in these ambiguity patterns over multiple patterns usually produces theright combinations as the most frequent. Periods as many other closed-class wordscannot be successfully covered by such technique.
After bootstrapping we applied the forward-backward (Baum-Welch) algorithm
(Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, with-
out using the annotation available in the Brown corpus and the WSJ corpus. Forevaluation purposes we trained (and bootstrapped) our tagger on the Brown corpusand applied it to the WSJ corpus and vice versa. We preferred this method to tenfoldcross-validation because it allowed us to produce only two tagging models instead oftwenty and also enabled us to test the tagger in harsher conditions, that is, when it isapplied to texts that are very distant from the ones on which it was trained.
The overall performance of the tagger was close to 96%, which is a bit lower
than the best quoted results. This can be accounted for by the fact that training andevaluation were performed on two very different text corpora, as explained above.The performance of the tagger on our target categories (periods and proper names)was very close to that of the DCA method, as can be seen in row F of Table 4.
10.2 POS Tagger and the DCA
We felt that the DCA method could be used as a complement to the POS tagger, sincethese techniques employ different types of information: in-document distribution andlocal context. Thus, a hybrid system can deliver at least two advantages. First, 10‚Äì15%of the ambiguous capitalized words unassigned by the DCA can be assigned usinga standard POS-tagging method based on the local syntactic context rather than theinaccurate lexical-lookup approach. Second, the local context can correct some of theerrors made by the DCA.
To implement this hybrid approach we incorporated the DCA system into the
POS tagger. We modiÔ¨Åed the tagger model by incorporating the DCA predictionsusing linear interpolation:
P(combined )= Œª‚àóP(tagger )+( 1‚àíŒª)‚àóP(DCA
Strategy )
where P(DCA Strategy )is the accuracy of a speciÔ¨Åc DCA strategy and P(tagger )is the
probability assigned by the tagger‚Äôs model. Although it was possible to estimate an
optimal value for Œªfrom the tagged corpus, we decided simply to set it to be 0.5 (i.e.,
giving similar weight to both sources of information). Instead of using the SBD ruleset described in Section 3, in this conÔ¨Åguration, period assignments were handled bythe tagger‚Äôs model.
Row G of Table 4 displays the results of the application of the hybrid system. We
see an improvement on proper-name recognition in comparison to the DCA or POS-tagging approaches (rows D and F) by about a 30‚Äì40% cut in the error rate: an overallerror rate of 1.87% on the Brown corpus and of 3.22% on the WSJ corpus. In turn thisenabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpusand a 0.31% error rate on the WSJ corpus, which corresponds to about a 20% cut inthe error rate in comparison to the DCA or the POS-tagging approaches alone.
Thus, although for applications that rely on POS tagging it probably makes more
sense to have a single system that assigns both POS tags and sentence boundaries,there is still a clear beneÔ¨Åt in using the DCA method because
‚Ä¢the DCA method incorporated into the POS tagger signiÔ¨Åcantly reduced
the error rate on the target categories (periods and proper names).
‚Ä¢the DCA method is domain independent, whereas taggers usually need
to be trained for each speciÔ¨Åc domain to obtain best results.
‚Ä¢the DCA system was used in resource preparation for training the tagger.
‚Ä¢the DCA system is signiÔ¨Åcantly faster than the tagger, does not require
resource development, and for tasks that do not require full POSinformation, it is a preferable solution.
So in general, the DCA method can be seen as an enhancer for a POS tagger and
also as a lightweight alternative to such a tagger when full POS information is notrequired.
11. Further Experiments
11.1 The Cache Extension
One of the features of the method advocated in this article is that the system collectssuggestive instances of usage for target words from each document, then applies thisinformation during the second pass through the document (actual processing), andthen ‚Äúforgets‚Äù what it has learned before handling another document. The main rea-son for not carrying over the information that has been inferred from one documentto process another document is that in general we do not know whether this newdocument comes from the same corpus as the Ô¨Årst document, and thus the regular-ities that have been identiÔ¨Åed in the Ô¨Årst document might not be useful, but ratherharmful, when applied to that new document. When we are dealing with documentsof reasonable length, this ‚Äúforgetful‚Äù behavior does not matter much, because suchdocuments usually contain enough disambiguation clues. As we showed in Section 8,however, when short documents of one to three sentences are being processed, quiteoften there are not enough disambiguation clues within the document itself, whichleads to inferior performance.
To improve the performance on short documents, we introduced a special caching
module that propagates some information identiÔ¨Åed in previously processed docu-ments to the processing of a new one. To propagate features of individual words fromone document to processing another one is a risky strategy, since words are very
ambiguous. Word sequences, however, are much more stable and can be propagated
across documents. We decided to accumulate in our cache all multiword proper namesand lower-cased word bigrams induced by the sequence strategy (Section 7.1). Theseword sequences are used by the sequence strategy exactly as are word sequences in-duced on the Ô¨Çy, and then the induced on-the-Ô¨Çy sequences are added to the cache.We also add to the cache the bigrams of abbreviations and regular words induced bythe abbreviation-handling module, as explained in Section 6. These bigrams are usedtogether with the bigrams induced on the Ô¨Çy. This strategy proved to be quite useful:it covered another 2% of unresolved cases (before applying the lexical lookup), withan error rate of less than 1%.
11.2 Handling Russian News
To test how easy it is to apply the DCA to a new language, we tested it on a corpusof British Broadcasting Corporation (BBC) news in Russian. We collected this corpusfrom the Internet /angbracketlefthttp://news.bbc.co.uk/hi/russian/world/default.htm /angbracketrightover a period of 30
days. This gave us a corpus of 300 short documents (one or two paragraphs each).We automatically created the supporting resources from 364,000 documents from theRussian corpus of the European Corpus Initiative, using the method described insection 5.
Since, unlike English, Russian is a highly inÔ¨Çected language, we had to deal with
the case normalization issue. Before using the DCA method, we applied a Russianmorphological processor (Mikheev and Liubushkina 1995) to convert each word inthe text to its main form: nominative case singular for nouns and adjectives, inÔ¨Ånitivefor verbs, etc. For words that could be normalized to several main forms (polysemy),when secondary forms of different words coincided, we retained all the main forms.Since the documents in the BBC news corpus were rather short, we applied the cachemodule, as described in Section 11.1. This allowed us to reuse information across thedocuments.
Russian proved to be a simpler case than English for our tasks. First, on average,
Russian words are longer than English words: thus the identiÔ¨Åcation of abbreviationsis simpler. Second, proper names in Russian coincide less frequently with commonwords; this makes the disambiguation of capitalized words in ambiguous positionseasier. The overall performance reached a 0.1% error rate on sentence boundaries anda 1.8% error rate on ambiguous capitalized words, with the coverage on both tasksat 100%.
12. Related Research
12.1 Research in Nonlocal Context
The use of nonlocal context and dynamic adaptation have been studied in languagemodeling for speech recognition. Kuhn and de Mori (1998) proposed a cache modelthat works as a kind of short-term memory by which the probability of the most re-cent nwords is increased over the probability of a general-purpose bigram or trigram
model. Within certain limits, such a model can adapt itself to changes in word frequen-cies, depending on the topic of the text passage. The DCA system is similar in spiritto such dynamic adaptation: it applies word n-grams collected on the Ô¨Çy from the
document under processing and favors them more highly than the default assignmentbased on prebuilt lists. But unlike the cache model, it uses a multipass strategy.
Clarkson and Robinson (1997) developed a way of incorporating standard n-grams
into the cache model, using mixtures of language models and also exponentially de-caying the weight for the cache prediction depending on the recency of the word‚Äôs last
occurrence. In our experiments we applied simple linear interpolation to incorporate
the DCA system into a POS tagger. Instead of decaying nonlocal information, we optedfor not propagating it from one document for processing of another. For handling verylong documents with our method, however, the information decay strategy seems tobe the right way to proceed.
Mani and MacMillan (1995) pointed out that little attention had been paid in the
named-entity recognition Ô¨Åeld to the discourse properties of proper names. They pro-posed that proper names be viewed as linguistic expressions whose interpretationoften depends on the discourse context, advocating text-driven processing rather thanreliance on pre-existing lists. The DCA outlined in this article also uses nonlocal dis-course context and does not heavily rely on pre-existing word lists. It has been appliednot only to the identiÔ¨Åcation of proper names, as described in this article, but also totheir classiÔ¨Åcation (Mikheev, Grover, and Moens 1998).
Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit
only one sense in a document or discourse (‚Äúone sense per discourse‚Äù). Since thenthis idea has been applied to several tasks, including word sense disambiguation(Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale,Church, and Yarowsky‚Äôs observation is also used in our DCA, especially for the iden-tiÔ¨Åcation of abbreviations. In capitalized-word disambiguation, however, we use thisassumption with caution and Ô¨Årst apply strategies that rely not just on single wordsbut on words together with their local contexts ( n-grams). This is similar to ‚Äúone sense
per collocation‚Äù idea of Yarowsky (1993).
The description of the EAGLE workbench for linguistic engineering (Baldwin et al.
1997) mentions a case normalization module that uses a heuristic in which a capitalizedword in an ambiguous position should be rewritten without capitalization if it is foundlower-cased in the same document. This heuristic also employs a database of bigramsand unigrams of lower-cased and capitalized words found in unambiguous positions.It is quite similar to our method for capitalized-word disambiguation. The descriptionof the EAGLE case normalization module provided by Baldwin et al. is, however, verybrief and provides no performance evaluation or other details.
12.2 Research in Text Preprocessing
12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD sys-
tems: rule based and machine learning. The rule-based systems use manually builtrules that are usually encoded in terms of regular-expression grammars supplementedwith lists of abbreviations, common words, proper names, etc. To put together a fewrules is fast and easy, but to develop a rule-based system with good performance isquite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen etal. 1995) contains a sentence-splitting module that employs over 100 regular-expressionrules written in Flex. Another well-acknowledged shortcoming of rule-based systemsis that such systems are usually closely tailored to a particular corpus or sublanguageand are not easily portable across domains.
Automatically trainable software is generally seen as a way of producing sys-
tems that are quickly retrainable for a new corpus, for a new domain, or even foranother language. Thus, the second class of SBD systems employs machine learningtechniques such as decision tree classiÔ¨Åers (Riley 1989), neural networks (Palmer andHearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Ma-chine learning systems treat the SBD task as a classiÔ¨Åcation problem, using featuressuch as word spelling, capitalization, sufÔ¨Åx, and word class found in the local con-text of a potential sentence-terminating punctuation sign. Although training of such
systems is completely automatic, the majority of machine learning approaches to the
SBD task require labeled examples for training. This implies an investment in the dataannotation phase.
The main difference between the existing machine learning and rule-based meth-
ods for the SBD task and our approach is that we decomposed the SBD task intoseveral subtasks. We decided to tackle the SBD task through the disambiguation ofthe period preceding and following words and then feed this information into a verysimple SBD rule set. In contrast, the standard practice in building SBD software is todisambiguate conÔ¨Ågurations of a period with its ambiguous local context in a singlestep, either by encoding disambiguation clues into the rules or inferring a classiÔ¨Åerthat accounts for the ambiguity of the words on the left and on the right of the period.
Our approach to SBD is closer in spirit to machine learning methods because its
retargeting does not require rule reengineering and can be done completely automat-ically. Unlike traditional machine learning SBD approaches, however, our approachdoes not require annotated data for training.
12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words
is usually handled by POS taggers, which treat capitalized words in the same wayas other categories, that is, by accounting for the immediate syntactic context andusing estimates collected from a training corpus. As Church (1988) rightly pointedout, however, ‚ÄúProper nouns and capitalized words are particularly problematic: somecapitalized words are proper nouns and some are not. Estimates from the BrownCorpus can be misleading. For example, the capitalized word ‚ÄòActs‚Äô is found twice inthe Brown Corpus, both times as a proper noun (in a title). It would be misleading toinfer from this evidence that the word ‚ÄòActs‚Äô is always a proper noun.‚Äù
In the information extraction Ô¨Åeld, the disambiguation of ambiguous capitalized
words has always been tightly linked to the classiÔ¨Åcation of proper names into seman-tic classes such as person name, location, and company name. Named-entity recogni-tion systems usually use sets of complex hand-crafted rules that employ a gazetteerand a local context (Krupa and Hausman 1998). In some systems such dependenciesare learned from labeled examples (Bikel et al. 1997). The advantage of the named-entity approach is that the system not only identiÔ¨Åes proper names but also determinestheir semantic class. The disadvantage is in the cost of building a wide-coverage set ofcontextual clues manually or producing annotated training data. Also, the contextualclues are usually highly speciÔ¨Åc to the domain and text genre, making such systemsvery difÔ¨Åcult to port.
Both POS taggers and named-entity recognizers are normally built using the local-
context paradigm. In contrast, we opted for a method that relies on the entire distri-bution of a word in a document. Although it is possible to train some classes of POStaggers without supervision, this usually leads to suboptimal performance. Thus themajority of taggers are trained using at least some labeled data. Named-entity recog-nition systems are usually hand-crafted or trained from labeled data. As was shownabove, our method does not require supervised training.
12.2.3 Disambiguation of Abbreviations. Not much information has been published
on abbreviation identiÔ¨Åcation. One of the better-known approaches is described inGrefenstette and Tapanainen (1994), which suggested that abbreviations Ô¨Årst be ex-tracted from a corpus using abbreviation-guessing heuristics akin to those describedin Section 6 and then reused in further processing. This is similar to our treatment ofabbreviation handling, but our strategy is applied on the document rather than corpuslevel. The main reason for restricting abbreviation discovery to a single document is
that this does not presuppose the existence of a corpus in which the current document
is similar to other documents.
Park and Byrd (2001) recently described a hybrid method for Ô¨Ånding abbrevia-
tions and their deÔ¨Ånitions. This method Ô¨Årst applies an ‚Äúabbreviation recognizer‚Äù thatgenerates a set of ‚Äúcandidate abbreviations‚Äù for a document. Then for this set of can-didates the system tries to Ô¨Ånd in the text their deÔ¨Ånitions (e.g., United Kingdom for
UK). The abbreviation recognizer for these purposes is allowed to overgenerate signif-
icantly. There is no harm (apart from the performance issues) in proposing too manycandidate abbreviations, because only those that can be linked to their deÔ¨Ånitions willbe retained. Therefore the abbreviation recognizer treats as a candidate any token oftwo to ten characters that contains at least one capital letter. Candidates then are Ô¨Ål-tered through a set of known common words and proper names. At the same timemany good abbreviations and acronyms are Ô¨Åltered out because not for all of themwill deÔ¨Ånitions exist in the current document.
In our task we are interested in Ô¨Ånding all and only abbreviations that end with
a period (proper abbreviations rather than acronyms), regardless of whether they canbe linked to their deÔ¨Ånitions in the current document or not. Therefore, in our methodwe cannot tolerate candidate overgeneration or excessive Ô¨Åltering and had to developmore selective methods for Ô¨Ånding abbreviations in text.</corps>
  <conclusion>Aucune conclusion</conclusion>
  <discussion>In this article we presented an approach that tackles three important aspects of text nor-
malization: sentence boundary disambiguation, disambiguation of capitalized wordswhen they are used in positions where capitalization is expected, and identiÔ¨Åcation ofabbreviations. The major distinctive features of our approach can be summarized asfollows:
‚Ä¢We tackle the sentence boundary task only after we have fully
disambiguated the word on the left and the word on the right of apotential sentence boundary punctuation sign.
‚Ä¢To disambiguate capitalized words and abbreviations, we use
information distributed across the entire document rather than theirimmediate local context.
‚Ä¢Our approach does not require manual rule construction or data
annotation for training. Instead, it relies on four word lists that can begenerated completely automatically from a raw (unlabeled) corpus.
In this approach we do not try to resolve each ambiguous word occurrence individu-
ally. Instead, the system scans the entire document for the contexts in which the wordsin question are used unambiguously, and this gives it grounds, acting by analogy, forresolving ambiguous contexts.
We deliberately shaped our approach so that it largely does not rely on precom-
piled statistics, because the most interesting events are inherently infrequent and henceare difÔ¨Åcult to collect reliable statistics for. At the same time precompiled statisticswould be smoothed across multiple documents rather than targeted to a speciÔ¨Åc docu-ment. By collecting suggestive instances of usage for target words from each particulardocument on the Ô¨Çy, rather than relying on preacquired resources smoothed across theentire document collection, our approach is robust to domain shifts and new lexicaand closely targeted to each document.
A signiÔ¨Åcant advantage of this approach is that it can be targeted to new domains
completely automatically, without human intervention. The four word lists that oursystem uses for its operation can be generated automatically from a raw corpus andrequire no human annotation. Although some SBD systems can be trained on relativelysmall sets of labeled examples, their performance in such cases is somewhat lower thantheir optimal performance. For instance, Palmer and Hearst (1997) report that the SATZsystem (decision tree variant) was trained on a set of about 800 labeled periods, whichcorresponds to a corpus of about 16,000 words. This is a relatively small training setthat can be manually marked in a few hours‚Äô time. But the error rate (1.5%) of thedecision tree classiÔ¨Åer trained on this small sample was about 50% higher than thatwhen trained on 6,000 labeled examples (1.0%).
The performance of our system does not depend on the availability of labeled
training examples. For its ‚Äútraining,‚Äù it uses a raw (unannotated in any way) corpusof texts. Although it needs such a corpus to be relatively large (a few hundred thousandwords), this is normally not a problem, since when the system is targeted to a newdomain, such a corpus is usually already available at no extra cost. Therefore there is notrade-off between the amount of human labor and the performance of the system. Thisnot only makes retargeting of such system easier but also enables it to be operationalin a completely autonomous way: it needs only to be pointed to texts from a newdomain, and then it can retarget itself automatically.
Although the DCA requires two passes through a document, the simplicity of the
underlying algorithms makes it reasonably fast. It processes about 3,000 words persecond using a Pentium II 400 MHz processor. This includes identiÔ¨Åcation of abbre-viations, disambiguation of capitalized words, and then disambiguation of sentenceboundaries. This is comparable to the speed of other preprocessing systems.
3The oper-
ational speed is about 10% higher than the training speed because, apart from applyingthe system to the training corpus, training also involves collecting, thresholding, andsorting of the word lists‚Äîall done automatically but at extra time cost. Training onthe 300,000-word NYT text collection took about two minutes.
Despite its simplicity, the performance of our approach was on the level with
the previously highest reported results on the same test collections. The error rateon sentence boundaries in the Brown corpus was not signiÔ¨Åcantly worse than thelowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpusour system performed slightly better than the combination of the Alembic and SATZsystems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Althoughthese error rates seem to be very small, they are quite signiÔ¨Åcant. Unlike general POStagging, in which it is unfair to expect an error rate of less than 2% because even humanannotators have a disagreement rate of about 3%, sentence boundaries are much lessambiguous (with a disagreement of about 1 in 5,000). This shows that an error rateof 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,one error in 200 periods means that there is one error in every two documents in theBrown corpus and one error in every four documents in the WSJ corpus.
With all its strong points, there are a number of restrictions to the proposed ap-
proach. First, in its present form it is suitable only for processing of reasonably ‚Äúwell-behaved‚Äù texts that consistently use capitalization (mixed case) and do not containmuch noisy data. Thus, for instance, we do not expect our system to perform wellon single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on
3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average
sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha3000).
optical character reader‚Äìgenerated texts. We noted in Section 8 that very short doc-
uments of one to three sentences also present a difÔ¨Åculty for our approach. This iswhere robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS taggerreported in Mikheev (2000), which do not heavily rely on word capitalization and arenot sensitive to document length, have an advantage.
Our DCA uses information derived from the entire document and thus can be
used as a complement to approaches based on the local context. When we incorpo-rated the DCA system into a POS tagger (Section 8), we measured a 30‚Äì35% cut in theerror rate on proper-name identiÔ¨Åcation in comparison to DCA or the POS-taggingapproaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20%error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre-sponds to about a 20% cut in the error rate in comparison to DCA or the POS-taggingapproaches alone.
We also investigated the portability of our approach to other languages and ob-
tained encouraging results on a corpus of news in Russian. This strongly suggests thatthe DCA method can be applied to the majority of European languages, since theyshare the same principles of capitalization and word abbreviation. Obvious exceptions,though, are German and some Scandinavian languages in which capitalization is usedfor things other than proper-name and sentence start signaling. This does not mean,however, that the DCA in general is not suitable for preprocessing of German texts‚Äîitjust needs to be applied with different disambiguation clues.
Initially the system described in this article was developed as a text normalization
module for a named-entity recognition system (Mikheev, Grover, and Moens 1998) thatparticipated in MUC-7. There the ability to identify proper names with high accuracyproved to be instrumental in enabling the entire system to achieve a very high level ofperformance. Since then this text normalization module has been used in several othersystems, and its ability to be adapted easily to new domains enabled rapid develop-ment of text analysis capabilities in medical, legal, and law enforcement domains.</discussion>
  <bibliographie>Aberdeen, John S., John D. Burger, David S.
Day, Lynette Hirschman, PatriciaRobinson, and Marc Vilain. 1995. ‚ÄúMitre:Description of the alembic system usedfor MUC-6.‚Äù In Proceedings of the Sixth
Message Understanding Conference (MUC-6) ,
Columbia, Maryland, November. MorganKaufmann.
Baldwin, Breck, Christine Doran, Jeffrey
Reynar, Michael Niv, Bangalore Srinivas,and Mark Wasson. 1997. ‚ÄúEAGLE: Anextensible architecture for generallinguistic engineering.‚Äù In Proceedings of
Computer-Assisted Information Searching onInternet (RIAO ‚Äô97) , Montreal, June.
Baum, Leonard E. and Ted Petrie. 1966.
Statistical inference for probabilisticfunctions of Ô¨Ånite Markov chains. Annals
of Mathematical Statistics 37:1559‚Äì1563.
Bikel, Daniel, Scott Miller, Richard
Schwartz, and Ralph Weischedel. 1997.‚ÄúNymble: A high performance learningname-Ô¨Ånder.‚Äù In Proceedings of the Fifth
Conference on Applied Natural LanguageProcessing (ANLP‚Äô97) , pages 194‚Äì200.
Washington, D.C., Morgan Kaufmann.
Brill, Eric. 1995a. Transformation-based
error-driven learning and naturallanguage parsing: A case study inpart-of-speech tagging. Computational
Linguistics 21(4):543‚Äì565.
Brill, Eric. 1995b. ‚ÄúUnsupervised learning of
disambiguation rules for part of speechtagging.‚Äù In David Yarovsky and KennethChurch, editors, Proceedings of the Third
Workshop on Very Large Corpora , pages
1‚Äì13, Somerset, New Jersey. Associationfor Computational Linguistics.
Burnage, Gavin. 1990. CELEX: A Guide for
Users . Centre for Lexical Information,
Nijmegen, Netherlands.
Chinchor, Nancy. 1998. ‚ÄúOverview of
MUC-7.‚Äù In Seventh Message Understanding
Conference (MUC-7): Proceedings of aConference Held in Fairfax , April. Morgan
Kaufmann.
Church, Kenneth. 1988. ‚ÄúA stochastic parts
program and noun-phrase parser forunrestricted text.‚Äù In Proceedings of the
Second ACL Conference on Applied NaturalLanguage Processing (ANLP‚Äô88) , pages
136‚Äì143, Austin, Texas.
Church, Kenneth. 1995. ‚ÄúOne term or two?‚Äù
InSIGIR‚Äô95, Proceedings of the 18th Annual
International ACM SIGIR Conference onResearch and Development in InformationRetrieval , pages 310‚Äì318, Seattle,
Washington, July. ACM Press.
Clarkson, Philip and Anthony J. Robinson.
1997. ‚ÄúLanguage model adaptation usingmixtures and an exponentially decayingcache.‚Äù In Proceedings IEEE International
Conference on Speech and Signal Processing ,
Munich, Germany.
Cucerzan, Silviu and David Yarowsky. 1999.
‚ÄúLanguage independent named entityrecognition combining morphological andcontextual evidence.‚Äù In Proceedings of
Joint SIGDAT Conference on EMNLP andVLC.
Francis, W. Nelson and Henry Kucera. 1982.
Frequency Analysis of English Usage: Lexiconand Grammar . Houghton MifÔ¨Çin, New
York.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. ‚ÄúOne sense perdiscourse.‚Äù In Proceedings of the Fourth
DARP A Speech and Natural LanguageWorkshop , pages 233‚Äì237.
Grefenstette, Gregory and Pasi Tapanainen.
1994. ‚ÄúWhat is a word, what is asentence? Problems of tokenization.‚Äù InThe Proceedings of Third Conference onComputational Lexicography and TextResearch (COMPLEX‚Äô94) , Budapest,
Hungary.
Krupka, George R. and Kevin Hausman.
1998. Isoquest Inc.: Description of thenetowl extractor system as used forMUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7) ,
Fairfax, VA. Morgan Kaufmann.
Kuhn, Roland and Renato de Mori. 1998. A
cache-based natural language model forspeech recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence12:570‚Äì583.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.Computer Speech and Language .
Mani, Inderjeet and T. Richard MacMillan.
1995. ‚ÄúIdentifying unknown propernames in newswire text.‚Äù In B. Boguraev
and J. Pustejovsky, editors, Corpus
Processing for Lexical Acquisition . MIT Press,
Cambridge, Massachusetts, pages 41‚Äì59.
Marcus, Mitchell, Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building alarge annotated corpus of English: ThePenn treebank. Computational Linguistics
19(2):313‚Äì329.
Mikheev, Andrei. 1997. Automatic rule
induction for unknown word guessing.Computational Linguistics 23(3):405‚Äì423.
Mikheev, Andrei. 1999. A knowledge-free
method for capitalized worddisambiguation. In Proceedings of the 37th
Conference of the Association forComputational Linguistics (ACL‚Äô99) , pages
159‚Äì168, University of Maryland, CollegePark.
Mikheev, Andrei. 2000. ‚ÄúTagging sentence
boundaries.‚Äù In Proceedings of the First
Meeting of the North American Chapter of theComputational Linguistics (NAACL‚Äô2000) ,
pages 264‚Äì271, Seattle, Washington.Morgan Kaufmann.
Mikheev, Andrei, Clair Grover, and Colin
Matheson. 1998. TTT: Text Tokenisation Tool .
Language Technology Group, Universityof Edinburgh. Available athttp://www.ltg.ed.ac.uk/software/ttt/index.html.
Mikheev, Andrei, Clair Grover, and Marc
Moens. 1998. Description of the ltgsystem used for MUC-7. In Seventh
Message Understanding Conference(MUC‚Äì7): Proceedings of a Conference Held inFairfax , Virginia. Morgan Kaufmann.
Mikheev, Andrei and Liubov Liubushkina.
1995. Russian morphology: Anengineering approach. Natural Language
Engineering 1(3):235‚Äì260.
Palmer, David D. and Marti A. Hearst. 1994.
‚ÄúAdaptive sentence boundarydisambiguation.‚Äù In Proceedings of the
Fourth ACL Conference on Applied NaturalLanguage Processing (ANLP‚Äô94) , pages
78‚Äì83, Stuttgart, Germany, October.Morgan Kaufmann.
Palmer, David D. and Marti A. Hearst. 1997.
Adaptive multilingual sentence boundarydisambiguation. Computational Linguistics
23(2):241‚Äì269.
Park, Youngja and Roy J. Byrd. 2001.
‚ÄúHybrid text mining for Ô¨Åndingabbreviations and their deÔ¨Ånitions.‚Äù InProceedings of the Conference on EmpiricalMethods in Natural Language Processing(EMLP‚Äô01) , pages 16‚Äì19, Washington,
D.C. Morgan Kaufmann.
Ratnaparkhi, Adwait. 1996. ‚ÄúA maximum
entropy model for part-of-speech
tagging.‚Äù In Proceedings of Conference on
Empirical Methods in Natural LanguageProcessing , pages 133‚Äì142, University of
Pennsylvania, Philadelphia.
Reynar, Jeffrey C. and Adwait Ratnaparkhi.
1997. ‚ÄúA maximum entropy approach toidentifying sentence boundaries.‚Äù InProceedings of the Fifth ACL Conference onApplied Natural Language Processing(ANLP‚Äô97) , pages 16‚Äì19. Morgan
Kaufmann.
Riley, Michael D. 1989. ‚ÄúSome applications
of tree-based modeling to speech andlanguage indexing.‚Äù In Proceedings of the
DARP A Speech and Natural LanguageWorkshop , pages 339‚Äì352. Morgan
Kaufmann.
Yarowsky, David. 1993. ‚ÄúOne sense per
collocation.‚Äù In Proceedings of ARP A
Human Language Technology Workshop ‚Äô93 ,
pages 266‚Äì271, Princeton, New Jersey.
Yarowsky, David. 1995. ‚ÄúUnsupervised
word sense disambiguation rivalingsupervised methods.‚Äù In Meeting of the
Association for Computational Linguistics(ACL‚Äô95) , pages 189‚Äì19</bibliographie>

  <preamble>Mikolov.pdf</preamble>
  <titre>EfÔ¨Åcient Estimation of Word Representations in
Vector Space</titre>
  <auteurs>
    <auteur>
      <name>Tomas Mikolov</name>
      <mail>tmikolov@google.com</mail>
      <affiliation>Google Inc., Mountain View, CA</affiliation>
    </auteur>
    <auteur>
      <name>Kai Chen</name>
      <mail>kaichen@google.com</mail>
      <affiliation>Google Inc., Mountain View, CA</affiliation>
    </auteur>
    <auteur>
      <name>Greg Corrado</name>
      <mail>gcorrado@google.com</mail>
      <affiliation>Google Inc., Mountain View, CA</affiliation>
    </auteur>
    <auteur>
      <name>Jeffrey Dean</name>
      <mail>jeff@google.com</mail>
      <affiliation>Google Inc., Mountain View, CA</affiliation>
    </auteur>
  </auteurs>
  <abstract>We propose two novel model architectures for computing continuous vector repre-
sentations of words from very large data sets. The quality of these representations
is measured in a word similarity task, and the results are compared to the previ-
ously best performing techniques based on different types of neural networks. We
observe large improvements in accuracy at much lower computational cost, i.e. it
takes less than a day to learn high quality word vectors from a 1.6 billion words
data set. Furthermore, we show that these vectors provide state-of-the-art perfor-
mance on our test set for measuring syntactic and semantic word similarities.</abstract>
  <introduction>
Many current NLP systems and techniques treat words as atomic units - there is no notion of similar-
ity between words, as these are represented as indices in a vocabulary. This choice has several good
reasons - simplicity, robustness and the observation that simple models trained on huge amounts of
data outperform complex systems trained on less data. An example is the popular N-gram model
used for statistical language modeling - today, it is possible to train N-grams on virtually all available
data (trillions of words [3]).
However, the simple techniques are at their limits in many tasks. For example, the amount of
relevant in-domain data for automatic speech recognition is limited - the performance is usually
dominated by the size of high quality transcribed speech data (often just millions of words). In
machine translation, the existing corpora for many languages contain only a few billions of words
or less. Thus, there are situations where simple scaling up of the basic techniques will not result in
any signiÔ¨Åcant progress, and we have to focus on more advanced techniques.
With progress of machine learning techniques in recent years, it has become possible to train more
complex models on much larger data set, and they typically outperform the simple models. Probably
the most successful concept is to use distributed representations of words [10]. For example, neural
network based language models signiÔ¨Åcantly outperform N-gram models [1, 27, 17].
1.1 Goals of the Paper
The main goal of this paper is to introduce techniques that can be used for learning high-quality word
vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As
far as we know, none of the previously proposed architectures has been successfully trained on more
than a few hundred of millions of words, with a modest dimensionality of the word vectors between
50 - 100.
We use recently proposed techniques for measuring the quality of the resulting vector representa-
tions, with the expectation that not only will similar words tend to be close to each other, but that
words can have multiple degrees of similarity [20]. This has been observed earlier in the context
of inÔ¨Çectional languages - for example, nouns can have multiple word endings, and if we search for
similar words in a subspace of the original vector space, it is possible to Ô¨Ånd words that have similar
endings [13, 14].
Somewhat surprisingly, it was found that similarity of word representations goes beyond simple
syntactic regularities. Using a word offset technique where simple algebraic operations are per-
formed on the word vectors, it was shown for example that vector(‚ÄùKing‚Äù) - vector(‚ÄùMan‚Äù) + vec-
tor(‚ÄùWoman‚Äù) results in a vector that is closest to the vector representation of the word Queen [20].
In this paper, we try to maximize accuracy of these vector operations by developing new model
architectures that preserve the linear regularities among words. We design a new comprehensive test
set for measuring both syntactic and semantic regularities1, and show that many such regularities
can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends
on the dimensionality of the word vectors and on the amount of the training data.
1.2 Previous Work
Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model
architecture for estimating neural network language model (NNLM) was proposed in [1], where a
feedforward neural network with a linear projection layer and a non-linear hidden layer was used to
learn jointly the word vector representation and a statistical language model. This work has been
followed by many others.
Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are
Ô¨Årst learned using neural network with a single hidden layer. The word vectors are then used to train
the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this
work, we directly extend this architecture, and focus just on the Ô¨Årst step where the word vectors are
learned using a simple model.
It was later shown that the word vectors can be used to signiÔ¨Åcantly improve and simplify many
NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different
model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word
vectors were made available for future research and comparison2. However, as far as we know, these
architectures were signiÔ¨Åcantly more computationally expensive for training than the one proposed
in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices
are used [23].
</introduction>
  <corps>Many different types of models were proposed for estimating continuous representations of words,
including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).
In this paper, we focus on distributed representations of words learned by neural networks, as it was
previously shown that they perform signiÔ¨Åcantly better than LSA for preserving linear regularities
among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.
Similar to [18], to compare different model architectures we deÔ¨Åne Ô¨Årst the computational complex-
ity of a model as the number of parameters that need to be accessed to fully train the model. Next,
we will try to maximize the accuracy, while minimizing the computational complexity.
1The test set is available at www.fit.vutbr.cz/ Àúimikolov/rnnlm/word-test.v1.txt
2http://ronan.collobert.com/senna/
http://metaoptimize.com/projects/wordreprs/
http://www.fit.vutbr.cz/ Àúimikolov/rnnlm/
http://ai.stanford.edu/ Àúehhuang/
For all the following models, the training complexity is proportional to
O=ETQ; (1)
where Eis number of the training epochs, Tis the number of the words in the training set and Qis
deÔ¨Åned further for each model architecture. Common choice is E= 3 50andTup to one billion.
All models are trained using stochastic gradient descent and backpropagation [26].
2.1 Feedforward Neural Net Language Model (NNLM)
The probabilistic feedforward neural network language model has been proposed in [1]. It consists
of input, projection, hidden and output layers. At the input layer, Nprevious words are encoded
using 1-of- Vcoding, where Vis size of the vocabulary. The input layer is then projected to a
projection layer Pthat has dimensionality ND, using a shared projection matrix. As only N
inputs are active at any given time, composition of the projection layer is a relatively cheap operation.
The NNLM architecture becomes complex for computation between the projection and the hidden
layer, as values in the projection layer are dense. For a common choice of N= 10 , the size of the
projection layer ( P) might be 500 to 2000, while the hidden layer size His typically 500 to 1000
units. Moreover, the hidden layer is used to compute probability distribution over all the words in the
vocabulary, resulting in an output layer with dimensionality V. Thus, the computational complexity
per each training example is
Q=ND+NDH+HV; (2)
where the dominating term is HV. However, several practical solutions were proposed for
avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized
models completely by using models that are not normalized during training [4, 9]. With binary tree
representations of the vocabulary, the number of output units that need to be evaluated can go down
to around log2(V). Thus, most of the complexity is caused by the term NDH.
In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary
tree. This follows previous observations that the frequency of words works well for obtaining classes
in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and
this further reduces the number of output units that need to be evaluated: while balanced binary tree
would require log2(V)outputs to be evaluated, the Huffman tree based hierarchical softmax requires
only about log2(Unigram perplexity (V)). For example when the vocabulary size is one million
words, this results in about two times speedup in evaluation. While this is not crucial speedup for
neural network LMs as the computational bottleneck is in the NDHterm, we will later propose
architectures that do not have hidden layers and thus depend heavily on the efÔ¨Åciency of the softmax
normalization.
2.2 Recurrent Neural Net Language Model (RNNLM)
Recurrent neural network based language model has been proposed to overcome certain limitations
of the feedforward NNLM, such as the need to specify the context length (the order of the model N),
and because theoretically RNNs can efÔ¨Åciently represent more complex patterns than the shallow
neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and
output layer. What is special for this type of model is the recurrent matrix that connects hidden
layer to itself, using time-delayed connections. This allows the recurrent model to form some kind
of short term memory, as information from the past can be represented by the hidden layer state that
gets updated based on the current input and the state of the hidden layer in the previous time step.
The complexity per training example of the RNN model is
Q=HH+HV; (3)
where the word representations Dhave the same dimensionality as the hidden layer H. Again, the
termHVcan be efÔ¨Åciently reduced to Hlog2(V)by using hierarchical softmax. Most of the
complexity then comes from HH.
2.3 Parallel Training of Neural Networks
To train models on huge data sets, we have implemented several models on top of a large-scale
distributed framework called DistBelief [6], including the feedforward NNLM and the new models
proposed in this paper. The framework allows us to run multiple replicas of the same model in
parallel, and each replica synchronizes its gradient updates through a centralized server that keeps
all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with
an adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use
one hundred or more model replicas, each using many CPU cores at different machines in a data
center.
3 New Log-linear Models
In this section, we propose two new model architectures for learning distributed representations
of words that try to minimize computational complexity. The main observation from the previous
section was that most of the complexity is caused by the non-linear hidden layer in the model. While
this is what makes neural networks so attractive, we decided to explore simpler models that might
not be able to represent the data as precisely as neural networks, but can possibly be trained on much
more data efÔ¨Åciently.
The new architectures directly follow those proposed in our earlier work [13, 14], where it was
found that neural network language model can be successfully trained in two steps: Ô¨Årst, continuous
word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these
distributed representations of words. While there has been later substantial amount of work that
focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.
Note that related models have been proposed also much earlier [26, 8].
3.1 Continuous Bag-of-Words Model
The Ô¨Årst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden
layer is removed and the projection layer is shared for all words (not just the projection matrix);
thus, all words get projected into the same position (their vectors are averaged). We call this archi-
tecture a bag-of-words model as the order of words in the history does not inÔ¨Çuence the projection.
Furthermore, we also use words from the future; we have obtained the best performance on the task
introduced in the next section by building a log-linear classiÔ¨Åer with four future and four history
words at the input, where the training criterion is to correctly classify the current (middle) word.
Training complexity is then
Q=ND+Dlog2(V): (4)
We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous
distributed representation of the context. The model architecture is shown at Figure 1. Note that the
weight matrix between the input and the projection layer is shared for all word positions in the same
way as in the NNLM.
3.2 Continuous Skip-gram Model
The second architecture is similar to CBOW, but instead of predicting the current word based on the
context, it tries to maximize classiÔ¨Åcation of a word based on another word in the same sentence.
More precisely, we use each current word as an input to a log-linear classiÔ¨Åer with continuous
projection layer, and predict words within a certain range before and after the current word. We
found that increasing the range improves quality of the resulting word vectors, but it also increases
the computational complexity. Since the more distant words are usually less related to the current
word than those close to it, we give less weight to the distant words by sampling less from those
words in our training examples.
The training complexity of this architecture is proportional to
Q=C(D+Dlog2(V)); (5)
where Cis the maximum distance of the words. Thus, if we choose C= 5, for each training word
we will select randomly a number Rin range &lt;1;C &gt; , and then use Rwords from history and
w(t-2)
w(t+1)w(t-1)
w(t+2)w(t)SUM       INPUT         PROJECTION         OUTPUT
w(t)          INPUT         PROJECTION      OUTPUT
w(t-2)
w(t-1)
w(t+1)
w(t+2)
                   CBOW                                                   Skip-gramFigure 1: New model architectures. The CBOW architecture predicts the current word based on the
context, and the Skip-gram predicts surrounding words given the current word.
Rwords from the future of the current word as correct labels. This will require us to do R2
word classiÔ¨Åcations, with the current word as input, and each of the R+Rwords as output. In the
following experiments, we use C= 10 .
4 Results
To compare the quality of different versions of word vectors, previous papers typically use a table
showing example words and their most similar words, and understand them intuitively. Although
it is easy to show that word France is similar to Italy and perhaps some other countries, it is much
more challenging when subjecting those vectors in a more complex similarity task, as follows. We
follow previous observation that there can be many different types of similarities between words, for
example, word bigis similar to bigger in the same sense that small is similar to smaller . Example
of another type of relationship can be word pairs big - biggest andsmall - smallest [20]. We further
denote two pairs of words with the same relationship as a question, as we can ask: ‚ÄùWhat is the
word that is similar to small in the same sense as biggest is similar to big?‚Äù
Somewhat surprisingly, these questions can be answered by performing simple algebraic operations
with the vector representation of words. To Ô¨Ånd a word that is similar to small in the same sense as
biggest is similar to big, we can simply compute vector X=vector ("biggest ") vector ("big") +
vector ("small "). Then, we search in the vector space for the word closest to Xmeasured by cosine
distance, and use it as the answer to the question (we discard the input question words during this
search). When the word vectors are well trained, it is possible to Ô¨Ånd the correct answer (word
smallest ) using this method.
Finally, we found that when we train high dimensional word vectors on a large amount of data, the
resulting vectors can be used to answer very subtle semantic relationships between words, such as
a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors
with such semantic relationships could be used to improve many existing NLP applications, such
as machine translation, information retrieval and question answering systems, and may enable other
future applications yet to be invented.
Table 1: Examples of Ô¨Åve types of semantic and nine types of syntactic questions in the Semantic-
Syntactic Word Relationship test set.
Type of relationship Word Pair 1 Word Pair 2
Common capital city Athens Greece Oslo Norway
All capital cities Astana Kazakhstan Harare Zimbabwe
Currency Angola kwanza Iran rial
City-in-state Chicago Illinois Stockton California
Man-Woman brother sister grandson granddaughter
Adjective to adverb apparent apparently rapid rapidly
Opposite possibly impossibly ethical unethical
Comparative great greater tough tougher
Superlative easy easiest lucky luckiest
Present Participle think thinking read reading
Nationality adjective Switzerland Swiss Cambodia Cambodian
Past tense walking walked swimming swam
Plural nouns mouse mice dollar dollars
Plural verbs work works speak speaks
4.1 Task Description
To measure quality of the word vectors, we deÔ¨Åne a comprehensive test set that contains Ô¨Åve types
of semantic questions, and nine types of syntactic questions. Two examples from each category are
shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions
in each category were created in two steps: Ô¨Årst, a list of similar word pairs was created manually.
Then, a large list of questions is formed by connecting two word pairs. For example, we made a
list of 68 large American cities and the states they belong to, and formed about 2.5K questions by
picking two word pairs at random. We have included in our test set only single token words, thus
multi-word entities are not present (such as New York ).
We evaluate the overall accuracy for all question types, and for each question type separately (se-
mantic, syntactic). Question is assumed to be correctly answered only if the closest word to the
vector computed using the above method is exactly the same as the correct word in the question;
synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely
to be impossible, as the current models do not have any input information about word morphology.
However, we believe that usefulness of the word vectors for certain applications should be positively
correlated with this accuracy metric. Further progress can be achieved by incorporating information
about structure of words, especially for the syntactic questions.
4.2 Maximization of Accuracy
We have used a Google News corpus for training the word vectors. This corpus contains about
6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we
are facing time constrained optimization problem, as it can be expected that both using more data
and higher dimensional word vectors will improve the accuracy. To estimate the best choice of
model architecture for obtaining as good as possible results quickly, we have Ô¨Årst evaluated models
trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.
The results using the CBOW architecture with different choice of word vector dimensionality and
increasing amount of the training data are shown in Table 2.
It can be seen that after some point, adding more dimensions or adding more training data provides
diminishing improvements. So, we have to increase both vector dimensionality and the amount
of the training data together. While this observation might seem trivial, it must be noted that it is
currently popular to train word vectors on relatively large amounts of data, but with insufÔ¨Åcient size
Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word
vectors from the CBOW architecture with limited vocabulary. Only questions containing words from
the most frequent 30k words are used.
Dimensionality / Training words 24M 49M 98M 196M 391M 783M
50 13.4 15.7 18.6 19.1 22.5 23.2
100 19.4 23.1 27.8 28.7 33.4 32.2
300 23.2 29.2 35.3 38.6 43.7 45.9
600 24.0 30.1 36.5 40.8 46.6 50.4
Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional
word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,
and on the syntactic relationship test set of [20]
Model Semantic-Syntactic Word Relationship test set MSR Word Relatedness
Architecture Semantic Accuracy [%] Syntactic Accuracy [%] Test Set [20]
RNNLM 9 36 35
NNLM 23 53 47
CBOW 24 64 61
Skip-gram 55 59 56
(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the
same increase of computational complexity as increasing vector size twice.
For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-
ent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so
that it approaches zero at the end of the last training epoch.
4.3 Comparison of Model Architectures
First we compare different model architectures for deriving the word vectors using the same training
data and using the same dimensionality of 640 of the word vectors. In the further experiments, we
use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to
the 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic
similarity between words3.
The training data consists of several LDC corpora and is described in detail in [18] (320M words,
82K vocabulary). We used these data to provide a comparison to a previously trained recurrent
neural network language model that took about 8 weeks to train on a single CPU. We trained a feed-
forward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],
using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the
projection layer has size 6408).
In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly
on the syntactic questions. The NNLM vectors perform signiÔ¨Åcantly better than the RNN - this is
not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden
layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the
same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic
task than the CBOW model (but still better than the NNLM), and much better on the semantic part
of the test than all the other models.
Next, we evaluated our models trained using one CPU only and compared the results against publicly
available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset
3We thank Geoff Zweig for providing us the test set.
Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-
ship test set, and word vectors from our models. Full vocabularies are used.
Model Vector Training Accuracy [%]
Dimensionality words
Semantic Syntactic Total
Collobert-Weston NNLM 50 660M 9.3 12.3 11.0
Turian NNLM 50 37M 1.4 2.6 2.1
Turian NNLM 200 37M 1.4 2.2 1.8
Mnih NNLM 50 37M 1.8 9.1 5.8
Mnih NNLM 100 37M 3.3 13.2 8.8
Mikolov RNNLM 80 320M 4.9 18.4 12.7
Mikolov RNNLM 640 320M 8.6 36.5 24.6
Huang NNLM 50 990M 13.3 11.6 12.3
Our NNLM 20 6B 12.9 26.4 20.3
Our NNLM 50 6B 27.9 55.8 43.2
Our NNLM 100 6B 34.2 64.5 50.8
CBOW 300 783M 15.5 53.1 36.1
Skip-gram 300 783M 50.0 55.9 53.3
Table 5: Comparison of models trained for three epochs on the same data and models trained for
one epoch. Accuracy is reported on the full Semantic-Syntactic data set.
Model Vector Training Accuracy [%] Training time
Dimensionality words [days]
Semantic Syntactic Total
3 epoch CBOW 300 783M 15.5 53.1 36.1 1
3 epoch Skip-gram 300 783M 50.0 55.9 53.3 3
1 epoch CBOW 300 783M 13.8 49.9 33.6 0.3
1 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.6
1 epoch CBOW 600 783M 15.4 53.3 36.2 0.7
1 epoch Skip-gram 300 783M 45.6 52.2 49.2 1
1 epoch Skip-gram 300 1.6B 52.2 55.1 53.8 2
1 epoch Skip-gram 600 783M 56.7 54.5 55.5 2.5
of the Google News data in about a day, while training time for the Skip-gram model was about three
days.
For experiments reported further, we used just one training epoch (again, we decrease the learning
rate linearly so that it approaches zero at the end of training). Training a model on twice as much
data using one epoch gives comparable or better results than iterating over the same data for three
epochs, as is shown in Table 5, and provides additional small speedup.
4.4 Large Scale Parallel Training of Models
As mentioned earlier, we have implemented various models in a distributed framework called Dis-
tBelief. Below we report the results of several models trained on the Google News 6B data set,
with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-
grad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an
Table 6: Comparison of models trained using the DistBelief distributed framework. Note that
training of NNLM with 1000-dimensional vectors would take too long to complete.
Model Vector Training Accuracy [%] Training time
Dimensionality words [days x CPU cores]
Semantic Syntactic Total
NNLM 100 6B 34.2 64.5 50.8 14 x 180
CBOW 1000 6B 57.3 68.9 63.7 2 x 140
Skip-gram 1000 6B 66.1 65.1 65.6 2.5 x 125
Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.
Architecture Accuracy [%]
4-gram [32] 39
Average LSA similarity [32] 49
Log-bilinear model [24] 54.8
RNNLMs [19] 55.4
Skip-gram 48.0
Skip-gram + RNNLMs 58.9
estimate since the data center machines are shared with other production tasks, and the usage can
Ô¨Çuctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of
the CBOW model and the Skip-gram model are much closer to each other than their single-machine
implementations. The result are reported in Table 6.
4.5 Microsoft Research Sentence Completion Challenge
The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing
language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one
word is missing in each sentence and the goal is to select word that is the most coherent with the
rest of the sentence, given a list of Ô¨Åve reasonable choices. Performance of several techniques has
been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear
model [24] and a combination of recurrent neural networks that currently holds the state of the art
performance of 55.4% accuracy on this benchmark [19].
We have explored the performance of Skip-gram architecture on this task. First, we train the 640-
dimensional model on 50M words provided in [32]. Then, we compute score of each sentence in
the test set by using the unknown word at the input, and predict all surrounding words in a sentence.
The Ô¨Ånal sentence score is then the sum of these individual predictions. Using the sentence scores,
we choose the most likely sentence.
A short summary of some previous results together with the new results is presented in Table 7.
While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores
from this model are complementary to scores obtained with RNNLMs, and a weighted combination
leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and
58.7% on the test part of the set).
5 Examples of the Learned Relationships
Table 8 shows words that follow various relationships. We follow the approach described above: the
relationship is deÔ¨Åned by subtracting two word vectors, and the result is added to another word. Thus
for example, Paris - France + Italy = Rome . As it can be seen, accuracy is quite good, although
there is clearly a lot of room for further improvements (note that using our accuracy metric that
Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-
gram model trained on 783M words with 300 dimensionality).
Relationship Example 1 Example 2 Example 3
France - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee
big - bigger small: larger cold: colder quick: quicker
Miami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii
Einstein - scientist Messi: midÔ¨Åelder Mozart: violinist Picasso: painter
Sarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan
copper - Cu zinc: Zn gold: Au uranium: plutonium
Berlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack
Microsoft - Windows Google: Android IBM: Linux Apple: iPhone
Microsoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs
Japan - sushi Germany: bratwurst France: tapas USA: pizza
assumes exact match, the results in Table 8 would score only about 60%). We believe that word
vectors trained on even larger data sets with larger dimensionality will perform signiÔ¨Åcantly better,
and will enable the development of new innovative applications. Another way to improve accuracy is
to provide more than one example of the relationship. By using ten examples instead of one to form
the relationship vector (we average the individual vectors together), we have observed improvement
of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.
It is also possible to apply the vector operations to solve different tasks. For example, we have
observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of
words, and Ô¨Ånding the most distant word vector. This is a popular type of problems in certain human
intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.</corps>
  <conclusion>In this paper we studied the quality of vector representations of words derived by various models on
a collection of syntactic and semantic language tasks. We observed that it is possible to train high
quality word vectors using very simple model architectures, compared to the popular neural network
models (both feedforward and recurrent). Because of the much lower computational complexity, it
is possible to compute very accurate high dimensional word vectors from a much larger data set.
Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram
models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That
is several orders of magnitude larger than the best previously published results for similar models.
An interesting task where the word vectors have recently been shown to signiÔ¨Åcantly outperform the
previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were
used together with other techniques to achieve over 50% increase in Spearman‚Äôs rank correlation
over the previous best result [31]. The neural network based word vectors were previously applied
to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can
be expected that these applications can beneÔ¨Åt from the model architectures described in this paper.
Our ongoing work shows that the word vectors can be successfully applied to automatic extension
of facts in Knowledge Bases, and also for veriÔ¨Åcation of correctness of existing facts. Results
from machine translation experiments also look very promising. In the future, it would be also
interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that
our comprehensive test set will help the research community to improve the existing techniques for
estimating the word vectors. We also expect that high quality word vectors will become an important
building block for future NLP applications.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>[1] Y . Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137-1155, 2003.
[2] Y . Bengio, Y . LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-
chines, MIT Press, 2007.
[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine
translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language
Processing and Computational Language Learning, 2007.
[4] R. Collobert and J. Weston. A UniÔ¨Åed Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In International Conference on Machine Learning,
ICML, 2008.
[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-
guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-
2537, 2011.
[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V . Le, M.Z. Mao, M.A. Ranzato, A.
Senior, P. Tucker, K. Yang, A. Y . Ng., Large Scale Distributed Deep Networks, NIPS, 2012.
[7] J.C. Duchi, E. Hazan, and Y . Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.
[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y . Ng. Improving Word Representations
via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational
Linguistics, 2012.
[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-
tributed processing: Explorations in the microstructure of cognition. V olume 1: Foundations,
MIT Press, 1986.
[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring
degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), 2012.
[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y . Ng, and C. Potts. Learning word vectors for
sentiment analysis. In Proceedings of ACL, 2011.
[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-
versity of Technology, 2007.
[14] T. Mikolov, J. Kopeck√Ω, L. Burget, O. Glembek and J. ÀáCernock√Ω. Neural network based lan-
guage models for higly inÔ¨Çective languages, In: Proc. ICASSP 2009.
[15] T. Mikolov, M. KaraÔ¨Å√°t, L. Burget, J. ÀáCernock√Ω, S. Khudanpur. Recurrent neural network
based language model, In: Proceedings of Interspeech, 2010.
[16] T. Mikolov, S. Kombrink, L. Burget, J. ÀáCernock√Ω, S. Khudanpur. Extensions of recurrent neural
network language model, In: Proceedings of ICASSP 2011.
[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ÀáCernock√Ω. Empirical Evaluation and Com-
bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.
4The code is available at https://code.google.com/p/word2vec/
[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ÀáCernock√Ω. Strategies for Training Large Scale
Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-
ing, 2011.
[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-
sity of Technology, 2012.
[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-
tations. NAACL HLT 2013.
[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of
Words and Phrases and their Compositionality. Accepted to NIPS 2013.
[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,
2007.
[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural
Information Processing Systems 21, MIT Press, 2009.
[24] A. Mnih, Y .W. Teh. A fast and simple algorithm for training neural probabilistic language
models. ICML, 2012.
[25] F. Morin, Y . Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,
2005.
[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-
propagating errors. Nature, 323:533.536, 1986.
[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,
2007.
[28] R. Socher, E.H. Huang, J. Pennington, A.Y . Ng, and C.D. Manning. Dynamic Pooling and
Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.
[29] J. Turian, L. Ratinov, Y . Bengio. Word Representations: A Simple and General Method for
Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.
[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-
tional Joint Conference on ArtiÔ¨Åcial Intelligence, 2005.
[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for
Measuring Relational Similarity. NAACL HLT 2013.
[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft
Research Technical Report MSR-TR-2011-129, 2011.
</bibliographie>

  <preamble>Nasr.pdf</preamble>
  <titre>MACAON
An NLP Tool Suite for Processing Word Lattices</titre>
  <auteurs>
    <auteur>
      <name>Alexis Nasr</name>
      <mail>alexis.nasr@lif.univ-mrs.fr</mail>
      <affiliation>Laboratoire d‚ÄôInformatique Fondamentale de Marseille- CNRS - UMR 6166
Universit√© Aix-Marseille</affiliation>
    </auteur>
    <auteur>
      <name>Fr√©d√©ric B√©chet</name>
      <mail>frederic.bechet@lif.univ-mrs.fr</mail>
      <affiliation>Laboratoire d‚ÄôInformatique Fondamentale de Marseille- CNRS - UMR 6166
Universit√© Aix-Marseille</affiliation>
    </auteur>
    <auteur>
      <name>Jean-Fran√ßois Rey</name>
      <mail>jean-francois.rey@lif.univ-mrs.fr</mail>
      <affiliation>Laboratoire d‚ÄôInformatique Fondamentale de Marseille- CNRS - UMR 6166
Universit√© Aix-Marseille</affiliation>
    </auteur>
    <auteur>
      <name>Beno√Æt Favre</name>
      <mail>benoit.favre@lif.univ-mrs.fr</mail>
      <affiliation>Laboratoire d‚ÄôInformatique Fondamentale de Marseille- CNRS - UMR 6166
Universit√© Aix-Marseille</affiliation>
    </auteur>
    <auteur>
      <name>Joseph Le Roux</name>
      <mail>joseph.le.roux@lif.univ-mrs.fr</mail>
      <affiliation>Laboratoire d‚ÄôInformatique Fondamentale de Marseille- CNRS - UMR 6166
Universit√© Aix-Marseille</affiliation>
    </auteur>
  </auteurs>
  <abstract>MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML Ô¨Åles . In addition, exchange pro-
tocols with external tools are easily deÔ¨Ånable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.</abstract>
  <introduction>
The automatic processing of textual data generated
by NLP software, resulting from Machine Transla-
tion, Automatic Speech Recognition or Automatic
Text Summarization, raises new challenges for lan-
guage processing tools. Unlike native texts (texts
produced by humans), this new kind of texts is the
result of imperfect processors and they are made
of several hypotheses, usually weighted with con-
Ô¨Ådence measures. Automatic text production sys-
tems can produce these weighted hypotheses as n-
best lists, word lattices, or confusion networks. It is
crucial for this space of ambiguous solutions to be
kept for later processing since the ambiguities of the
lower levels can sometimes be resolved during high-
level processing stages. It is therefore important to
be able to represent this ambiguity.
‚àóThis work has been funded by the French Agence Nationale
pour la Recherche, through the projects SEQUOIA (ANR-08-
EMER-013) and DECODA (2009-CORD-005-01)MACAON is a suite of tools developped to pro-
cess ambiguous input and extend inference of in-
put modules within a global scope. It con-
sists in several modules that perform classical
NLP tasks (tokenization, word recognition, part-of-
speech tagging, lemmatization, morphological anal-
ysis, partial or full parsing) on either native text
or word lattices. MACAON is distributed under
GNU public licence and can be downloaded from
http://www.macaon.lif.univ-mrs.fr/ .
From a general point of view, a MACAON module
can be seen as an annotation device1which adds a
new level of annotation to its input that generally de-
pends on annotations from preceding modules. The
modules communicate through XML Ô¨Åles that allow
the representation different layers of annotation as
well as ambiguities at each layer. Moreover, the ini-
tial XML structuring of the processed Ô¨Åles (logical
structuring of a document, information from the Au-
tomatic Speech Recognition module . . . ) remains
untouched by the processing stages.
As already mentioned, one of the main charac-
teristics of MACAON is the ability for each module
to accept ambiguous inputs and produce ambiguous
outputs, in such a way that ambiguities can be re-
solved at a later stage of processing. The compact
representation of ambiguous structures is at the heart
of the MACAON exchange format, described in sec-
tion 2. Furthermore every module can weight the
solutions it produces. such weights can be used to
rank solutions or limit their number for later stages
1Annotation must be taken here in a general sense which in-
cludes tagging, segmentation or the construction of more com-
plex objets as syntagmatic or dependencies trees.of processing.
Several processing tools suites alread exist for
French among which SXPIPE (Sagot and Boullier,
2008), OUTILEX (Blanc et al., 2006), NOOJ2orUNI-
TEX3. A general comparison of MACAON with these
tools is beyond the scope of this paper. Let us just
mention that MACAON shares with most of them the
use of Ô¨Ånite state machines as core data represen-
tation. Some modules are implemented as standard
operations on Ô¨Ånite state machines.
MACAON can also be compared to the numerous
development frameworks for developping process-
ing tools, such as GATE4, FREELING5, ELLOGON6
or L INGPIPE7that are usually limited to the process-
ing of native texts.
The MACAON exchange format shares a cer-
tain number of features with linguistic annotation
scheme standards such as the Text Encoding Initia-
tive8,XCES9, or EAGLES10. They all aim at deÔ¨Åning
standards for various types of corpus annotations.
The main difference between MACAON and these
approaches is that MACAON deÔ¨Ånes an exchange for-
mat between NLP modules and not an annotation
format. More precisely, this format is dedicated to
the compact representation of ambiguity: some in-
formation represented in the exchange format are
to be interpreted by MACAON modules and would
not be part of an annotation format. Moreover,
theMACAON exchange format was deÔ¨Åned from the
bottom up, originating from the authors‚Äô need to use
several existing tools and adapt their input/output
formats in order for them to be compatible. This is in
contrast with a top down approach which is usually
chosen when specifying a standard. Still, MACAON
shares several characteristics with the LAF (Ide and
Romary, 2004) which aims at deÔ¨Åning high level
standards for exchanging linguistic data.
2www.nooj4nlp.net/pages/nooj.html
3www-igm.univ-mlv.fr/ Àúunitex
4gate.ac.uk
5garraf.epsevg.upc.es/freeling
6www.ellogon.org
7alias-i.com/lingpipe
8www.tei-c.org/P5
9www.xml-ces.org
10www.ilc.cnr.it/eagles/home.html</introduction>
  <corps>The MACAON exchange format is based on four con-
cepts: segment ,attribute ,annotation level andseg-
mentation .
A segment refers to a segment of the text or
speech signal that is to be processed, as a sentence,
a clause, a syntactic constituent, a lexical unit, a
named entity . . . A segment can be equipped with at-
tributes that describe some of its aspects. A syntac-
tic constituent, for example, will deÔ¨Åne the attribute
type which speciÔ¨Åes its syntactic type (Noun Phrase,
Verb Phrase . . . ). A segment is made of one or more
smaller segments.
A sequence of segments covering a whole sen-
tence for written text, or a spoken utterance for oral
data, is called a segmentation . Such a sequence can
be weighted.
Anannotation level groups together segments of
a same type, as well as segmentations deÔ¨Åned on
these segments. Four levels are currently deÔ¨Åned:
pre-lexical, lexical, morpho-syntactic and syntactic.
Two relations are deÔ¨Åned on segments: the prece-
dence relation that organises linearly segments of a
given level into segmentations and the dominance
relation that describes how a segment is decomposed
in smaller segments either of the same level or of a
lower level.
We have represented in Ô¨Ågure 2, a schematic rep-
resentation of the analysis of the reconstructed out-
put a speech recognizer would produce on the in-
puttime Ô¨Çies like an arrow11. Three annotation lev-
els have been represented, lexical, morpho-syntactic
and syntactic. Each level is represented by a Ô¨Ånite-
state automaton which models the precedence rela-
tion deÔ¨Åned over the segments of this level. Seg-
ment time, for example, precedes segment Ô¨Çies. The
segments are implicitly represented by the labels of
the automaton‚Äôs arcs. This label should be seen as
a reference to a more complex objet, the actual seg-
ment. The dominance relations are represented with
dashed lines that link segments of different levels.
Segment time, for example, is dominated by seg-
ment NNof the morpho-syntactic level.
This example illustrates the different ambiguity
cases and the way they are represented.
11For readability reasons, we have used an English example,
MACAON , as mentioned above, currently exists for French.thyme
timeflieslike
likenan arrow
a rowJJ IN
VBDT NN
DT NNVB
NNNN
VBZVB VBVP
VPNPNP
VP
NPVP
VPPP
NP
NPFigure 1: Three annotation levels for a sample sentence.
Plain lines represent annotation hypotheses within a level
while dashed lines represent links between levels. Trian-
gles with the tip up are ‚Äúand‚Äù nodes and triangles with
the tip down are ‚Äúor‚Äù nodes. For instance, in the part-of-
speech layer, The Ô¨Årst NN can either refer to ‚Äútime‚Äù or
‚Äúthyme‚Äù. In the chunking layer, segments that span mul-
tiple part-of-speech tags are linked to them through ‚Äúand‚Äù
nodes.
The most immediate ambiguity phenomenon is
the segmentation ambiguity: several segmentations
are possible at every level. This ambiguity is rep-
resented in a compact way through the factoring of
segments that participate in different segmentations,
by way of a Ô¨Ånite state automaton.
The second ambiguity phenomenon is the dom-
inance ambiguity, where a segment can be decom-
posed in several ways into lower level segments.
Such a case appears in the preceding example, where
theNNsegment appearing in one of the outgoing
transition of the initial state of the morpho-syntactic
level dominates both thyme andtime segments of the
lexical level. The triangle with the tip down is an
‚Äúor‚Äù node, modeling the fact that NNcorresponds to
time orthyme .
Triangles with the tip up are ‚Äúand‚Äù nodes. They
model the fact that the PPsegment of the syntac-
tic level dominates segments IN,DTandNNof the
morpho-syntactic level.
2.1 XML representation
The MACAON exchange format is implemented in
XML . A segment is represented with the XML tag&lt;segment&gt; which has four mandatory attributes:
‚Ä¢type indicates the type of the segment, four dif-
ferent types are currently deÔ¨Åned: atome (pre-
lexical unit usually referred to as token in en-
glish), ulex (lexical unit), cat(part of speech)
andchunk (a non recursive syntactic unit).
‚Ä¢idassociates to a segment a unique identiÔ¨Åer in
the document, in order to be able to reference
it.
‚Ä¢start andenddeÔ¨Åne the span of the segment.
These two attributes are numerical and repre-
sent either the index of the Ô¨Årst and last char-
acter of the segment in the text string or the
beginning and ending time of the segment in
a speech signal.
A segment can deÔ¨Åne other attributes that can be
useful for a given description level. We often Ô¨Ånd
thestype attribute that deÔ¨Ånes subtypes of a given
type.
The dominance relation is represented through the
use of the &lt;sequence&gt; tag. The domination of the
three segments IN,DTandNNby a PPsegment,
mentionned above is represented below, where p1,
p2andp3are respectively the ids of segments IN,
DTandNN.
&lt;segment type="chunk" stype="PP" id="c1"&gt;
&lt;sequence&gt;
&lt;elt segref="p1"/&gt;
&lt;elt segref="p2"/&gt;
&lt;elt segref="p3"/&gt;
&lt;/sequence&gt;
&lt;/segment&gt;
The ambiguous case, described above where seg-
ment NNdominates segments time orthyme is rep-
resented below as a disjunction of sequences inside
a segment. The disjunction itself is not represented
as an XML tag.l1andl2are respectively the ids
of segments time andthyme .
&lt;segment type="cat" stype="NN" id="c1"&gt;
&lt;sequence&gt;
&lt;elt segref="l1" w="-3.37"/&gt;
&lt;/sequence&gt;
&lt;sequence&gt;
&lt;elt segref="l2" w="-4.53"/&gt;
&lt;/sequence&gt;
&lt;/segment&gt;The dominance relation can be weighted, by way
of the attribute w. Such a weight represents in the
preceding example the conditional log-probability
of a lexical unit given a part of speech, as in a hidden
Markov model.
The precedence relation (i.e. the organization
of segments in segmentations), is represented as a
weighted Ô¨Ånite state automaton. Automata are rep-
resented as a start state, accept states and a list of
transitions between states, as in the following exam-
ple that corresponds to the lexical level of our exam-
ple.
&lt;fsm n="9"&gt;
&lt;start n="0"/&gt;
&lt;accept n="6"/&gt;
&lt;ltrans&gt;
&lt;trans o="0" d="1" i="l1" w="-7.23"/&gt;
&lt;trans o="0" d="1" i="l2" w="-9.00"/&gt;
&lt;trans o="1" d="2" i="l3" w="-3.78"/&gt;
&lt;trans o="2" d="3" i="l4" w="-7.37"/&gt;
&lt;trans o="3" d="4" i="l5" w="-3.73"/&gt;
&lt;trans o="2" d="4" i="l6" w="-6.67"/&gt;
&lt;trans o="4" d="5" i="l7" w="-4.56"/&gt;
&lt;trans o="5" d="6" i="l8" w="-2.63"/&gt;
&lt;trans o="4" d="6" i="l9" w="-7.63"/&gt;
&lt;/ltrans&gt;
&lt;/fsm&gt;
The &lt;trans/&gt; tag represents a transition, its
o,d,i andwfeatures are respectively the origin, and
destination states, its label (the idof a segment) and
a weight.
An annotation level is represented by the
&lt;section&gt; tag which regroups two tags, the
&lt;segments&gt; tag that contains the different segment
tags deÔ¨Åned at this annotation level and the &lt;fsm&gt;
tag that represents all the segmentations of this level.
3 The MACAON architecture
Three aspects have guided the architecture of
MACAON : openness, modularity, and speed. Open-
ness has been achieved by the deÔ¨Ånition of an ex-
change format which has been made as general as
possible, in such a way that mapping can be de-
Ô¨Åned from and to third party modules as ASR, MT
systems or parsers. Modularity has been achieved
by the deÔ¨Ånition of independent modules that com-
municate with each other through XML Ô¨Åles using
standard UNIX pipes. A module can therefore be re-
placed easily. Speed has been obtained using efÔ¨Å-
cient algorithms and a representation especially de-signed to load linguistic data and models in a fast
way.
MACAON is composed of libraries and compo-
nents. Libraries contain either linguistic data, mod-
els or API functions. Two kinds of components are
presented, the MACAON core components and third
party components for which mappings to and from
theMACAON exchange format have been deÔ¨Åned.
3.1 Libraries
The main MACAON library is macaon common .
It deÔ¨Ånes a simple interface to the MACAON ex-
change format and functions to load XML MACAON
Ô¨Åles into memory using efÔ¨Åcient data structures.
Other libraries macaon lex,macaon code and
macaon tagger lib represent the lexicon, the
morphological data base and the tagger models in
memory.
MACAON only relies on two third-party libraries,
which are gfsm12, a Ô¨Ånite state machine library and
libxml , an XML library13.
3.2 The MACAON core components
A brief description of several standard components
developed in the MACAON framework is given be-
low. They all comply with the exchange format de-
scribed above and add a &lt;macaon stamp&gt; to the
XML Ô¨Åle that indicates the name of the component,
the date and the component version number, and rec-
ognizes a set of standard options.
maca select is a pre-processing component: it adds
amacaon tag under the target tags speciÔ¨Åed by
the user to the input XML Ô¨Åle. The follow-
ing components will only process the document
parts enclosed in macaon tags.
maca segmenter segments a text into sentences by
examining the context of punctuation with a
regular grammar given as a Ô¨Ånite state automa-
ton. It is disabled for automatic speech tran-
scriptions which do not typically include punc-
tuation signs and come with their own segmen-
tation.
12ling.uni-potsdam.de/ Àúmoocow/projects/
gfsm/
13xmlsoft.org maca tokenizer tokenizes a sentence into pre-
lexical units. It is also based on regular gram-
mars that recognize simple tokens as well as a
predeÔ¨Åned set of special tokens, such as time
expressions, numerical expressions, urls. . . .
maca lexer allows to regroup pre-lexical units into
lexical units. It is based on the le fffFrench lex-
icon (Sagot et al., 2006) which contains around
500,000 forms. It implements a dynamic pro-
gramming algorithm that builds all the possible
grouping of pre-lexical units into lexical units.
maca tagger associates to every lexical unit one or
more part-of-speech labels. It is based on a
trigram Hidden Markov Model trained on the
French Treebank (Abeill√© et al., 2003). The es-
timation of the HMM parameters has been re-
alized by the SRILM toolkit (Stolcke, 2002).
maca anamorph produces the morphological anal-
ysis of lexical units associated to a part of
speech. The morphological information come
from the le ffflexicon.
maca chunker gathers sequences of part-of-speech
tags in non recursive syntactic units. This com-
ponent implements a cascade of Ô¨Ånite state
transducers, as proposed by Abney (1996). It
adds some features to the initial Abney pro-
posal, like the possibility to deÔ¨Åne the head of
a chunk.
maca conv is a set of converters from and to the
MACAON exchange format. htk2macaon
andfsm2macaon convert word lattices from
the HTK format (Young, 1994) and ATT
FSM format (Mohri et al., 2000) to the
MACAON exchange format. macaon2txt and
txt2macaon convert from and to plain text
Ô¨Åles. macaon2lorg andlorg2macaon
convert to and from the format of the LORG
parser (see section 3.3).
maca view is a graphical interface that allows to in-
spect MACAON XML Ô¨Åles and run the compo-
nents.
3.3 Third party components
MACAON is an open architecture and provides a rich
exchange format which makes possible the repre-sentation of many NLP tools input and output in the
MACAON format. MACAON has been interfaced with
the SPEERAL Automatic Speech Recognition Sys-
tem (Nocera et al., 2006). The word lattices pro-
duced by SPEERAL can be converted to pre-lexical
MACAON automata.
MACAON does not provide any native module for
parsing yet but it can be interfaced with any already
existing parser. For the purpose of this demonstra-
tion we have chosen the LORG parser developed at
NCLT, Dublin14. This parser is based on PCFGs
with latent annotations (Petrov et al., 2006), a for-
malism that showed state-of-the-art parsing accu-
racy for a wide range of languages. In addition it of-
fers a sophisticated handling of unknown words re-
lying on automatically learned morphological clues,
especially for French (Attia et al., 2010). Moreover,
this parser accepts input that can be tokenized, pos-
tagged or pre-bracketed. This possibility allows for
different settings when interfacing it with MACAON .
4 Applications
MACAON has been used in several projects, two of
which are brieÔ¨Çy described here, the D EFINIENS
project and the L UNA project.
DEFINIENS (Barque et al., 2010) is a project that
aims at structuring the deÔ¨Ånitions of a large coverage
French lexicon, the Tr√©sor de la langue fran√ßaise .
The lexicographic deÔ¨Ånitions have been processed
byMACAON in order to decompose the deÔ¨Ånitions
into complex semantico-syntactic units. The data
processed is therefore native text that possesses a
rich XML structure that has to be preserved during
processing.
LUNA15is a European project that aims at extract-
ing information from oral data about hotel booking.
The word lattices produced by an ASR system have
been processed by MACAON up to a partial syntactic
level from which frames are built. More details can
be found in (B√©chet and Nasr, 2009). The key aspect
of the use of MACAON for the L UNA project is the
ability to perform the linguistic analyses on the mul-
tiple hypotheses produced by the ASR system. It is
therefore possible, for a given syntactic analysis, to
14www.computing.dcu.ie/ Àúlorg . This software
should be freely available for academic research by the time
of the conference.
15www.ist-luna.eu Figure 2: Screenshot of the MACAON visualization inter-
face (for French models). It allows to input a text and see
the n-best results of the annotation.
Ô¨Ånd all the word sequences that are compatible with
this analysis.
Figure 2 shows the interface that can be used to
see the output of the pipeline.</corps>
  <conclusion>In this paper we have presented MACAON , an NLP
tool suite which allows to process native text as well
as several hypotheses automatically produced by an
ASR or an MT system. Several evolutions are cur-
rently under development, such as a named entity
recognizer component and an interface with a de-
pendency parser.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>Anne Abeill√©, Lionel Cl√©ment, and Fran√ßois Toussenel.
2003. Building a treebank for french. In Anne
Abeill√©, editor, Treebanks . Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via Ô¨Ånite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8‚Äì15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL .
Lucie Barque, Alexis Nasr, and Alain Polgu√®re. 2010.
From the deÔ¨Ånitions of the tr√©sor de la langue fran√ßaise
to a semantic database of the french language. In EU-
RALEX 2010 , Leeuwarden, Pays Bas.Fr√©d√©ric B√©chet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech , Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes√©crits. In TALN 2006 , Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering , 10(3-4):211‚Äì225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted Ô¨Ånite-state transducer library.
Theoretical Computer Science , 231(1):17‚Äì32.
P. Nocera, G. Linares, D. Massoni√©, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue , pages 83‚Äì
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Beno√Æt Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement pr√©syntaxique de cor-
pus bruts. Traitement Automatique des Langues ,
49(2):155‚Äì188.
Beno√Æt Sagot, Lionel Cl√©ment, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The le fff2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation , Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing , Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd , 2:2‚Äì44.</bibliographie>

  <preamble>Torres.pdf</preamble>
  <titre>Summary Evaluation
with and without References</titre>
  <auteurs>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>is with LIA/Universit√© d‚ÄôAvignon,
France and √âcole Polytechnique de Montr√©al, Canada</affiliation>
    </auteur>
    <auteur>
      <name>Horacio Saggion</name>
      <mail>horacio.saggion@upf.edu</mail>
      <affiliation>is with DTIC/Universitat Pompeu Fabra, Spain</affiliation>
    </auteur>
    <auteur>
      <name>Iria da Cunha</name>
      <mail>iria.dacunha@upf.edu</mail>
      <affiliation>is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Universit√© d‚ÄôAvignon, France and Instituto de Ingenier ¬¥ƒ±a/UNAM, Mexico</affiliation>
    </auteur>
    <auteur>
      <name>Eric SanJuan</name>
      <mail>eric.sanjuan@univ-avignon.fr</mail>
      <affiliation>is with LIA/Universit√© d‚ÄôAvignon, France</affiliation>
    </auteur>
    <auteur>
      <name>Patricia Vel√°zquez-Morales</name>
      <mail>velazquez@yahoo.com</mail>
      <affiliation>is with VM Labs, France
(patricia</affiliation>
    </auteur>
  </auteurs>
  <abstract>We study a new content-based method for
the evaluation of text summarization systems without
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called FRESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as COVERAGE ,
RESPONSIVENESS ,PYRAMIDS and ROUGE studying their
associations in various text summarization tasks including
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and
generic single-document summarization in French and Spanish.</abstract>
  <introduction>
TEXT summarization evaluation has always been a
complex and controversial issue in computational
linguistics. In the last decade, signiÔ¨Åcant advances have been
made in this Ô¨Åeld as well as various evaluation measures have
been designed. Two evaluation campaigns have been led by
the U.S. agence DARPA. The Ô¨Årst one, SUMMAC, ran from
1996 to 1998 under the auspices of the Tipster program [1],
and the second one, entitled DUC (Document Understanding
Conference) [2], was the main evaluation forum from 2000
until 2007. Nowadays, the Text Analysis Conference (TAC)
[3] provides a forum for assessment of different information
access technologies including text summarization.
Evaluation in text summarization can be extrinsic or
intrinsic [4]. In an extrinsic evaluation, the summaries are
assessed in the context of an speciÔ¨Åc task carried out by a
human or a machine. In an intrinsic evaluation, the summaries
are evaluated in reference to some ideal model. SUMMAC
was mainly extrinsic while DUC and TAC followed an
intrinsic evaluation paradigm. In an intrinsic evaluation, an
Manuscript received June 8, 2010. Manuscript accepted for publication July
25, 2010.
Juan-Manuel Torres-Moreno is with LIA/Universit√© d‚ÄôAvignon,
France and √âcole Polytechnique de Montr√©al, Canada
(juan-manuel.torres@univ-avignon.fr).
Eric SanJuan is with LIA/Universit√© d‚ÄôAvignon, France
(eric.sanjuan@univ-avignon.fr).
Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
(horacio.saggion@upf.edu).
Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Universit√© d‚ÄôAvignon, France and Instituto de Ingenier ¬¥ƒ±a/UNAM, Mexico
(iria.dacunha@upf.edu).
Patricia Vel√°zquez-Morales is with VM Labs, France
(patricia velazquez@yahoo.com).automatically generated summary ( peer) has to be compared
with one or more reference summaries ( models ). DUC used
an interface called SEE to allow human judges to compare
apeer with a model . Thus, judges give a C OVERAGE score
to each peer produced by a system and the Ô¨Ånal system
COVERAGE score is the average of the C OVERAGE ‚Äôs scores
asigned. These system‚Äôs C OVERAGE scores can then be used
to rank summarization systems. In the case of query-focused
summarization (e.g. when the summary should answer a
question or series of questions) a R ESPONSIVENESS score
is also assigned to each summary, which indicates how
responsive the summary is to the question(s).
Because manual comparison of peer summaries with model
summaries is an arduous and costly process, a body of
research has been produced in the last decade on automatic
content-based evaluation procedures. Early studies used text
similarity measures such as cosine similarity (with or without
weighting schema) to compare peer and model summaries
[5]. Various vocabulary overlap measures such as n-grams
overlap or longest common subsequence between peer and
model have also been proposed [6], [7]. The B LEU machine
translation evaluation measure [8] has also been tested in
summarization [9]. The DUC conferences adopted the R OUGE
package for content-based evaluation [10]. R OUGE implements
a series of recall measures based on n-gram co-occurrence
between a peer summary and a set of model summaries. These
measures are used to produce systems‚Äô rank. It has been shown
that system rankings, produced by some R OUGE measures
(e.g., R OUGE -2, which uses 2-grams), have a correlation with
rankings produced using C OVERAGE .
In recent years the P YRAMIDS evaluation method [11] has
been introduced. It is based on the distribution of ‚Äúcontent‚Äù
of a set of model summaries. Summary Content Units (SCUs)
are Ô¨Årst identiÔ¨Åed in the model summaries, then each SCU
receives a weight which is the number of models containing
or expressing the same unit. Peer SCUs are identiÔ¨Åed in the
peer, matched against model SCUs, and weighted accordingly.
The P YRAMIDS score given to a peer is the ratio of the sum
of the weights of its units and the sum of the weights of the
best possible ideal summary with the same number of SCUs as
the peer. The P YRAMIDS scores can be also used for ranking
summarization systems. [11] showed that P YRAMIDS scores
produced reliable system rankings when multiple (4 or more)
models were used and that P YRAMIDS rankings correlate with
rankings produced by R OUGE -2 and R OUGE -SU2 (i.e. R OUGE
with skip 2-grams). However, this method requires the creation
of models and the identiÔ¨Åcation, matching, and weighting of
SCUs in both: models and peers.
[12] evaluated the effectiveness of the Jensen-Shannon
(JS) [13] theoretic measure in predicting systems ranks
in two summarization tasks: query-focused and update
summarization. They have shown that ranks produced
by P YRAMIDS and those produced by JS measure
correlate. However, they did not investigate the effect
of the measure in summarization tasks such as generic
multi-document summarization (DUC 2004 Task 2),
biographical summarization (DUC 2004 Task 5), opinion
summarization (TAC 2008 OS), and summarization in
languages other than English.
In this paper we present a series of experiments aimed at
a better understanding of the value of the JS divergence
for ranking summarization systems. We have carried out
experimentation with the proposed measure and we have
veriÔ¨Åed that in certain tasks (such as those studied by
[12]) there is a strong correlation among P YRAMIDS ,
RESPONSIVENESS and theJS divergence, but as we will
show in this paper, there are datasets in which the correlation
is not so strong. We also present experiments in Spanish
and French showing positive correlation between the JS
and R OUGE which is the de facto evaluation measure used
in evaluation of non-English summarization. To the best of
our knowledge this is the more extensive set of experiments
interpreting the value of evaluation without human models.
The rest of the paper is organized in the following way:
First in Section II we introduce related work in the area of
content-based evaluation identifying the departing point for
our inquiry; then in Section III we explain the methodology
adopted in our work and the tools and resources used for
experimentation. In Section IV we present the experiments
carried out together with the results. Section V discusses the
results and Section VI concludes the paper and identiÔ¨Åes future
work.
</introduction>
  <corps>One of the Ô¨Årst works to use content-based measures in
text summarization evaluation is due to [5], who presented an
evaluation framework to compare rankings of summarization
systems produced by recall and cosine-based measures. They
showed that there was weak correlation among rankings
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented a
set of evaluation measures based on the notion of vocabulary
overlap including n-gram overlap, cosine similarity, and
longest common subsequence, and they applied them to
multi-document summarization in English and Chinese.
However, they did not evaluate the performance of the
measures in different summarization tasks. [7] also compared
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random fromnon-random systems, no clear conclusion was reached on the
value of each of the studied measures.
Nowadays, a widespread summarization evaluation
framework is R OUGE [14], which offers a set of statistics
that compare peer summaries with models. It counts
co-occurrences of n-grams in peer and models to derive a
score. There are several statistics depending on the used
n-grams and the text processing applied to the input texts
(e.g., lemmatization, stop-word removal).
[15] proposed a method of evaluation based on the
use of ‚Äúdistances‚Äù or divergences between two probability
distributions (the distribution of units in the automatic
summary and the distribution of units in the model
summary). They studied two different Information Theoretic
measures of divergence: the Kullback-Leibler ( KL) [16] and
Jensen-Shannon (JS) [13] divergences. KL computes the
divergence between probability distributions PandQin the
following way:
DKL(PjjQ) =1
2X
wPwlog2Pw
Qw(1)
WhileJS divergence is deÔ¨Åned as follows:
DJS(PjjQ) =1
2X
wPwlog22Pw
Pw+Qw+Qwlog22Qw
Pw+Qw
(2)
These measures can be applied to the distribution of units in
system summaries Pand reference summaries Q. The value
obtained may be used as a score for the system summary. The
method has been tested by [15] over the DUC 2002 corpus for
single and multi-document summarization tasks showing good
correlation among divergence measures and both coverage and
ROUGE rankings.
[12] went even further and, as in [5], they proposed to
compare directly the distribution of words in full documents
with the distribution of words in automatic summaries to
derive a content-based evaluation measure. They found a
high correlation between rankings produced using models
and rankings produced without models. This last work is the
departing point for our inquiry into the value of measures that
do not rely on human models.
III. M ETHODOLOGY
The followed methodology in this paper mirrors the one
adopted in past work (e.g. [5], [7], [12]). Given a particular
summarization task T,pdata points to be summarized
with input material fIigp 1
i=0(e.g. document(s), question(s),
topic(s)),speer summariesfSUMi;kgs 1
k=0for inputi, and
mmodel summaries fMODELi;jgm 1
j=0for inputi, we will
compare rankings of the speer summaries produced by various
evaluation measures. Some measures that we use compare
summaries with nof themmodels:
MEASURE M(SUMi;k;fMODELi;jgn 1
j=0) (3)
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel√°zquez-Morales
while other measures compare peers with all or some of the
input material:
MEASURE M(SUMi;k;I0
i) (4)
whereI0
iis some subset of input Ii. The values produced
by the measures for each summary SUM i;kare averaged
for each system k= 0;:::;s 1and these averages are
used to produce a ranking. Rankings are then compared
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables
whose values are used to rank objects. We have chosen
to use this correlation to compare directly results to those
presented in [12]. Computation of correlations is done using
theStatistics-RankCorrelation-0.12 package1, which computes
the rank correlation between two vectors. We also veriÔ¨Åed
the good conformity of the results with the correlation test
of Kendallcalculated with the statistical software R. The
two nonparametric tests of Spearman and Kendall do not
really stand out as the treatment of ex-√¶quo. The good
correspondence between the two tests shows that they do not
introduce bias in our analysis. Subsequently will mention only
theof Sperman more widely used in this Ô¨Åeld.
A. Tools
We carry out experimentation using a new summarization
evaluation framework: F RESA ‚ÄìFRamework for Evaluating
Summaries Automatically‚Äì, which includes document-based
summary evaluation measures based on probabilities
distribution2. As in the R OUGE package, F RESA supports
differentn-grams and skip n-grams probability distributions.
The F RESA environment can be used in the evaluation of
summaries in English, French, Spanish and Catalan, and it
integrates Ô¨Åltering and lemmatization in the treatment of
summaries and documents. It is developed in Perl and will
be made publicly available. We also use the R OUGE package
[10] to compute various R OUGE statistics in new datasets.
B. Summarization Tasks and Data Sets
We have conducted our experimentation with the following
summarization tasks and data sets:
1) Generic multi-document-summarization in English
(production of a short summary of a cluster of related
documents) using data from DUC‚Äô043, task 2: 50
clusters, 10 documents each ‚Äì 294,636 words.
2) Focused-based summarization in English (production of
a short focused multi-document summary focused on the
question ‚Äúwho is X?‚Äù, where X is a person‚Äôs name) using
data from the DUC‚Äô04 task 5: 50 clusters, 10 documents
each plus a target person name ‚Äì 284,440 words.
1http://search.cpan.org/ gene/Statistics-RankCorrelation-0.12/
2FRESA is available at: http://lia.univavignon.fr/Ô¨Åleadmin/axes/TALNE/
Ressources.html
3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html3) Update-summarization task that consists of creating a
summary out of a cluster of documents and a topic. Two
sub-tasks are considered here: A) an initial summary has
to be produced based on an initial set of documents and
topic; B) an update summary has to be produced from
a different (but related) cluster assuming documents
used in A) are known. The English TAC‚Äô08 Update
Summarization dataset is used, which consists of 48
topics with 20 documents each ‚Äì 36,911 words.
4) Opinion summarization where systems have to analyze
a set of blog articles and summarize the opinions
about a target in the articles. The TAC‚Äô08 Opinion
Summarization in English4data set (taken from the
Blogs06 Text Collection) is used: 25 clusters and targets
(i.e., target entity and questions) were used ‚Äì 1,167,735
words.
5) Generic single-document summarization in Spanish
using the Medicina Cl ¬¥ƒ±nica5corpus, which is composed
of 50 medical articles in Spanish, each one with its
corresponding author abstract ‚Äì 124,929 words.
6) Generic single document summarization in French using
the ‚ÄúCanadien French Sociological Articles‚Äù corpus
from the journal Perspectives interdisciplinaires sur le
travail et la sant√©(PISTES)6. It contains 50 sociological
articles in French, each one with its corresponding
author abstract ‚Äì 381,039 words.
7) Generic multi-document-summarization in French using
data from the RPM27corpus [18], 20 different themes
consisting of 10 articles and 4 abstracts by reference
thematic ‚Äì 185,223 words.
For experimentation in the TAC and the DUC datasets we use
directly the peer summaries produced by systems participating
in the evaluations. For experimentation in Spanish and French
(single and multi-document summarization) we have created
summaries at a similar ratio to those of reference using the
following systems:
‚ÄìENERTEX [19], a summarizer based on a theory of
textual energy;
‚ÄìCORTEX [20], a single-document sentence extraction
system for Spanish and French that combines various
statistical measures of relevance (angle between sentence
and topic, various Hamming weights for sentences, etc.)
and applies an optimal decision algorithm for sentence
selection;
‚ÄìSUMMTERM [21], a terminology-based summarizer that
is used for summarization of medical articles and
uses specialized terminology for scoring and ranking
sentences;
‚ÄìREG [22], summarization system based on an greedy
algorithm;
4http://www.nist.gov/tac/data/index.html
5http://www.elsevier.es/revistas/ctl servlet? f=7032&amp;revistaid=2
6http://www.pistes.uqam.ca/
7http://www-labs.sinequa.com/rpm2
Summary Evaluation with and without References
‚ÄìJSsummarizer, a summarization system that scores
and ranks sentences according to their Jensen-Shannon
divergence to the source document;
‚Äì a lead-based summarization system that selects the lead
sentences of the document;
‚Äì a random-based summarization system that selects
sentences at random;
‚ÄìOpen Text Summarizer [23], a multi-lingual summarizer
based on the frequency and
‚Äì commercial systems: Word ,SSSummarizer8,Pertinence9
andCopernic10.
C. Evaluation Measures
The following measures derived from human assessment of
the content of the summaries are used in our experiments:
‚Äì C OVERAGE is understood as the degree to which one
peer summary conveys the same information as a model
summary [2]. C OVERAGE was used in DUC evaluations.
This measure is used as indicated in equation 3 using
human references or models.
‚Äì R ESPONSIVENESS ranks summaries in a 5-point scale
indicating how well the summary satisÔ¨Åed a given
information need [2]. It is used in focused-based
summarization tasks. This measure is used as indicated
in equation 4 since a human judges the summary
with respect to a given input ‚Äúuser need‚Äù (e.g., a
question). R ESPONSIVENESS was used in DUC and TAC
evaluations.
‚Äì P YRAMIDS [11] is a content assessment measure which
compares content units in a peer summary to weighted
content units in a set of model summaries. This
measure is used as indicated in equation 3 using human
references or models. P YRAMIDS is the adopted metric
for content-based evaluation in the TAC evaluations.
For DUC and TAC datasets the values of these measures are
available and we used them directly. We used the following
automatic evaluation measures in our experiments:
‚Äì R OUGE [14], which is a recall metric that takes into
accountn-grams as units of content for comparing peer
and model summaries. The R OUGE formula speciÔ¨Åed in
[10] is as follows:
ROUGE-n(R;M) =P
m2MP
n gram2Pcountmatch(n gram)
P
m2MPcount(n-gram)(5)
where R is the summary to be evaluated, Mis the set of
model (human) summaries, countmatchis the number of
commonn-grams inmandP, and count is the number
ofn-grams in the model summaries. For the experiments
8http://www.kryltech.com/summarizer.htm
9http://www.pertinence.net
10http://www.copernic.com/en/products/summarizerpresented here we used uni-grams, 2-grams, and the skip
2-grams with maximum skip distance of 4 (R OUGE -1,
ROUGE -2 and R OUGE -SU4). R OUGE is used to compare
a peer summary to a set of model summaries in our
framework (as indicated in equation 3).
‚Äì Jensen-Shannon divergence formula given in Equation 2
is implemented in our F RESA package with the following
speciÔ¨Åcation (Equation 6) for the probability distribution
of wordsw.
Pw=CT
w
N
Qw=(CS
w
NSifw2S
CT
w+ffi
N+ffiBotherwise(6)
WherePis the probability distribution of words win
textTandQis the probability distribution of words w
in summary S;Nis the number of words in text and
summaryN=NT+NS,B= 1:5jVj,CT
wis the number
of words in the text and CS
wis the number of words in
the summary. For smoothing the summary‚Äôs probabilities
we have used ffi= 0:005. We have also implemented
other smoothing approaches (e.g. Good-Turing [24], that
uses the CPAN Perl‚Äôs Statistics-Smoothing-SGT-2.1.2
package11) in F RESA , but we do not use them in
the experiments reported here. Following the R OUGE
approach, in addition to word uni-grams we use 2-grams
and skipn-grams computing divergences such as JS
(using uni-grams) JS 2(using 2-grams),JS 4(using the
skipn-grams of R OUGE -SU4), andJSMwhich is an
average of theJSi.JSs measures are used to compare a
peer summary to its source document(s) in our framework
(as indicated in equation 4). In the case of summarization
of multiple documents, these are concatenated (in the
given input order) to form a single input from which
probabilities are computed.
IV. E XPERIMENTS AND RESULTS
We Ô¨Årst replicated the experiments presented in [12] to
verify that our implementation of JS produced correlation
results compatible with that work. We used the TAC‚Äô08
Update Summarization data set and computed JS and
ROUGE measures for each peer summary. We produced
two system rankings (one for each measure), which were
compared to rankings produced using the manual P YRAMIDS
and R ESPONSIVENESS scores. Spearman correlations were
computed among the different rankings. The results are
presented in Table I. These results conÔ¨Årm a high correlation
among P YRAMIDS , R ESPONSIVENESS andJS. We also
veriÔ¨Åed high correlation between JS and R OUGE -2 (0:83
Spearman correlation, not shown in the table) in this task and
dataset.
Then, we experimented with data from DUC‚Äô04, TAC‚Äô08
Opinion Summarization pilot task as well as single and
11http://search.cpan.org/ bjoernw/Statistics-Smoothing-SGT-2.1.2/
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel√°zquez-Morales
TABLE I
SPEARMAN CORRELATION OF CONTENT -BASED MEASURES IN TAC‚Äô08
UPDATE SUMMARIZATION TASK
Mesure PYRAMIDS p-value RESPONSIVENESS p-value
ROUGE -2 0.96p&lt;0:005 0.92 p&lt;0:005
JS 0.85p&lt;0:005 0.74 p&lt;0:005
multi-document summarization in Spanish and French. In spite
of the fact that the experiments for French and Spanish corpora
use less data points (i.e., less summarizers per task) than
for English, results are still quite signiÔ¨Åcant. For DUC‚Äô04,
we computed the JS measure for each peer summary in
tasks 2 and 5 and we used JS, ROUGE , COVERAGE and
RESPONSIVENESS scores to produce systems‚Äô rankings. The
various Spearman‚Äôs rank correlation values for DUC‚Äô04 are
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have veriÔ¨Åed a strong correlation between
JS and C OVERAGE . For task 5, the correlation between
JS and C OVERAGE is weak, and that between JS and
RESPONSIVENESS is weak and negative.
Although the Opinion Summarization (OS) task is a new
type of summarization task and its evaluation is a complicated
issue, we have decided to compare JS rankings with those
obtained using P YRAMIDS and R ESPONSIVENESS in TAC‚Äô08.
Spearman‚Äôs correlation values are listed in Table IV. As it can
be seen, there is weak and negative correlation of JS with
both P YRAMIDS and R ESPONSIVENESS . Correlation between
PYRAMIDS and R ESPONSIVENESS rankings is high for this
task (0.71 Spearman‚Äôs correlation value).
For experimentation in mono-document summarization
in Spanish and French, we have run 11 multi-lingual
summarization systems; for experimentation in French, we
have run 12 systems. In both cases, we have produced
summaries at a compression rate close to the compression rate
of the authors‚Äô provided abstracts. We have then computed JS
and R OUGE measures for each summary and we have averaged
the measure‚Äôs values for each system. These averages were
used to produce rankings per each measure. We computed
Spearman‚Äôs correlations for all pairs of rankings.
Results are presented in Tables V, VI and VII. All results
show medium to strong correlation between the JSmeasures
and R OUGE measures. However the JS measure based on
uni-grams has lower correlation than JSs which use n-grams
of higher order. Note that table VII presents results for
generic multi-document summarization in French, in this
case correlation scores are lower than correlation scores for
single-document summarization in French, a result which may
be expected given the diversity of input in multi-document
summarization.</corps>
  <conclusion>This paper has presented a series of experiments in
content-based measures that do not rely on the use of model
summaries for comparison purposes. We have carried out
extensive experimentation with different summarization tasks
drawing a clearer picture of tasks where the measures could
be applied. This paper makes the following contributions:
‚Äì We have shown that if we are only interested in ranking
summarization systems according to the content of their
automatic summaries, there are tasks were models could
be subtituted by the full document in the computation of
theJS measure obtaining reliable rankings. However,
we have also found that the substitution of models
by full-documents is not always advisable. We have
Summary Evaluation with and without References
TABLE II
SPEARMANOF CONTENT -BASED MEASURES WITH COVERAGE IN DUC‚Äô04 T ASK 2
Mesure COVERAGE p-value
ROUGE -2 0.79p&lt;0:0050
JS 0.68p&lt;0:0025
TABLE III
SPEARMANOF CONTENT -BASED MEASURES IN DUC‚Äô04 T ASK 5
Mesure COVERAGE p-value RESPONSIVENESS p-value
ROUGE -2 0.78p&lt;0:001 0.44 p&lt;0:05
JS 0.40p&lt;0:050 -0.18 p&lt;0:25
TABLE IV
SPEARMANOF CONTENT -BASED MEASURES IN TAC‚Äô08 OS TASK
Mesure PYRAMIDS p-value RESPONSIVENESS p-value
JS -0.13p&lt;0:25 -0.14 p&lt;0:25
TABLE V
SPEARMANOF CONTENT -BASED MEASURES WITH ROUGE IN THE Medicina Cl ¬¥ƒ±nica CORPUS (SPANISH )
Mesure ROUGE -1p-value ROUGE -2p-value ROUGE -SU4p-value
JS 0.56p&lt;0:100 0.46p&lt;0:100 0.45p&lt;0:200
JS2 0.88p&lt;0:001 0.80p&lt;0:002 0.81p&lt;0:005
JS4 0.88p&lt;0:001 0.80p&lt;0:002 0.81p&lt;0:005
JSM 0.82p&lt;0:005 0.71p&lt;0:020 0.71p&lt;0:010
found weak correlation among different rankings in
complex summarization tasks such as the summarization
of biographical information and the summarization of
opinions.
‚Äì We have also carried out large-scale experiments in
Spanish and French which show positive medium to
strong correlation among system‚Äôs ranks produced by
ROUGE and divergence measures that do not use the
model summaries.
‚Äì We have also presented a new framework, F RESA , for
the computation of measures based on JS divergence.
Following the R OUGE approach, F RESA package use
word uni-grams, 2-grams and skip n-grams computing
divergences. This framework will be available to the
community for research purposes.
Although we have made a number of contributions, this paper
leaves many open questions than need to be addressed. In
order to verify correlation between R OUGE andJS, in the
short term we intend to extend our investigation to other
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also
plan to apply F RESA to the rest of the DUC and TAC
summarization tasks, by using several smoothing techniques.
As a novel idea, we contemplate the possibility of adapting
the evaluation framework for the phrase compression task
[29], which, to our knowledge, does not have an efÔ¨Åcient
evaluation measure. The main idea is to calculate JS from
an automatically-compressed sentence taking the complete
sentence by reference. In the long term, we plan to incorporatea representation of the task/topic in the calculation of
measures. To carry out these comparisons, however, we are
dependent on the existence of references.
FRESA will also be used in the new question-answer task
campaign INEX‚Äô2010 (http://www.inex.otago.ac.nz/tracks/qa/
qa.asp) for the evaluation of long answers. This task aims
to answer a question by extraction and agglomeration of
sentences in Wikipedia. This kind of task corresponds
to those for which we have found a high correlation
among the measures JS and evaluation methods with
human intervention. Moreover, the JS calculation will be
among the summaries produced and a representative set of
relevant passages from Wikipedia. F RESA will be used to
compare three types of systems, although different tasks: the
multi-document summarizer guided by a query, the search
systems targeted information (focused IR) and the question
answering systems.</conclusion>
  <discussion>The departing point for our inquiry into text summarization
evaluation has been recent work on the use of content-basedevaluation metrics that do not rely on human models but that
compare summary content to input content directly [12]. We
have some positive and some negative results regarding the
direct use of the full document in content-based evaluation.
We have veriÔ¨Åed that in both generic muti-document
summarization and in topic-based multi-document
summarization in English correlation among measures
that use human models (P YRAMIDS , R ESPONSIVENESS
and R OUGE ) and a measure that does not use models
(JS divergence) is strong. We have found that correlation
among the same measures is weak for summarization of
biographical information and summarization of opinions in
blogs. We believe that in these cases content-based measures
should be considered, in addition to the input document, the
summarization task (i.e. text-based representation, description)
to better assess the content of the peers [25], the task being a
determinant factor in the selection of content for the summary.
Our multi-lingual experiments in generic single-document
summarization conÔ¨Årm a strong correlation among the
JS divergence and R OUGE measures. It is worth noting
that R OUGE is in general the chosen framework for
presenting content-based evaluation results in non-English
summarization.
For the experiments in Spanish, we are conscious that we
only have one model summary to compare with the peers.
Nevertheless, these models are the corresponding abstracts
written by the authors. As the experiments in [26] show, the
professionals of a specialized domain (as, for example, the
medical domain) adopt similar strategies to summarize their
texts and they tend to choose roughly the same content chunks
for their summaries. Previous studies have shown that author
abstracts are able to reformulate content with Ô¨Ådelity [27] and
these abstracts are ideal candidates for comparison purposes.
Because of this, the summary of the author of a medical article
can be taken as reference for summaries evaluation. It is worth
noting that there is still debate on the number of models to be
used in summarization evaluation [28]. In the French corpus
PISTES, we suspect the situation is similar to the Spanish
case.</discussion>
  <bibliographie>[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, ‚ÄúSummac: a text summarization evaluation,‚Äù Natural
Language Engineering , vol. 8, no. 1, pp. 43‚Äì68, 2002.
[2] P. Over, H. Dang, and D. Harman, ‚ÄúDUC in context,‚Äù IPM, vol. 43,
no. 6, pp. 1506‚Äì1520, 2007.
[3]Proceedings of the Text Analysis Conference . Gaithesburg, Maryland,
USA: NIST, November 17-19 2008.
[4] K. Sp √§rck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review , ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.
[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, ‚ÄúA comparison of
rankings produced by summarization evaluation measures,‚Äù in NAACL
Workshop on Automatic Summarization , 2000, pp. 69‚Äì78.
[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, ‚ÄúMeta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,‚Äù in COLING 2002 , Taipei, Taiwan, August 2002, pp. 849‚Äì855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. √á elebi,
D. Liu, and E. Dr√°bek, ‚ÄúEvaluation challenges in large-scale document
summarization,‚Äù in ACL‚Äô03 , 2003, pp. 375‚Äì382.
[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, ‚ÄúBLEU: a method
for automatic evaluation of machine translation,‚Äù in ACL‚Äô02 , 2002, pp.
311‚Äì318.
[9] K. Pastra and H. Saggion, ‚ÄúColouring summaries BLEU,‚Äù in Evaluation
Initiatives in Natural Language Processing . Budapest, Hungary: EACL,
14 April 2003.
[10] C.-Y . Lin, ‚ÄúROUGE: A Package for Automatic Evaluation of
Summaries,‚Äù in Text Summarization Branches Out: ACL-04 Workshop ,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74‚Äì81.
[11] A. Nenkova and R. J. Passonneau, ‚ÄúEvaluating Content Selection in
Summarization: The Pyramid Method,‚Äù in HLT-NAACL , 2004, pp.
145‚Äì152.
[12] A. Louis and A. Nenkova, ‚ÄúAutomatically Evaluating Content Selection
in Summarization without Human Models,‚Äù in Empirical Methods in
Natural Language Processing , Singapore, August 2009, pp. 306‚Äì314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, ‚ÄúDivergence Measures based on the Shannon Entropy,‚Äù IEEE
Transactions on Information Theory , vol. 37, no. 145-151, 1991.
[14] C.-Y . Lin and E. Hovy, ‚ÄúAutomatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,‚Äù in HLT-NAACL . Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71‚Äì78.
[15] C.-Y . Lin, G. Cao, J. Gao, and J.-Y . Nie, ‚ÄúAn information-theoretic
approach to automatic evaluation of summaries,‚Äù in HLT-NAACL ,
Morristown, USA, 2006, pp. 463‚Äì470.
[16] S. Kullback and R. Leibler, ‚ÄúOn information and sufÔ¨Åciency,‚Äù Ann. of
Math. Stat. , vol. 22, no. 1, pp. 79‚Äì86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
Sciences . McGraw-Hill, 1998.[18] C. de Loupy, M. Gu√©gan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
‚ÄúA French Human Reference Corpus for multi-documents
summarization and sentence compression,‚Äù in LREC‚Äô10 , vol. 2,
Malta, 2010, p. In press.
[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, ‚ÄúTextual Energy
of Associative Memories: performants applications of Enertex algorithm
in text summarization and topic segmentation,‚Äù in MICAI‚Äô07 , 2007, pp.
861‚Äì871.
[20] J.-M. Torres-Moreno, P. Vel√°zquez-Morales, and J.-G. Meunier,
‚ÄúCondens√©s de textes par des m√©thodes num√©riques,‚Äù in JADT‚Äô02 , vol. 2,
St Malo, France, 2002, pp. 723‚Äì734.
[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Vel√°zquez-Morales,
‚ÄúAutomatic summarization using terminological and semantic
resources,‚Äù in LREC‚Äô10 , vol. 2, Malta, 2010, p. In press.
[22] J.-M. Torres-Moreno and J. Ramirez, ‚ÄúREG : un algorithme glouton
appliqu√© au r√©sum√© automatique de texte,‚Äù in JADT‚Äô10 . Rome, 2010,
p. In press.
[23] V . Yatsko and T. Vishnyakov, ‚ÄúA method for evaluating modern
systems of automatic text summarization,‚Äù Automatic Documentation
and Mathematical Linguistics , vol. 41, no. 3, pp. 93‚Äì103, 2007.
[24] C. D. Manning and H. Sch√ºtze, Foundations of Statistical Natural
Language Processing . Cambridge, Massachusetts: The MIT Press,
1999.
[25] K. Sp √§rck Jones, ‚ÄúAutomatic summarising: The state of the art,‚Äù IPM,
vol. 43, no. 6, pp. 1449‚Äì1481, 2007.
[26] I. da Cunha, L. Wanner, and M. T. Cabr√©, ‚ÄúSummarization of specialized
discourse: The case of medical articles in spanish,‚Äù Terminology , vol. 13,
no. 2, pp. 249‚Äì286, 2007.
[27] C.-K. Chuah, ‚ÄúTypes of lexical substitution in abstracting,‚Äù in ACL
Student Research Workshop . Toulouse, France: Association for
Computational Linguistics, 9-11 July 2001 2001, pp. 49‚Äì54.
[28] K. Owkzarzak and H. T. Dang, ‚ÄúEvaluation of automatic summaries:
Metrics under varying data conditions,‚Äù in UCNLG+Sum‚Äô09 , Suntec,
Singapore, August 2009, pp. 23‚Äì30.
[29] K. Knight and D. Marcu, ‚ÄúStatistics-based summarization-step one:
Sentence compression,‚Äù in Proceedings of the National Conference on
ArtiÔ¨Åcial Intelligence . Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2000, pp. 703‚Äì710.
Summary Evaluation with and without References
</bibliographie>

  <preamble>Torres-moreno1998.pdf</preamble>
  <titre>EfÔ¨Åcient Adaptive Learning for ClassiÔ¨Åcation Tasks with
Binary Units
</titre>
  <auteurs>
    <auteur>
      <name>J. Manuel Torres Moreno</name>
      <mail>Pas d'adresse mail</mail>
      <affiliation>D√©partement de Recherche Fondamentale sur la Mati√®re Condens√©e, CEA Grenoble,38054 Grenoble Cedex 9, France</affiliation>
    </auteur>
    <auteur>
      <name>Mirta B. Gordon</name>
      <mail>Pas d'adresse mail</mail>
      <affiliation>D√©partement de Recherche Fondamentale sur la Mati√®re Condens√©e, CEA Grenoble,38054 Grenoble Cedex 9, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>This article presents a new incremental learning algorithm for classi-
Ô¨Åcation tasks, called NetLines, which is well adapted for both binaryand real-valued input patterns. It generates small, compact feedforwardneural networks with one hidden layer of binary units and binary outputunits. A convergence theorem ensures that solutions with a Ô¨Ånite num-ber of hidden units exist for both binary and real-valued input patterns.An implementation for problems with more than two classes, validfor any binary classiÔ¨Åer, is proposed. The generalization error andthe size of the resulting networks are compared to the best publishedresults on well-known classiÔ¨Åcation benchmarks. Early stopping is shownto decrease overÔ¨Åtting, without improving the generalization perfor-mance.</abstract>
  <introduction>
Feedforward neural networks have been successfully applied to the prob-
lem of learning pattern classiÔ¨Åcation from examples. The relationship of thenumber of weights to the learning capacity and the network‚Äôs generalizationability is well understood only for the simple perceptron, a single binaryunit whose output is a sigmoidal function of the weighted sum of its inputs.In this case, efÔ¨Åcient learning algorithms based on theoretical results allowthe determination of the optimal weights. However, simple perceptrons cangeneralize only those (very few) problems in which the input patterns arelinearly separable (LS). In many actual classiÔ¨Åcation tasks, multilayered per-ceptrons with hidden units are needed. However, neither the architecture(number of units, number of layers) nor the functions that hidden unitshave to learn are known a priori, and the theoretical understanding of thesenetworks is not enough to provide useful hints.
Although pattern classiÔ¨Åcation is an intrinsically discrete task, it may be
cast as a problem of function approximation or regression by assigning realvalues to the targets. This is the approach used by backpropagation and
Neural Computation 10, 1007‚Äì1030 (1998) c¬∞1998 Massachusetts Institute of Technology
related algorithms, which minimize the squared training error of the out-
put units. The approximating function must be highly nonlinear because ithas to Ô¨Åt a constant value inside the domains of each class and present alarge variation at the boundaries between classes. For example, in a binaryclassiÔ¨Åcation task in which the two classes are coded as C1 and¬°1, the
approximating function must be constant and positive in the input spaceregions or domains corresponding to class 1 and constant and negativefor those of class¬°1. The network‚Äôs weights are trained to Ô¨Åt this function
everywhere‚Äîin particular, inside the class domains‚Äîinstead of concentrat-ing on the relevant problem of the determination of the frontiers betweenclasses. Because the number of parameters needed for the Ô¨Åt is not knowna priori, it is tempting to train a large number of weights that can span, atleast in principle, a large set of functions expected to contain the ‚Äútrue‚Äù one.This introduces a small bias (Geman, Bienenstock, &amp; Doursat, 1992), butleaves us with the difÔ¨Åcult problem of minimizing a cost function in a high-dimensional space, with the risk that the algorithm gets stuck in spuriouslocal minima, whose number grows with the number of weights. In prac-tice, the best generalizer is determined through a trial-and-error process inwhich both the numbers of neurons and weights are varied.
An alternative approach is provided by incremental, adaptive, or growth
algorithms, in which the hidden units are successively added to the network.One advantage is fast learning, not only because the problem is reduced totraining simple perceptrons but also because adaptive procedures do notneed the trial-and-error search for the most convenient architecture. Growthalgorithms allow the use of binary hidden neurons, well suited for buildinghardware-dedicated devices. Each binary unit determines a domain bound-ary in input space. Patterns lying on either side of the boundary are givendifferent hidden states. Thus, all the patterns inside a domain in input spaceare mapped to the same internal representation (IR). This binary encoding isdifferent for each domain. The output unit performs a logic (binary) functionof these IRs, a feature that may be useful for rule extraction. Because thereis not a unique way of associating IRs to the input patterns, different incre-mental learning algorithms propose different targets to be learned by theappended hidden neurons. This is not the only difference. Several heuristicsexist that generate fully connected feedforward networks with one or morelayers, and treelike architectures with different types of neurons (linear, ra-dial basis functions). Most of these algorithms are not optimal with respectto the number of weights or hidden units. Indeed, growth algorithms haveoften been criticized because they may generate networks that are too large,generally believed to be poor generalizers because of overÔ¨Åtting.
This article presents a new incremental learning algorithm for binary
classiÔ¨Åcation tasks that generates small feedforward networks. These net-works have a single hidden layer of binary neurons fully connected to theinputs and a single output neuron connected to the hidden units. We callitNetLines , for Neural Encoder Through Linear Separations. During the
learning process, the targets that each appended hidden unit has to learn
help to decrease the number of classiÔ¨Åcation errors of the output neuron.The crucial test for any learning algorithm is the generalization ability ofthe resulting network. It turns out that the networks built with NetLines aregenerally smaller and generalize better than the best networks found so faron well-known benchmarks. Thus, large networks do not necessarily fol-low from growth heuristics. On the other hand, although smaller networksmay be generated with NetLines through early stopping, we found thatthey do not generalize better than the networks that were trained until thenumber of training errors vanished. Thus, overÔ¨Åtting does not necessarilyspoil the network‚Äôs performance. This surprising result is in good agreementwith recent work on the bias-variance dilemma (Friedman, 1996) showingthat, unlike in regression problems where bias and variance compete in thedetermination of the optimal generalizer, in the case of classiÔ¨Åcation theycombine in a highly nonlinear way.
Although NetLines creates networks for two-class problems, multiclass
problems may be solved using any strategy that combines binary classiÔ¨Åers,like winner-takes-all. We propose a more involved approach, through theconstruction of a tree of networks, that may be coupled with any binaryclassiÔ¨Åer.
NetLines is an efÔ¨Åcient approach for creating small, compact classiÔ¨Åers
for problems with binary or continuous inputs. It is best suited for problemsrequiring a discrete classiÔ¨Åcation decision. Although it may estimate poste-rior probabilities, as discussed in section 2.6, this requires more informationthan the bare network‚Äôs output. Another weakness of NetLines is that it isnot simple to retrain the network when new patterns are available or classpriors change over time.
In section 2, we give the basic deÔ¨Ånitions and present a simple example
of our strategy, followed by the formal presentation of the growth heuristicsand the perceptron learning algorithm used to train the individual units.In section 3 we compare NetLines to other growth strategies. The construc-tion of trees of networks for multiclass problems is presented in section 4.A comparison of the generalization error and the network‚Äôs size, with re-sults obtained with other learning procedures, is presented in section 5. Theconclusions are set out in section 6.
</introduction>
  <corps>2.1 DeÔ¨Ånitions. We are given a training set of Pinput-output examples
fE¬ª‚Äû;¬ø‚Äûg, where‚ÄûD1;2;:::; P. The inputsE¬ª‚ÄûD.1;¬ª‚Äû
1;¬ª‚Äû
2;:::;¬ª‚Äû
N/may be
binary or real valued NC1 dimensional vectors. The Ô¨Årst component ¬ª‚Äû
0¬∑1,
the same for all the patterns, allows us to treat the bias as a supplementaryweight. The outputs are binary, ¬ø
‚ÄûD¬ß 1. These patterns are used to learn
the classiÔ¨Åcation task with the growth algorithm. Assume that, at a givenstage of the learning process, the network already has hbinary neurons
in the hidden layer. These neurons are connected to the NC1 input units
through synaptic weights EwkD.wk0;wk1¬¢¬¢¬¢wkN/,1‚Ä¢k‚Ä¢h,wk0being the
bias.
Then, given an input pattern E¬ª, the states¬ækof the hidden neurons (1 ‚Ä¢
k‚Ä¢h) given by
¬ækDsignÀÜNX
iD0wki¬ªi!
¬∑sign.Ewk¬¢E¬ª/ (2.1)
deÔ¨Åne the pattern‚Äôs h-dimensional IR,E¬æ.h/D.1;¬æ1;:::;¬æ h/. The network‚Äôs
output‚Ä°.h/is:
‚Ä°.h/DsignÀÜhX
kD0Wk¬æk!
¬∑signh
EW.h/¬¢E¬æ.h/i
(2.2)
whereEW.h/D.W0;W1;:::; Whare the output unit weights. Hereafter,
E¬æ‚Äû.h/D.1;¬æ‚Äû
1;:::;¬æ‚Äû
h/is the h-dimensional IR associated by the network
ofhhidden units to pattern E¬ª‚Äû. During the training process, hincreases
through the addition of hidden neurons, and we denote the Ô¨Ånal numberof hidden units as H.
2.2 Example. We Ô¨Årst describe the general strategy on a schematic ex-
ample (see Figure 1). Patterns in the gray region belong to class ¬øDC1, the
others to¬øD¬° 1. The algorithm proceeds as follows. A Ô¨Årst hidden unit
is trained to separate the input patterns at best and Ô¨Ånds one solution, say
Ew
1, represented on Figure 1 by the line labeled 1, with the arrow pointing
into the positive half-space. Because training errors remain, a second hid-den neuron is introduced. It is trained to learn targets ¬ø
2DC1 for patterns
well classiÔ¨Åed by the Ô¨Årst neuron and ¬ø2D¬° 1 for the others (the opposite
convention could be adopted, both being strictly equivalent), and supposethat solutionEw
2is found. Then an output unit is connected to the two hid-
den neurons and is trained with the original targets. Clearly it will fail toseparate all the patterns correctly because the IR .¬°1;1/and.C¬°/are not
faithful, as patterns of both classes are mapped onto them. The output neu-ron is dropped, and a third hidden unit is appended and trained with targets¬ø
3DC1 for patterns that were correctly classiÔ¨Åed by the output neuron and
¬ø3D¬° 1 for the others. Solution Ew3is found, and it is easy to see that now
the IRs are faithful, that is, patterns belonging to different classes are givendifferent IRs. The algorithm converged with three hidden units that deÔ¨Ånethree domain boundaries determining six regions or domains in the inputspace. It is straightforward to verify that the IRs corresponding to each do-main on Figure 1 are linearly separable. Thus, the output unit will Ô¨Ånd thecorrect solution to the training problem. If the faithful IRs were not linearlyseparable, the output unit would not Ô¨Ånd a solution without training errors,and the algorithm would go on appending hidden units that should learn
3
1
2- + -
+ - -- + +
+ + +
+ - ++ + -
Figure 1: Patterns inside the gray region belong to one class, those in the white
region to the other. The lines (labeled 1, 2, and 3) represent the hyperplanes foundwith the NetLines strategy. The arrows point into the correspondent positivehalf-spaces. The IRs of each domain are indicated (the Ô¨Årst component, ¬æ
0D1,
is omitted for clarity).
targets¬øD1 for well-learned patterns, and ¬øD¬°1 for the others. A proof
that a solution to this strategy with a Ô¨Ånite number of hidden units exists isleft to the appendix.
2.3 The Algorithm NetLines. Like most other adaptive learning algo-
rithms, NetLines combines a growth heuristics with a particular learningalgorithm for training the individual units, which are simple perceptrons.In this section, we present the growth heuristics Ô¨Årst, followed by the de-scription of Minimerror, our perceptron learning algorithm.
We Ô¨Årst introduce the following useful remark: if a neuron has to learn a
target¬ø, and the learned state turns out to be ¬æ, then the product ¬æ¬øD1i f
the target has been correctly learned, and ¬æ¬øD¬°1 otherwise.
Given a maximal accepted number of hidden units, H
max, and a maximal
number of tolerated training errors, Emax, the Netlines algorithm may be
summarized as follows:
Algorithm.
‚Ä†Initialize
hD0;
setthe targets¬ø‚Äû
hC1D¬ø‚Äûfor‚ÄûD1;:::; P;
‚Ä†Repeat
1. /* train the hidden units */
hDhC1; /* connect hidden unit hto the inputs */
learn the training setfE¬ª‚Äû;¬ø‚Äû
hg,‚ÄûD1;:::; P;
after learning, ¬æ‚Äû
hDsign.Ewh¬¢E¬ª‚Äû/,‚ÄûD1;:::; P;
ifhD1 /* for the Ô¨Årst hidden neuron */
if¬æ‚Äû
1D¬ø‚Äû
18‚Äûthen stop. /* the training set is LS */;
else set¬ø‚Äû
hC1D¬æ‚Äû
h¬ø‚Äûfor‚ÄûD1;:::; P;go to 1;
end if
2. /* learn the mapping between the IRs and the outputs */
connect the output neuron to the htrained hidden units;
learn the training setfE¬æ‚Äû.h/;¬ø‚Äûg;‚ÄûD1;:::; P;
after learning, ‚Ä°‚Äû.h/Dsign‚Ä°
EW.h/¬¢E¬æ‚Äû¬∑
,‚ÄûD1;:::; P;
set¬ø‚Äû
hC1D‚Ä°‚Äû¬ø‚Äûfor‚ÄûD1;:::; P;
count the number of training errors eDP
‚Äû.1¬°¬ø‚Äû
hC1/=2;
‚Ä†Until.hDHmaxore‚Ä¢Emax/;
The generated network has HDhhidden units. In the appendix we present
a solution to the learning strategy with a bounded number of hidden units.In practice, the algorithm ends up with much smaller networks than thisupper bound, as will be shown in section 5.
2.4 The Perceptron Learning Algorithm. The Ô¨Ånal number of hidden
neurons, which are simple perceptrons, depends on the performance of thelearning algorithm used to train them. The best solution should minimizethe number of errors. If the training set is LS, it should endow the units withthe lowest generalization error. Our incremental algorithm uses Minimerror(Gordon &amp; Berchier, 1993) to train the hidden and output units. Minimer-ror is based on the minimization of a cost function Ethat depends on the
perceptron weights Ewthrough the stabilities of the training patterns. If the
input vector isE¬ª
‚Äûand¬ø‚Äûthe corresponding target, then the stability ¬∞‚Äûof
pattern‚Äûis a continuous and derivable function of the weights, given by:
¬∞‚ÄûD¬ø‚ÄûEw¬¢E¬ª‚Äû
kEwk; (2.3)
wherekEwkDp
Ew¬¢Ew. The stability is independent of the norm of the weights
kEwk. It measures the distance of the pattern to the separating hyperplane,
which is normal to Ew; it is positive if the pattern is well classiÔ¨Åed, negative
otherwise. The cost function Eis:
ED1
2PX
‚ÄûD1‚Ä¢
1¬°tanh¬∞‚Äû
2T‚Äö
: (2.4)
The contribution to Eof patterns with large negative stabilities is ‚Äô1, that
is, they are counted as errors, whereas the contribution of patterns withlarge, positive stabilities is vanishingly small. Patterns at both sides of thehyperplane within a window of width ‚Ä¶4Tcontribute to the cost function
even if they have positive stability.
The properties of the global minimum of equation 2.4 have been studied
theoretically with methods of statistical mechanics (Gordon &amp; Grempel,1995). It was shown that in the limit T!0, the minimum of Ecorresponds
to the weights that minimize the number of training errors. If the trainingset is LS, these weights are not unique (Gyorgyi &amp; Tishby, 1990). In that case,there is an optimal learning temperature such that the weights minimizingEat that temperature endow the perceptron with a generalization error
numerically indistinguishable from the optimal (Bayesian) value.
The algorithm Minimerror (Gordon &amp; Berchier, 1993; RafÔ¨Ån &amp; Gordon,
1995) implements a minimization of Erestricted to a subspace of normalized
weights, through a gradient descent combined with a slow decrease of thetemperature T, which is equivalent to a deterministic annealing. It has been
shown that the convergence is faster if patterns with negative stabilities areconsidered at a temperature T
¬°larger than those with positive stabilities,
TC, with a constant ratio ¬µDT¬°=TC. The weights and the temperatures are
iteratively updated through:
‚ÄìEw.t/D‚Ä†"X
‚Äû=¬∞‚Äû‚Ä¢0¬ø‚ÄûE¬ª‚Äû
cosh2.¬∞‚Äû=2T¬°/CX
‚Äû=¬∞‚Äû&gt;0¬ø‚ÄûE¬ª‚Äû
cosh2.¬∞‚Äû=2TC/#
(2.5)
T¬°1
C.tC1/DT¬°1
C.t/C‚ÄìT¬°1IT¬°D¬µTCI (2.6)
Ew.tC1/Dp
NC1Ew.t/C‚ÄìEw.t/
kEw.t/C‚ÄìEw.t/k: (2.7)
Notice from equation 2.5 that only the incorrectly learned patterns at dis-
tances shorter than ‚Ä¶2T¬°from the hyperplane, and those correctly learned
lying closer than‚Ä¶2TC, contribute effectively to learning. The contribu-
tion of patterns outside this region is vanishingly small. By decreasing thetemperature, the algorithm selects to learn patterns increasingly localizedin the neighborhood of the hyperplane, allowing for a highly precise de-termination of the parameters deÔ¨Åning the hyperplane, which are the neu-ron‚Äôs weights. Normalization 2.7 restricts the search to the subspace withkEwkDp
NC1.
The only adjustable parameters of the algorithm are the temperature ratio
¬µDT¬°=TC, the learning rate ‚Ä†, and the annealing rate ‚ÄìT¬°1. In principle,
they should be adapted to each speciÔ¨Åc problem. However, as a result of
our normalizing the weights top
NC1 and to data standardization (see the
next section), all the problems are brought to the same scale, simplifying thechoice of the parameters.
2.5 Data Standardization. Instead of determining the best parameters
for each new problem, we standardize the input patterns of the training setthrough a linear transformation, applied to each component:
Q¬ª
‚Äû
iD¬ª‚Äû
i¬°h¬ªii
1iI1‚Ä¢i‚Ä¢N: (2.8)
The meanh¬ªiiand the variance42
i, deÔ¨Åned as usual,
h¬ªiiD1
PPX
‚ÄûD1¬ª‚Äû
i (2.9)
1i2D1
PPX
‚ÄûD1.¬ª‚Äû
i¬°h¬ªii/2D1
PPX
‚ÄûD1.¬ª‚Äû
i/2¬°.h¬ªii/2; (2.10)
need only a single pass of the Ptraining patterns to be determined. After
learning, the inverse transformation is applied to the weights,
Qw0Dp
NC1w0¬°NP
iD1wih¬ªii=1i
rh
w0¬°PN
jD1wjh¬ªji=1ji2
CPN
jD1.wj=1j/2(2.11)
QwiDp
NC1wi=1irh
w0¬°PN
jD1wjh¬ªji=1ji2
CPN
jD1.wj=1j/2; (2.12)
so that the normalization (see equation 2.8) is completely transparent to the
user: with the transformed weights (see equations 2.11 and 2.12), the neuralclassiÔ¨Åer is applied to the data in the original user‚Äôs units, which do notneed to be renormalized.
As a consequence of the weights scaling (see equation 2.7) and the in-
puts standardization (see equation 2.8), all the problems are automaticallyrescaled. This allows us to use always the same values of Minimerror‚Äôs pa-rameters: the standard values ‚Ä†D0:02,‚ÄìT
¬°1D10¬°3, and¬µD6. They were
used throughout this article, the reported results being highly insensitive toslight variations of them. However, in some extremely difÔ¨Åcult cases, likelearning the parity in dimensions N&gt;10 and Ô¨Ånding the separation of the
sonar signals (see section 5), larger values of ¬µwere needed.
2.6 Interpretation. It has been shown (Gordon, Peretto, &amp; Berchier, 1993)
that the contribution of each pattern to the cost function of Minimerror,[1¬°tanh.¬∞
‚Äû=2T/]=2, may be interpreted as the probability of misclassiÔ¨Åca-
tion at the temperature Tat which the minimum of the cost function has
been determined. By analogy, the neuron‚Äôs prediction on a new input E¬ªmay
be given a conÔ¨Ådence measure by replacing the (unknown) pattern stabil-ity by its absolute value k¬∞kDkEw¬¢E¬ªk=kEwk, which is its distance to the
hyperplane. This interpretation of the sigmoidal function tanh .k¬∞k=2T/as
the conÔ¨Ådence on the neuron‚Äôs output is similar to the one proposed earlier(Goodman, Smyth, Higgins, &amp; Miller, 1992) within an approach based oninformation theory.
The generalization of these ideas to multilayered networks is not straight-
forward. An estimate of the conÔ¨Ådence on the classiÔ¨Åcation by the outputneuron should include the magnitude of the weighted sums of the hiddenneurons, as they measure the distances of the input pattern to the domainboundaries. However, short distances to the separating hyperplanes are notalways correlated to low conÔ¨Ådence on the network‚Äôs output. For an exam-ple, we refer again to Figure 1. Consider a pattern lying close to hyperplane1. A small, weighted sum on neuron 1 may cast doubt on the classiÔ¨Åcationif the pattern‚Äôs IR is ( ¬°CC ) but not if it is (¬°C¬° ), because a change of the
sign of the weighted sum in the latter case will map the pattern to the IR(CC¬° ) which, being another IR of the same class, will be given the same
output by the network. It is worth noting that the same difÔ¨Åculty is met bythe interpretation of the outputs of multilayered perceptrons, trained withbackpropagation, as posterior probabilities. We do not explore this problemany further because it is beyond the scope of this article.
3 Comparison with Other Strategies
There are few learning algorithms for neural networks composed of binary
units. To our knowledge, all of them are incremental. In this section, wegive a short overview of some of them, in order to put forward the maindifferences with NetLines. We discuss the growth heuristics and then theindividual unit training algorithms.
The Tiling algorithm (M√©zard &amp; Nadal, 1989) introduces hidden layers,
one after the other. The Ô¨Årst neuron of each layer is trained to learn an IR thathelps to decrease the number of training errors; supplementary hidden unitsare then appended to the layer until the IRs of all the patterns in the train-ing set are faithful. This procedure may generate very large networks. TheUpstart algorithm (Frean, 1990) introduces successive couples of daughterhidden units between the input layer and the previously included hiddenunits, which become their parents. The daughters are trained to correctthe parents‚Äô classiÔ¨Åcation errors, one daughter for each class. The obtainednetwork has a treelike architecture. There are two different algorithms im-plementing the Tilinglike Learning in the Parity Machine (Biehl &amp; Opper,
1991), Offset (Martinez &amp; Est√®ve, 1992), and MonoPlane (Torres Moreno &amp;
Gordon, 1995). In both, each appended unit is trained to correct the errorsof the previously included unit in the same hidden layer, a procedure thathas been shown to generate a parity machine: the class of the input patternsis the parity of the learned IRs. Unlike Offset, which implements the paritythrough a second hidden layer that needs to be pruned, MonoPlane goeson adding hidden units (if necessary) in the same hidden layer until thenumber of training errors at the output vanishes. Convergence proofs forbinary input patterns have been produced for all these algorithms. In thecase of real-valued input patterns, a solution to the parity machine with abounded number of hidden units also exists (Gordon, 1996).
The rationale behind the construction of the parity machine is that it
is not worth training the output unit before all the training errors of thehidden units have been corrected. However, Marchand, Golea, and Ruj√°n
(1990) pointed out that it is not necessary to correct all the errors of thesuccessively trained hidden units. It is sufÔ¨Åcient that the IRs be faithful andLS. If the output unit is trained immediately after each appended hiddenunit, the network may discover that the IRs are already faithful and stopadding units. This may be seen in Figure 1. None of the parity machineimplementations would Ô¨Ånd the solution represented on the Ô¨Ågure, becauseeach of the three perceptrons systematically unlearns part of the patternslearned by the preceding one.
To our knowledge,
Sequential Learning (Marchand et al., 1990) is the
only incremental learning algorithm that might Ô¨Ånd a solution equivalent(although not the same) to the one of Figure 1. In this algorithm, the Ô¨Årstunit is trained to separate the training set keeping one ‚Äúpure‚Äù half-space‚Äîcontaining patterns of only one class. Wrongly classiÔ¨Åed patterns, if any,must all lie in the other half-space. Each appended neuron is trained toseparate wrongly classiÔ¨Åed patterns with this constraint of always keepingone pure, error-free half-space. Thus, neurons must be appended in a preciseorder, making the algorithm difÔ¨Åcult to implement in practice. For example,
Sequential Learning applied to the problem of Figure 1 needs to impose that
the Ô¨Årst unit Ô¨Ånds the weights Ew3, the only solution satisfying the purity
restriction.
Other proposed incremental learning algorithms strive to solve the prob-
lem with different architectures, and/or with real valued units. For example,in the algorithm
Cascade Correlation (Fahlman &amp; Lebiere, 1990), each ap-
pended unit is selected among a pool of several real-valued neurons, trainedto learn the correlation between the targets and the training errors. The unitis then connected to the input units and to all the other hidden neuronsalready included in the network.
Another approach to learning classiÔ¨Åcation tasks is through the construc-
tion of decision trees (Breiman, Friedman, Olshen, &amp; Stone, 1984), which hi-erarchically partition the input space through successive dichotomies. Theneural networks implementations generate treelike architectures. Each neu-
ron of the tree introduces a dichotomy of the input space, which is treated
separately by the children nodes, which eventually produce new splits. Be-sides the weights, the resulting networks need to store the decision path.The proposed heuristics (Sirat &amp; Nadal, 1990; Farrell &amp; Mammone, 1994;Knerr, Personnaz, &amp; Dreyfus, 1990) differ in the algorithm used to train eachnode and/or in the stopping criterion. In particular, Neural-Trees (Sirat &amp;Nadal, 1990) may be regarded as a generalization of ClassiÔ¨Åcation and Re-gression Trees (CART) (Breiman et al., 1984) in which the hyperplanes arenot constrained to be perpendicular to the coordinate axis. The heuristics ofthe ModiÔ¨Åed Neural Tree Network (MNTN) (Farrell &amp; Mammone, 1994),similar to Neural-Trees, includes a criterion of early stopping based on aconÔ¨Ådence measure of the partition. As NetLines considers the whole inputspace to train each hidden unit, it generates domain boundaries that maygreatly differ from the splits produced by trees. We are not aware of anysystematic study or theoretical comparison of both approaches.
Other algorithms, like Restricted Coulomb Energy (RCE) (Reilly, Cooper,
&amp; Elbaum, 1982), Grow and Learn (GAL) (Alpaydin, 1990), Glocal (Depe-nau, 1995), and Growing Cells (Fritzke, 1994), propose to cover or mask theinput space with hyperspheres of adaptive size containing patterns of thesame class. These approaches generally end up with a very large number ofunits. Covering Regions by the LP Method (Mukhopadhyay, Roy, Kim, &amp;Govil, 1993) is a trial-and-error procedure devised to select the most efÔ¨Åcientmasks among hyperplanes, hyperspheres, and hyperellipsoids. The mask‚Äôsparameters are determined through linear programming.
Many incremental strategies use the Pocket algorithm (Gallant, 1986)
to train the appended units. Its main drawback is that it has no naturalstopping condition, which is left to the user‚Äôs patience. The proposed alter-native algorithms (Frean, 1992; Bottou &amp; Vapnik, 1992) are not guaranteedto Ô¨Ånd the best solution to the problem of learning. The algorithm used bythe MNTN (Farrell &amp; Mammone, 1994) and the ITRULE (Goodman et al.,1992) minimize cost functions similar to equation 2.4, but using differentmisclassiÔ¨Åcation measures at the place of our stability (see equation 2.3).The essential difference with Minimerror is that none of these algorithms isable to control which patterns contribute to learning, as Minimerror doeswith the temperature.
4 Generalization to Multiclass Problems
The usual way to cope with problems having more than two classes is to
generate as many networks as classes. Each network is trained to separatepatterns of one class from all the others, and a winner-takes-all (WTA) strat-egy based on the value of the output‚Äôs weighted sum in equation 2.2 is usedto decide the class if more than one network recognizes the input pattern. Inour case, because we use normalized weights, the output‚Äôs weighted sumis merely the distance of the IR to the separating hyperplane. All the pat-
terns mapped to the same IR are given the same output‚Äôs weighted sum,
independent of the relative position of the pattern in input space. A strongweighted sum on the output neuron is not inconsistent with small weightedsums on the hidden neurons. Therefore, a naive WTA decision may not givegood results, as shown in the example in section 5.3.1.
We now describe an implementation for the multiclass problem that re-
sults in a treelike architecture of networks. It is more involved than the naiveWTA and may be applied to any binary classiÔ¨Åer. Suppose that we have aproblem with Cclasses. We must choose in which order the classes will
be learned, say .c
1;c2;:::; cC/. This order constitutes a particular learning
sequence. Given a particular learning sequence, a Ô¨Årst network is trainedto separate class c
1, which is given output target ¬ø1DC 1, from the others
(which are given targets ¬ø1D¬° 1). The opposite convention is equivalent
and could equally be used. After training, all the patterns of class c1are
eliminated from the training set, and we generate a second network trainedto separate patterns of class c
2from the remaining classes. The procedure,
reiterated with training sets of decreasing size, generates C¬°1 hierarchi-
cally organized tree of networks (TON): the outputs are ordered sequences
E‚Ä°D.‚Ä°1;‚Ä°2;:::;‚Ä° C¬°1/. The predicted class of a pattern is ci, where iis the
Ô¨Årst network in the sequence having an output C1(‚Ä°iDC1 and‚Ä°jD¬°1 for
j&lt;i), the outputs of the networks with j&gt;ibeing irrelevant.
The performance of the TON may depend on the chosen learning se-
quence. Therefore, it is convenient that an odd number of TONs, trainedwith different learning sequences, compete through a vote. We veriÔ¨Åed em-pirically, as is shown in section 5.3, that this vote improves the results ob-tained with each of the individual TONs participating in the vote. Noticethat our procedure is different from bagging (Breiman, 1994); all the net-works of the TON are trained with the same training set, without the need
of any resampling procedure.
5 Applications
Although convergence proofs of learning algorithms are satisfactory on the-
oretical grounds, they are not a guarantee of good generalization. In fact,they demonstrate only that correct learning is possible; they do not addressthe problem of generalization. This last issue still remains quite empirical(Vapnik, 1992; Geman et al., 1992; Friedman, 1996), and the generalizationperformance of learning algorithms is usually tested on well-known bench-marks (Prechelt, 1994).
We Ô¨Årst tested the algorithm on learning the parity function of Nbits for
2‚Ä¢N‚Ä¢11. It is well known that the smallest network with the architecture
considered here needs HDNhidden neurons. The optimal architecture
was found in all the cases. Although this is quite an unusual performance,the parity is not a representative problem: learning is exhaustive, and gen-eralization cannot be tested. Another test, the classiÔ¨Åcation of sonar signals
(Gorman &amp; Sejnowski, 1988), revealed the quality of Minimerror, as it solved
the problem without hidden units. In fact, we found that not only the train-ing set of this benchmark is linearly separable, a result already reported(Hoehfeld &amp; Fahlman, 1991; Roy, Kim, &amp; Mukhopadhyay, 1993), but thatthe complete database‚Äîthe training and the test sets together‚Äîis also lin-early separable (Torres Moreno &amp; Gordon, 1998).
We next present our results, generalization error ‚Ä†
gand number of weights,
on several benchmarks corresponding to different kinds of problems: binaryclassiÔ¨Åcation of binary input patterns, binary classiÔ¨Åcation of real-valuedinput patterns, and multiclass problems. These benchmarks were chosenbecause they have already served as a test for many other algorithms, pro-viding us with unbiased results for comparison. The generalization error‚Ä†
gof NetLines was estimated as usual, through the fraction of misclassiÔ¨Åed
patterns on a test set of data.
The results are reported as a function of the training sets sizes Pwhenever
these sizes are not speciÔ¨Åed by the benchmark. Besides the generalizationerror‚Ä†
g, averaged over a (speciÔ¨Åed) number of classiÔ¨Åers trained with ran-
domly selected training sets, we also present the number of weights of thecorresponding networks which is a measure of the classiÔ¨Åer‚Äôs complexity,as it corresponds to the number of its parameters.
Training times are usually cited among the characteristics of the training
algorithms. Only the numbers of epochs used by backpropagation on twoof the studied benchmarks have been published; we restrict the comparisonto these cases. As NetLines updates only Nweights per epoch, whereas
backpropagation updates all the network‚Äôs weights, we compare the totalnumber of weights updates. They are of the same order of magnitude forboth algorithms. However, these comparisons should be taken with cau-tion. NetLines is a deterministic algorithm; it learns the architecture andthe weights through a single run, whereas with backpropagation severalarchitectures must be previously investigated, and this time is not includedin the training time.
The following notation is used: Dis the total number of available patterns,
Pthe number of training patterns, and Gthe number of test patterns.
5.1 Binary Inputs. The case of binary input patterns has the property,
not shared by real-valued inputs, that every pattern may be separated fromthe others by a single hyperplane. This solution, usually called grandmother ,
needs as many hidden units as patterns in the training set. In fact, the conver-gence proofs for incremental algorithms in the case of binary input patternsare based on this property.
5.1.1 Monk‚Äôs Problem. This benchmark, thoroughly studied with many
different learning algorithms (Trhun et al., 1991), contains three distinctproblems. Each has an underlying logical proposition that depends on sixdiscrete variables, coded with ND17 binary numbers. The total number of
possible input patterns is DD432, and the targets correspond to the truth ta-
ble of the corresponding proposition. Both NetLines and MonoPlane foundthe underlying logical proposition of the Ô¨Årst two problems; they general-ized correctly, giving ‚Ä†
gD0. In fact, these are easy problems: all the neural
network‚Äìbased algorithms, and some nonneural learning algorithms werereported to generalize them correctly. In the third Monk‚Äôs problem, 6 pat-terns among the P
3D122 examples are given wrong targets. The general-
ization error is calculated over the complete set of DD432 patterns, that is,
including the training patterns, but in the test set all the patterns are giventhe correct targets. Thus, any training method that learns the training setcorrectly will make at least 1 :4% of generalization errors. Four algorithms
specially adapted to noisy problems were reported to reach ‚Ä†
gD0. However,
none of them generalizes correctly the two other (noiseless) Monk‚Äôs prob-lems. Besides them, the best performance, ‚Ä†
gD0:0277, which corresponds
to 12 misclassiÔ¨Åed patterns, is reached only by neural networks methods:backpropagation, backpropagation with weight decay, cascade correlation,and NetLines. The number of hidden units generated with NetLines (58weights) is intermediate between backpropagation with weight decay (39)and cascade correlation (75) or backpropagation (77). MonoPlane reached aslightly worse performance ( ‚Ä†
gD0:0416, or 18 misclassiÔ¨Åed patterns) with
the same number of weights as NetLines, showing that the parity machineencoding may not be optimal.
5.1.2 Two or More Clumps. In this problem (Denker et al., 1987) the net-
work has to discriminate if the number of clumps in a ring of Nbits is strictly
smaller than 2 or not. One clump is a sequence of identical bits bounded bybits of the other kind. The patterns are generated through a Monte Carlomethod in which the mean number of clumps is controlled by a parameterk(M√©zard &amp; Nadal, 1989). We generated training sets of Ppatterns with
kD3, corresponding to a mean number of clumps of ‚Ä¶1:5, for rings of
ND10 and ND25 bits. The generalization error corresponding to sev-
eral learning algorithms, estimated with independently generated testingsets of the same sizes as the training sets, GDP, are displayed in Figure 2
as a function of P. Points with error bars correspond to averages over 25
independent training sets. Points without error bars correspond to best re-sults. NetLines, MonoPlane, and Upstart for ND25 have nearly the same
performances when trained to reach error-free learning.
We tested the effect of early stopping by imposing on NetLines a maximal
number of two hidden units ( HD2). The residual training error ‚Ä†
tis plotted
on Figure 2, as a function of P. Note that early stopping does not help to de-
crease‚Ä†g. OverÔ¨Åtting, which arises when NetLines is applied until error-free
training is reached, does not degrade the network‚Äôs generalization perfor-mance. This behavior is very different from the one of networks trainedwith backpropagation. The latter reduces classiÔ¨Åcation learning to a regres-sion problem, in which the generalization error can be decomposed in two
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13 /G19/G13/G13/G31/G48/G57/G2F/G4C/G51/G48/G56 Œµ/G57/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G2B/G20/G15/G0C
/G31/G48/G57/G2F/G4C/G51/G48/G56/G37/G4C/G4F/G4C/G51/G4A/G0F/G2A/G55/G52/G5A/G57/G4B
/G38/G53/G56/G57/G44/G55/G57
/G33/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G15/G18
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13/G13/G11/G13/G13/G11/G14/G13/G11/G15/G13/G11/G16/G13/G11/G17/G13/G11/G18
Œµ/G4A
/G33/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G14/G13/G25/G44/G46/G4E/G53/G55/G52/G53
/G36/G57/G48/G53/G5A/G4C/G56/G48
/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56
Figure 2: Two or more clumps for two ring sizes, ND10 and ND25. Gen-
eralization error ‚Ä†gversus size of the training set P, for different algorithms.
ND10: backpropagation (Solla, 1989), Stepwise (Knerr et al., 1990). ND25:
Tiling (M√©zard &amp; Nadal, 1989), Upstart (Frean, 1990). Results with the Growth
Algorithm (Nadal, 1989) are indistinguishable from those of Tiling at the scaleof the Ô¨Ågure. Points without error bars correspond to best results. Results ofMonoPlane and NetLines are averages over 25 tests.
competing terms: bias and variance. With backpropagation, early stopping
helps to decrease overÔ¨Åtting because some hidden neurons do not reachlarge enough weights to work in the nonlinear part of the sigmoidal trans-fer functions. All the neurons working in the linear part may be replaced bya single linear unit. Thus, with early stopping, the network is equivalent toa smaller one with all the units working in the nonlinear regime. Our resultsare consistent with recent theories (Friedman, 1996) showing that, contraryto regression, the bias and variance components of the generalization errorin classiÔ¨Åcation combine in a highly nonlinear way.
The number of weights used by the different algorithms is plotted on a
logarithmic scale as a function of Pin Figure 3. It turns out that the strategy
of NetLines is slightly better than that of MonoPlane with respect to bothgeneralization performance and network size.
5.2 Real Valued Inputs. We tested NetLines on two problems that have
real valued inputs (we include graded-valued inputs here).
5.2.1 Wisconsin Breast Cancer Database. The input patterns of this bench-
mark (Wolberg &amp; Mangasarian, 1990) have ND9 attributes characterizing
/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13 /G19/G13/G13/G31/G48/G57/G2F/G4C/G51/G48/G56
/G38/G53/G56/G57/G44/G55/G57/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G15/G18
/G33/G13 /G14/G13/G13 /G15/G13/G13 /G16/G13/G13 /G17/G13/G13 /G18/G13/G13/G14/G13/G14/G13/G13/G14/G13/G13/G13
/G33/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56/G15 /G52/G55 /G50/G52/G55/G48 /G46/G4F/G58/G50/G53/G56
/G31/G20/G14/G13
/G25/G44/G46/G4E/G53/G55/G52/G53/G44/G4A/G44/G57/G4C/G52/G51/G36/G57/G48/G53/G5A/G4C/G56/G48
/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56
Figure 3: Two or more clumps. Number of weights (logarithmic scale) versus
size of the training set P, for ND10 and ND25. Results of MonoPlane and
NetLines are averages over 25 tests. The references are the same as in Figure 2.
samples of breast cytology, classiÔ¨Åed as benign or malignant. We excluded
from the original database 16 patterns that have the attribute ¬ª6(‚Äúbare nu-
clei‚Äù) missing. Among the remaining DD683 patterns, the two classes are
unevenly represented, 65.5% of the examples being benign. We studied thegeneralization performance of networks trained with sets of several sizes P.
The Ppatterns for each learning test were selected at random. In Figure 4a,
the generalization error at classifying the remaining G¬∑D¬°Ppatterns is
displayed as a function of the corresponding number of weights in a loga-rithmic scale. For comparison, we included in the same Ô¨Ågure results of asingle perceptron trained with PD75 patterns using Minimerror. The re-
sults, averaged values over 50 independent tests for each P, show that both
NetLines and MonoPlane have lower ‚Ä†
gand fewer parameters than other
algorithms on this benchmark.
The total number of weights updates needed by NetLines, including the
weights of the dropped output units, is 7 ¬¢104; backpropagation needed
‚Ä¶104(Prechelt, 1994).
The trained network may be used to classify the patterns with missing
attributes. The number of misclassiÔ¨Åed patterns among the 16 cases forwhich attribute ¬ª
6is missing is plotted as a function of the possible values
of¬ª6on Figure 4b. For large values of ¬ª6, there are discrepancies between the
medical and the network‚Äôs diagnosis on half the cases. This is an exampleof the kind of information that may be obtained in practical applications.
/G14/G13 /G14/G13/G13/G13/G11/G13/G13/G13/G11/G13/G14/G13/G11/G13/G15/G13/G11/G13/G16/G13/G11/G13/G17/G13/G11/G13/G18/G13/G11/G13/G19
/G14/G15
/G17/G16
/G18
/G1A
/G19
/G30/G52/G51/G52/G33/G4F/G44/G51/G48 /G0B/G33/G20/G14/G19/G13/G0C/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G33/G20/G14/G19/G13/G0C/G30/G4C/G51/G4C/G50/G48/G55/G55/G52/G55 /G0B/G33/G20/G1A/G18/G0C/G25/G55/G48/G44/G56/G57 /G46/G44/G51/G46/G48/G55 /G0B/G44/G0C
/G30/G52/G51/G52/G33/G4F/G44/G51/G48/G31/G48/G57/G2F/G4C/G51/G48/G56Œµ/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56/G14 /G15/G16/G17 /G18/G19 /G1A/G1B/G1C /G14 /G13
/G33/G52/G56/G56/G4C/G45/G4F/G48 /G59/G44/G4F/G58/G48/G56 /G52/G49 /G44/G57/G57/G55/G4C/G45/G58/G57/G48 Œæ/G19/G30/G52/G51/G52/G33/G4F/G44/G51/G48
/G31/G48/G57/G2F/G4C/G51/G48/G56/G25/G55/G48/G44/G56/G57 /G46/G44/G51/G46/G48/G55 /G0B/G45/G0C
Figure 4: Breast cancer classiÔ¨Åcation. (a) Generalization error ‚Ä†gversus num-
ber of weights (logarithmic scale), for PD525. 1‚Äì3: Rprop with no shortcuts
(Prechelt, 1994); 4‚Äì6: Rprop with shortcuts (Prechelt, 1994); 7: Cascade Correla-tion (Depenau, 1995). For comparison, results with smaller training sets, PD75
(single perceptron) and PD160, are displayed. Results of MonoPlane and Net-
Lines are averages over 50 tests. (b) ClassiÔ¨Åcation errors versus possible valuesof the missing attribute bare nuclei for the 16 incomplete patterns, averagedover 50 independently trained networks.
5.2.2 Diabetes Diagnosis. This benchmark (Prechelt, 1994) contains DD
768 patterns described by ND8 real-valued attributes, corresponding to
‚Ä¶35% of Pima women suffering from diabetes, 65% being healthy. Training
sets of PD576 patterns were selected at random, and generalization was
tested on the remaining GD192 patterns. The comparison with published
results obtained with other algorithms tested under the same conditions,presented in Figure 5, shows that NetLines reaches the best performancepublished so far on this benchmark, needing many fewer parameters. Train-ing times of NetLines are of ‚Ä¶10
5updates. The numbers of updates needed
by Rprop (Prechelt, 1994) range between 4 ¬¢103and 5¬¢105, depending on
the network‚Äôs architecture.
5.3 Multiclass Problems. We applied our learning algorithm to two dif-
ferent problems, both of three classes. We compare the results obtained witha WTA classiÔ¨Åcation based on the results of three networks, each indepen-dently trained to separate one class from the two others, to the results ofthe TON architectures described in section 4. Because the number of classesis low, we determined the three TONs, corresponding to the three possible
/G14/G13 /G14/G13/G13/G13/G11/G15/G13/G13/G11/G15/G15/G13/G11/G15/G17/G13/G11/G15/G19/G13/G11/G15/G1B/G13/G11/G16/G13
/G14
/G15/G16
/G18/G19
/G17
/G31/G48/G57/G2F/G4C/G51/G48/G56/G2C/G51/G47/G4C/G44/G51/G56 /G33/G4C/G50/G44 /G27/G4C/G44/G45/G48/G57/G48/G56
Œµ/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56
Figure 5: Diabetes diagnosis: Generalization error ‚Ä†gversus number of weights.
Results of NetLines are averages over 50 tests. 1‚Äì3: Rprop no shortcuts, 4‚Äì6:Rprop with shortcuts (Prechelt, 1994).
learning sequences. The vote of the three TONs improves the performances,
as expected.
5.3.1 Breiman‚Äôs Waveform Recognition Problem. This problem was intro-
duced as a test for the algorithm CART (Breiman et al., 1984). The inputpatterns are deÔ¨Åned by ND21 real-valued amplitudes x.t/observed at reg-
ularly spaced intervals tD1;2;:::; N. Each pattern is a noisy convex linear
combination of two among three elementary waves (triangular waves cen-tered on three different values of t). There are three possible combinations,
and the pattern‚Äôs class identiÔ¨Åes from which combination it is issued.
We trained the networks with the same 11 training sets of PD300 ex-
amples, and generalization was tested on the same independent test setofGD5000, as in Gascuel (1995). Our results are displayed in Figure 6,
where only results of algorithms reaching ‚Ä†
g&lt;0:25 in Gascuel (1995) are
included. Although it is known that due to the noise, the classiÔ¨Åcation errorhas a lower bound of ‚Ä¶14% (Breiman et al., 1984), the results of NetLines
and MonoPlane presented here correspond to error-free training. The net-works generated by NetLines have between three and six hidden neurons,depending on the training sets. The results obtained with a single percep-tron trained with Minimerror and with the perceptron learning algorithm,which may be considered the extreme case of early stopping, are hardly im-proved by the more complex networks. Here again the overÔ¨Åtting producedby error-free learning with NetLines does not cause the generalization per-
/G14/G13 /G14/G13/G13 /G14/G13/G13/G13 /G14/G13/G13/G13/G13 /G14/G13/G13/G13/G13/G13/G13/G11/G14/G17/G13/G11/G14/G19/G13/G11/G14/G1B/G13/G11/G15/G13/G13/G11/G15/G15/G13/G11/G15/G17/G13/G11/G15/G19
/G14
/G15 /G16/G17
/G18/G19
/G1A/G1B/G30/G52/G51/G52/G33/G4F/G44/G51/G48 /G0B/G3A/G37/G24/G0C
/G37/G4B/G48/G52/G55/G48/G57/G4C/G46/G44/G4F /G4F/G4C/G50/G4C/G57/G30/G4C/G51/G4C/G50/G48/G55/G55/G52/G55
/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G39/G52/G57/G48/G0C/G25/G55/G48/G4C/G50/G44/G51/G0A/G56 /G3A/G44/G59/G48/G49/G52/G55/G50/G56
Œµ/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G53/G44/G55/G44/G50/G48/G57/G48/G55/G56
Figure 6: Breiman waveforms: Generalization error ‚Ä†gaveraged over 11 tests
versus number of parameters. Error bars on the number of weights generatedby NetLines and MonoPlane are not visible at the scale of the Ô¨Ågure. 1: linear dis-crimination; 2: perceptron; 3: backpropagation; 4: genetic algorithm; 5: quadraticdiscrimination; 6: Parzen‚Äôs kernel; 7: K-NN; 8: constraint (Gascuel, 1995).
formance to deteriorate. The TONs vote reduces the variance but does not
decrease the average ‚Ä†g.
5.3.2 Fisher‚Äôs Iris Plants Database. In this classic three-class problem, one
has to determine the class of iris plants based on the values of ND4 real-
valued attributes. The database of DD150 patterns contains 50 examples
of each class. Networks were trained with PD149 patterns, and the gener-
alization error is the mean value of all the 150 leave-one-out possible tests.Results of‚Ä†
gare displayed as a function of the number of weights in Figure 7.
Error bars are available for only our own results. In this difÔ¨Åcult problem,the vote of the three possible TONs trained with the three possible classsequences (see section 4) improves the generalization performance.</corps>
  <conclusion>We presented an incremental learning algorithm for classiÔ¨Åcation, which we
call NetLines. It generates small feedforward neural networks with a singlehidden layer of binary units connected to a binary output neuron. NetLinesallows for an automatic adaptation of the neural network to the complexityof the particular task. This is achieved by coupling an error-correcting strat-egy for the successive addition of hidden neurons with Minimerror, a very
/G14/G13 /G14/G13/G13/G13/G11/G13/G13/G13/G11/G13/G15/G13/G11/G13/G17/G13/G11/G13/G19/G13/G11/G13/G1B/G13/G11/G14/G13
/G14/G15/G16/G0F/G17/G18
/G19
/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G59/G52/G57/G48/G0C/G31/G48/G57/G2F/G4C/G51/G48/G56 /G0B/G3A/G37/G24/G0C/G2C/G35/G2C/G36 /G47/G44/G57/G44/G45/G44/G56/G48
Œµ/G4A
/G31/G58/G50/G45/G48/G55 /G52/G49 /G5A/G48/G4C/G4A/G4B/G57/G56
Figure 7: Iris database: Generalization error ‚Ä†gversus number of parameters.
1: offset, 2: backpropagation (Martinez &amp; Est√®ve, 1992); 4,5: backpropagation
(Verma &amp; Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma&amp; Mulawka, 1995).
efÔ¨Åcient perceptron training algorithm. Learning is fast not only because
it reduces the problem to that of training single perceptrons, but mainlybecause there is no longer a need for the usual preliminary tests required todetermine the correct architecture for the particular application. Theoremsvalid for binary as well as for real-valued inputs guarantee the existence ofa solution with a bounded number of hidden neurons obeying the growthstrategy.
The networks are composed of binary hidden units whose states consti-
tute a faithful encoding of the input patterns. They implement a mappingfrom the input space to a discrete H-dimensional hidden space, Hbeing
the number of hidden neurons. Thus, each pattern is labeled with a binaryword of Hbits. This encoding may be seen as a compression of the pattern‚Äôs
information. The hidden neurons deÔ¨Åne linear boundaries, or portions ofboundaries, between classes in input space. The network‚Äôs output may begiven a probabilistic interpretation based on the distance of the patterns tothese boundaries.
Tests on several benchmarks showed that the networks generated by our
incremental strategy are small, in spite of the fact that the hidden neuronsare appended until error-free learning is reached. Even when the networksobtained with NetLines are larger than those used by other algorithms, itsgeneralization error remains among the smallest values reported. In noisyor difÔ¨Åcult problems, it may be useful to stop the network‚Äôs growth before
the condition of zero training errors is reached. This decreases overÔ¨Åtting, as
smaller networks (with less parameters) are thus generated. However, theprediction quality (measured by the generalization error) of the classiÔ¨Åersgenerated with NetLines is not improved by early stopping.
Our results were obtained without cross-validation or any data manip-
ulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,&amp; Simard, 1993). Those costly procedures combine results of very largenumbers of classiÔ¨Åers, with the aim of improving the generalization perfor-mance through the reduction of the variance. Because NetLines is a stableclassiÔ¨Åer, presenting small variance, we do not expect that such techniqueswould signiÔ¨Åcantly improve our results.</conclusion>
  <discussion>N/A</discussion>
  <bibliographie>Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un-
published doctoral dissertation, Ecole Polytechnique F√©d√©rale de Lausanne,
Switzerland.
Biehl, M., &amp; Opper, M. (1991). Tilinglike learning in the parity machine. Physical
Review A, 44 , 6888.
Bottou, L., &amp; Vapnik, V . (1992). Local learning algorithms. Neural Computation,
4(6), 888‚Äì900.
Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department
of Statistics, University of California at Berkeley.
Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). ClassiÔ¨Åcation
and regression trees . Monterey, CA: Wadsworth and Brooks/Cole.
Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., &amp; HopÔ¨Åeld, J.
(1987). Large automatic learning, rule extraction, and generalization. Complex
Systems, 1 , 877‚Äì922.
Depenau, J. (1995). Automated design of neural network architecture for classiÔ¨Åcation.
Unpublished doctoral dissertation, Computer Science Department, AarhusUniversity.
Drucker, H., Schapire, R., &amp; Simard, P . (1993). Improving performance in neu-
ral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &amp;C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42‚Äì
49). San Mateo, CA: Morgan Kaufmann.
Fahlman, S. E., &amp; Lebiere, C. (1990). The cascade-correlation learning architec-
ture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,
2(pp. 524‚Äì532). San Mateo: Morgan Kaufmann.
Farrell, K. R., &amp; Mammone, R. J. (1994). Speaker recognition using neural tree
networks. In J. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in Neural
Information Processing Systems, 6 (pp. 1035‚Äì1042). San Mateo, CA: Morgan
Kaufmann.
Frean, M. (1990). The Upstart algorithm: A method for constructing and training
feedforward neural networks. Neural Computation, 2 (2), 198‚Äì209.
Frean, M. (1992). A ‚Äúthermal‚Äù perceptron learning rule. Neural Computation, 4 (6),
946‚Äì957.
Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality
(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.
Fritzke, B. (1994). Supervised learning with growing cell structures. In
J. D. Cowan, G. Tesauro, &amp; J. Alspector (Eds.), Advances in neural informa-
tion processing systems, 6 (pp. 255‚Äì262). San Mateo, CA: Morgan Kaufmann.
Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern
Recognition, Oct. 28‚Äì31, Paris , vol. 4.
Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).
5√®mes Journ√©es Nationales du PRC-IA Teknea, Nancy.
Geman, S., Bienenstock, E., &amp; Doursat, R. (1992). Neural networks and the
bias/variance dilemma. Neural Computation, 4 (1), 1‚Äì58.
Goodman, R. M., Smyth, P ., Higgins, C. M., &amp; Miller, J. W. (1992). Rule-based
neural networks for classiÔ¨Åcation and probability estimation. Neural Compu-
tation, 4 (6), 781‚Äì804.
Gordon, M. B. (1996). A convergence theorem for incremental learning with real-
valued inputs. In IEEE International Conference on Neural Networks , pp. 381‚Äì
386.
Gordon, M. B., &amp; Berchier, D. (1993). Minimerror: A perceptron learning rule
that Ô¨Ånds the optimal weights. In M. Verleysen (Ed.), European Symposium on
ArtiÔ¨Åcial Neural Networks (pp. 105‚Äì110). Brussels: D Facto.
Gordon, M. B., &amp; Grempel, D. (1995). Optimal learning with a temperature
dependent algorithm. Europhysics Letters, 29 (3), 257‚Äì262.
Gordon, M. B., Peretto, P ., &amp; Berchier, D. (1993). Learning algorithms for percep-
trons from statistical physics. Journal of Physics I (France), 3 , 377‚Äì387.
Gorman, R. P ., &amp; Sejnowski, T. J. (1988). Analysis of hidden units in a layered
network trained to classify sonar targets. Neural Networks, 1 , 75‚Äì89.
Gyorgyi, G., &amp; Tishby, N. (1990). Statistical theory of learning a rule. In
W. K. Theumann &amp; R. Koeberle (Eds.), Neural networks and spin glasses . Sin-
gapore: World ScientiÔ¨Åc.
Hoehfeld, M., &amp; Fahlman, S. (1991). Learning with limited numerical precision using
the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:
Carnegie Mellon University.
Knerr, S., Personnaz, L., &amp; Dreyfus, G. (1990). Single-layer learning revisited: A
stepwise procedure for building and training a neural network. In J. H√©rault
&amp; F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications
(pp. 41‚Äì50). Berlin: Springer-Verlag.
Marchand, M., Golea, M., &amp; Ruj√°n, P . (1990). A convergence theorem for sequen-
tial learning in two-layer perceptrons. Europhysics Letters, 11 , 487‚Äì492.
Martinez, D., &amp; Est√®ve, D. (1992). The offset algorithm: Building and learning
method for multilayer neural networks. Europhysics Letters, 18 , 95‚Äì100.
M√©zard, M., &amp; Nadal, J.-P . (1989). Learning in feedforward layered networks:
The Tiling algorithm. J. Phys. A: Math. and Gen., 22 , 2191‚Äì2203.
Mukhopadhyay, S., Roy, A., Kim, L. S., &amp; Govil, S. (1993). A polynomial time al-
gorithm for generating neural networks for pattern classiÔ¨Åcation: Its stabilityproperties and some test results. Neural Computation, 5 (2), 317‚Äì330.
Nadal, J.-P . (1989). Study of a growth algorithm for a feedforward neural net-
work. Int. J. Neur. Syst., 1 , 55‚Äì59.
Prechelt, L. (1994). PROBEN1‚ÄîA set of benchmarks and benchmarking rules for neu-
ral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,
Faculty of Informatics.
RafÔ¨Ån, B., &amp; Gordon, M. B. (1995). Learning and generalization with Minimerror,
a temperature dependent learning algorithm. Neural Computation, 7 (6), 1206‚Äì
1224.
Reilly, D. E, Cooper, L. N., &amp; Elbaum, C. (1982). A neural model for category
learning. Biological Cybernetics, 45 , 35‚Äì41.
Roy, A., Kim, L., &amp; Mukhopadhyay, S. (1993). A polynomial time algorithm
for the construction and training of a class of multilayer perceptron. Neural
Networks, 6 (1), 535‚Äì545.
Sirat, J. A., &amp; Nadal, J.-P . (1990). Neural trees: A new tool for classiÔ¨Åcation.
Network, 1 , 423‚Äì438.
Solla, S. A. (1989). Learning and generalization in layered neural networks: The
contiguity problem. In L. Personnaz &amp; G. Dreyfus (Eds.), Neural Networks
from Models to Applications . Paris: I.D.S.E.T.
Torres Moreno, J.-M., &amp; Gordon, M. B. (1995). An evolutive architecture coupled
with optimal perceptron learning for classiÔ¨Åcation. In M. Verleysen (Ed.),European Symposium on ArtiÔ¨Åcial Neural Networks . Brussels: D Facto.
Torres Moreno, J.-M., &amp; Gordon, M. B. (1998). Characterization of the sonar
signals benchmark. Neural Proc. Letters, 7 (1), 1‚Äì4.
Trhun, S. B., et al. (1991). The monk‚Äôs problems: A performance comparison of different
learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie
Mellon University.
Vapnik, V . (1992). Principles of risk minimization for learning theory. In
J. E. Moody, S. J. Hanson, &amp; R. P . Lippmann (Eds.), Advances in neural informa-
tion processing systems, 4 (pp. 831‚Äì838). San Mateo, CA: Morgan Kaufmann.
Verma, B. K., &amp; Mulawka, J. J. (1995). A new algorithm for feedforward neu-
ral networks. In M. Verleysen (Ed.), European Symposium on ArtiÔ¨Åcial Neural
Networks (pp. 359‚Äì364). Brussels: D Facto.
Wolberg, W. H., &amp; Mangasarian, O. L. (1990). Multisurface method of pattern
separation for medical diagnosis applied to breast cytology. In Proceedings of
the National Academy of Sciences, USA, 87 , 9193‚Äì9196.
Received February 13, 1997; accepted September 4, 1997.This article has been cited by:
1.C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral
analysis. IEEE Transactions on Neural Networks  10, 725-740. [ CrossRef ]
2.Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural
Networks. International Journal of Neural Systems  08, 647-659. [ CrossRef</bibliographie>
</article>