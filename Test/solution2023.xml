<article>
  <preamble>acl2012.pdf</preamble>
  <titre>Finding Salient Dates for Building Thematic Timelines</titre>
  <auteurs>
    <auteur>
      <name>R√©my Kessler</name>
      <mail>kessler@limsi.fr</mail>
      <affiliation>LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>Xavier Tannier</name>
      <mail>xtannier@limsi.fr</mail>
      <affiliation>Univ. Paris-Sud,
LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>Caroline Hag√®ge</name>
      <mail>hagege@xrce.xerox.com</mail>
      <affiliation>Xerox Research Center Europe
Meylan, France</affiliation>
    </auteur>
    <auteur>
      <name>V√©ronique Moriceau</name>
      <mail>moriceau@limsi.fr</mail>
      <affiliation>Univ. Paris-Sud, LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>Andr√© Bittar</name>
      <mail>bittar@xrce.xerox.com</mail>
      <affiliation>Xerox Research Center Europe
Meylan, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>We present an approach for detecting salient
(important) dates in texts in order to auto-
matically build event timelines from a search
query ( e.g. the name of an event or person,
etc.). This work was carried out on a corpus
of newswire texts in English provided by the
Agence France Presse (AFP). In order to ex-
tract salient dates that warrant inclusion in an
event timeline, we Ô¨Årst recognize and normal-
ize temporal expressions in texts and then use
a machine-learning approach to extract salient
dates that relate to a particular topic. We fo-
cused only on extracting the dates and not the
events to which they are related.</abstract>
  <introduction>
Our aim here was to build thematic timelines for
a general domain topic deÔ¨Åned by a user query.
This task, which involves the extraction of important
events, is related to the tasks of Retrospective Event
Detection (Yang et al., 1998), or New Event Detec-
tion, as deÔ¨Åned for example in Topic Detection and
Tracking (TDT) campaigns (Allan, 2002).
The majority of systems designed to tackle this
task make use of textual information in a bag-of-
words manner. They use little temporal informa-
tion, generally only using document metadata, such
as the document creation time (DCT). The few sys-
tems that do make use of temporal information (such
as the now discontinued Google timeline), only ex-
tract absolute, full dates (that feature a day, month
and year). In our corpus, described in Section 3.1,
we found that only 7% of extracted temporal expres-
sions are absolute dates.We distinguish our work from that of previous re-
searchers in that we have focused primarily on ex-
tracted temporal information as opposed to other
textual content. We show that using linguistic tem-
poral processing helps extract important events in
texts. Our system extracts a maximum of temporal
information and uses only this information to detect
salient dates for the construction of event timelines.
Other types of content are used for initial thematic
document retrieval. Output is a list of dates, ranked
from most important to least important with respect
to the given topic. Each date is presented with a set
of relevant sentences.
We can see this work as a new, easily evaluable
task of ‚Äúdate extraction‚Äù, which is an important com-
ponent of timeline summarization.
In what follows, we Ô¨Årst review some of the re-
lated work in Section 2. Section 3 presents the re-
sources used and gives an overview of the system.
The system used for temporal analysis is described
in Section 4, and the strategy used for indexing and
Ô¨Ånding salient dates, as well as the results obtained,
are given in Section 51.
</introduction>
  <body>The ISO-TimeML language (Pustejovsky et al.,
2010) is a speciÔ¨Åcation language for manual anno-
tation of temporal information in texts, but, to the
best of our knowledge, it has not yet actually been
used in information retrieval systems. Neverthe-
1This work has been partially funded by French National
Research Agency (ANR) under project Chronolines (ANR-10-
CORD-010). We would like to thank the French News Agency
(AFP) for providing us with the corpus.less, (Alonso et al., 2007; Alonso, 2008; Kanhabua,
2009) and (Mestl et al., 2009), among others, have
highlighted that the analysis of temporal informa-
tion is often an essential component in text under-
standing and is useful in a wide range of informa-
tion retrieval applications. (Harabagiu and Bejan,
2005; Saquete et al., 2009) highlight the importance
of processing temporal expressions in Question An-
swering systems. For example, in the TREC-10 QA
evaluation campaign, more than 10% of questions
required an element of temporal processing in order
to be correctly processed (Li et al., 2005a). In multi-
document summarization, temporal processing en-
ables a system to detect redundant excerpts from
various texts on the same topic and to present re-
sults in a relevant chronological order (Barzilay and
Elhadad, 2002). Temporal processing is also useful
for aiding medical decision-making. (Kim and Choi,
2011) present work on the extraction of temporal in-
formation in clinical narrative texts. Similarly, (Jung
et al., 2011) present an end-to-end system that pro-
cesses clinical records, detects events and constructs
timelines of patients‚Äô medical histories.
The various editions of the TDT task have given
rise to the development of different systems that de-
tect novelty in news streams (Allan, 2002; Kumaran
and Allen, 2004; Fung et al., 2005). Most of these
systems are based on statistical bag-of-words mod-
els that use similarity measures to determine prox-
imity between documents (Li et al., 2005b; Brants
et al., 2003). (Smith, 2002) used spatio-temporal in-
formation from texts to detect events from a digital
library. His method used place/time collocations and
ranked events according to statistical measures.
Some efforts have been made for automatically
building textual and graphical timelines. For ex-
ample, (Allan et al., 2001) present a system that
uses measures of pertinence and novelty to con-
struct timelines that consist of one sentence per date.
(Chieu and Lee, 2004) propose a similar system that
extracts events relevant to a query from a collection
of documents. Important events are those reported
in a large number of news articles and each event is
constructed according to one single query and rep-
resented by a set of sentences. (Swan and Allen,
2000) present an approach to generating graphical
timelines that involves extracting clusters of noun
phrases and named entities. More recently, (Yan etal., 2011b; Yan et al., 2011a) used a summarization-
based approach to automatically generate timelines,
taking into account the evolutionary characteristics
of news.
3 Resources and System Overview
3.1 AFP Corpus
For this work, we used a corpus of newswire texts
provided by the AFP French news agency. The En-
glish AFP corpus is composed of 1.3 million texts
that span the 2004-2011 period (511 documents/day
in average and 426 millions words). Each document
is an XML Ô¨Åle containing a title, a date of creation
(DCT), set of keywords, and textual content split
into paragraphs.
3.2 AFP Chronologies
AFP ‚Äúchronologies‚Äù (textual event timelines) are a
speciÔ¨Åc type of articles written by AFP journal-
ists in order to contextualize current events. These
chronologies may concern any topic discussed in the
media, and consist in a list of dates (typically be-
tween 10 and 20) associated with a text describing
the related event(s). Figure 1 shows an example of
such a chronology. Further examples are given in
Figure 2. We selected 91 chronologies satisfying the
following constraints:
All dates in the chronologies are between 2004
and 2011 to be sure that the related events
are described in the corpus. For example,
‚ÄúChronology of climax to Vietnam War‚Äù was
excluded because its corresponding dates do
not appear in the content of the articles.
All dates in the chronology are anterior to the
chronology‚Äôs creation date. For example, the
chronology ‚ÄúSpace in 2005: A calendar‚Äù , pub-
lished in January 2005 and listing scheduled
events, was not selected (because almost no
rocket launches Ô¨Ånally happened on the ex-
pected day).
The temporal granularity of the chronology is
the day. For example, ‚ÄúA timeline of how the
London transport attacks unfolded‚Äù , relating
the events hour by hour, is not in our focus.&lt;NewsML Version="1.2" &gt;
&lt;NewsItem xml:lang="en" &gt;
&lt;HeadLine &gt;Key dates in Thai-
land‚Äôs political crisis &lt;/HeadLine &gt;
&lt;DateId &gt;20100513T100519Z &lt;/DateId &gt;
&lt;NameLabel &gt;Thailand-politics &lt;/NameLabel &gt;
&lt;DataContent &gt;
&lt;p&gt;The following is a timeline of events since
the protests began, soon after Thailand‚Äôs Supreme
Court conÔ¨Åscated 1.4 billion dollars of Thaksin‚Äôs
wealth for abuse of power. &lt;/p&gt;
&lt;p&gt;March 14 : Tens of thousands of Red Shirts
demonstrate in the capital calling for Abhisit‚Äôs gov-
ernment to step down, [...] &lt;/p&gt;
&lt;p&gt;March 28 : The government and the Reds en-
ter into talks but hit a stalemate after two days
[...]&lt;/p&gt;
&lt;p&gt;April 3 : Tens of thousands of protesters move
from Bangkok‚Äôs historic district into the city‚Äôs com-
mercial heart [...] &lt;/p&gt;
&lt;p&gt;April 7 : Abhisit declares state of emergency
in capital after Red Shirts storm parliament. &lt;/p&gt;
&lt;p&gt;April 8 : Authorities announce arrest warrants
for protest leaders. &lt;/p&gt;
. . .
&lt;/DataContent &gt;
&lt;/NewsItem &gt;
&lt;/NewsML &gt;
Figure 1: Example of an AFP manual chronology.
For learning and evaluation purposes, all
chronologies were converted to a single XML
format. Each document was manually associated
with a user search query made up of the keywords
required to retrieve the chronology.
3.3 System Overview
Figure 3 shows the general architecture of the sys-
tem. First, pre-processing of the AFP corpus tags
and normalizes temporal expressions in each of the
articles (step ¬¨in the Figure). Next, the corpus is
indexed by the Lucene search engine2(step¬≠).
Given a query, a number of documents are re-
trieved by Lucene ( ¬Æ). These documents can be Ô¨Ål-
tered (¬Ø), and dates are extracted from the remain-
ing documents. These dates are then ranked in order
to show the most important ones to the user ( ¬∞), to-
2http://lucene.apache.org- Chronology of 18 months of trouble in Ivory Coast
- Chechen rebels‚Äô history of hostage-takings
- Iraqi political wrangling since March 7 election
- Athletics: Timeline of men‚Äôs 800m world record
- Major accidents in Chinese mines
- Space in 2005: A calendar
- Developments in Iranian nuclear standoff
- Chronology of climax to Vietnam War
- Timeline of ex-IMF chief‚Äôs sex attack case
- A timeline of how the London transport attacks un-
folded
Figure 2: Examples of AFP chronologies.
Figure 3: System overview.
gether with the sentences that contain them.
4 Temporal and Linguistic Processing
In this section, we describe the linguistic and tempo-
ral information extracted during the pre-processing
phase and how the extraction is carried out. We
rely on the powerful linguistic analyzer XIP (A ¬®ƒ±t-
Mokhtar et al., 2002), that we adapted for our pur-
poses.
4.1 XIP
The linguistic analyzer we use performs a deep syn-
tactic analysis of running text. It takes as input
XML Ô¨Åles and analyzes the textual content enclosed
in the various XML tags in different ways that are
speciÔ¨Åed in an XML guide (a Ô¨Åle providing instruc-
tions to the parser, see (Roux, 2004) for details).
XIP performs complete linguistic processing rang-
ing from tokenization to deep grammatical depen-
dency analysis. It also performs named entity recog-nition (NER) of the most usual named entity cat-
egories and recognizes temporal expressions. Lin-
guistic units manipulated by the parser are either
terminal categories or chunks. Each of these units
is associated with an attribute-value matrix that con-
tains the unit‚Äôs relevant morphological, syntactic and
semantic information. Linguistic constituents are
linked by oriented and labelled n-ary relations de-
noting syntactic or semantic properties of the input
text. A Java API is provided with the parser so that
all linguistic structures and relations can be easily
manipulated by Java code.
In the following subsections, we give details of
the linguistic information that is used for the detec-
tion of salient dates.
4.2 Named Entity Recognition
Named Entity (NE) Recognition is one of the out-
puts provided by XIP. NEs are represented as unary
relations in the parser output. We used the exist-
ing NE recognition module of the English grammar
which tags the following NE types: location names ,
person names andorganization names . Ambigu-
ous NE types (ambiguity between type location or
organization for country names for instance) are
also considered.
4.3 Temporal Analysis
A previous module for temporal analysis was de-
veloped and integrated into the English grammar
(Hag√®ge and Tannier, 2008), and evaluated during
TempEval campaign (Verhagen et al., 2007). This
module was adapted for tagging salient dates. Our
goal with temporal analysis is to be able to tag and
normalize3a selected subset of temporal expressions
(TEs) which we consider to be relevant for our task.
This subset of expressions is described in the follow-
ing sections.
4.3.1 Absolute Dates
Absolute dates are dates that can be normalized
without external or contextual knowledge. This is
the case, for instance, of ‚ÄúOn January 5th 2003‚Äù .
In these expressions, all information needed for nor-
malization is contained in the linguistic expression.
3We call normalization the operation of turning a temporal
expression into a formated, fully speciÔ¨Åed representation. This
includes Ô¨Ånding the absolute value of relative dates.However, absolute dates are relatively infrequent in
our corpus (7%), so in order to broaden the cover-
age for the detection of salient dates, we decided to
consider relative dates, which are far more frequent.
4.3.2 DCT-relative Dates
DCT-relative temporal expressions are those
which are relative to the creation date of the docu-
ment. This class represents 40% of dates extracted
from the AFP corpus. Unlike the absolute dates, the
linguistic expression does not provide all the infor-
mation needed for normalization. External informa-
tion is required, in particular, the date which corre-
sponds to the moment of utterance. In news articles,
this is the DCT. Two sub-classes of relative TEs can
be distinguished. The Ô¨Årst sub-class only requires
knowledge of the DCT value to perform the normal-
ization. This is the case of expressions like next Fri-
day, which correspond to the calendar date of the
Ô¨Årst Friday following the DCT. The second sub-class
requires further contextual knowledge for normal-
ization. For example, on Friday will correspond ei-
ther to last Friday or to next Friday depending on
the context where this expression appears ( e.g. He
is expected to come on Friday corresponds to next
Friday while He arrived on Friday corresponds to
last Friday ). In such cases, the tense of the verb
that governs the TE is essential for normalization.
This information is provided by the linguistic analy-
sis carried out by XIP.
4.3.3 UnderspeciÔ¨Åed Dates
Considering the kind of corpus we deal with
(news), we decided to consider TEs whose granu-
larity is at least equal to a day. As a result, TEs
were normalized to a numerical YYYYMMDD for-
mat (where YYYY corresponds to the year, MM to
the month and DD to the day). In case of TEs with
a granularity superior to the day or month, DD and
MM Ô¨Åelds remain unspeciÔ¨Åed accordingly. How-
ever, these underspeciÔ¨Åed dates are not used in our
experiments.
4.4 Modality and Reported Speech
An important issue that can affect the calculation of
salient dates is the modality associated with time-
stamped events in text. For instance, the status of a
salient date candidate in a sentence like ‚ÄúThe meet-ing takes place on Friday‚Äù has to be distinguished
from the one in ‚ÄúThe meeting should take place on
Friday‚Äù or‚ÄúThe meeting will take place on Friday,
Mr. Hong said‚Äù . The time-stamped event meeting
takes place is factual in the Ô¨Årst example and can
be taken as granted. In the second and third exam-
ples, however, the event does not necessarily occur.
This is expressed by the modality introduced by the
modal auxiliary should (second example), or by the
use of the future tense or reported speech (third ex-
ample). We annotate TEs with information regard-
ing the factuality of the event they modify. More
speciÔ¨Åcally, we consider the following features:
Events that are mentioned in the future: If a
time-stamped event is in the future tense, we add a
speciÔ¨Åc attribute MODALITY with value FUTURE to
the corresponding TE annotation.
Events used with a modal verb: If a time-
stamped event is introduced by a modal verb such
asshould orwould , then attribute MODALITY to the
corresponding TE annotation has the value MODAL .
Reported speech verbs: Reported speech verbs
(or verbs of speaking) introduce indirect or reported
speech. We dealt with time-stamped events gov-
erned by a reported speech verb, or otherwise ap-
pearing in reported speech. Once again, XIP‚Äôs lin-
guistic analysis provided the necessary information,
including the marking of reported speech verbs and
clause segmentation of complex sentences. If a rel-
evant TE modiÔ¨Åes a reported speech verb, the anno-
tation of this TE contains a speciÔ¨Åc attribute, DE-
CLARATION =‚ÄùYES‚Äù. If the relevant TE modiÔ¨Åes
a verb that appears in a clause introduced by a re-
ported speech verb then the annotation contains the
attribute REPORTED =‚ÄùYES‚Äù.
Note that the different annotations can be com-
bined ( e.g.modality and reported speech can occur
for a same time-stamped event). For example, the
TEFriday in‚ÄúThe meeting should take place on Fri-
day, Mr. Hong said‚Äù is annotated with both modality
and reported speech attributes.
4.5 Corpus-dependent Special Cases
While we developed the linguistic and temporal an-
notators, we took into account some speciÔ¨Åcities of
our corpus. We decided that the TEs today and&lt;DCTvalue="20050105" /&gt;
&lt;ECTYPE="TIMEX" value="unknown" &gt;The year
2004&lt;/EC&gt;was the deadliest &lt;ECTYPE="TIMEX"
value="unknown" &gt;in a decade &lt;/EC&gt;for journalists
around the world, mainly because of the number of reporters
killed in &lt;ECTYPE="LOCORG" &gt;Iraq&lt;/EC&gt;, the
media rights group &lt;ENTYPE="ORG" &gt;Reporters
Sans Frontieres &lt;/EN&gt; (Reporters Without Bor-
ders) said &lt;ECTYPE="DATE" SUBTYPE="REL"
REF="ST" DECLARATION="YES" value
="20050105" &gt;Wednesday &lt;/EC&gt;.
Figure 4: Example of XIP output for a sample article.
now were not relevant for the detection of salient
dates. In the AFP news corpus, these expressions
are mostly generic expressions synomymous with
nowadays and do not really time-stamp an event
with respect to the DCT. Another speciÔ¨Åcity of the
corpus is the fact that if the DCT corresponds to a
Monday, and if an event in a past tense is described
with the associated TE on Monday orMonday , it
means that this event occurs on the DCT day itself,
and not on the Monday before. We adapted the TE
normalizer to these special cases.
4.6 Implementation and Example
As said previously, a NER module is integrated into
the XIP parser, which we used ‚Äúas is‚Äù. The TE tag-
ger and normalizer was adapted from (Hag√®ge and
Tannier, 2008). We used the Java API provided with
the parser to perform the annotation and normal-
ization of TEs. The output for the linguistic and
temporal annotation consists in XML Ô¨Åles where
only selected information is kept (structural infor-
mation distinguishing headlines from news content,
DCT), and enriched with the linguistic annotations
described before (NEs and TEs with relevant at-
tributes corresponding to the normalization and typ-
ing). Information concerning modality, future tense
and reported speech, appears as attributes on the TE
tag. Figure 4 shows an example of an analyzed ex-
cerpt of a news article.
In this news excerpt, only one TE ( Wednesday ) is
normalized as both The year 2004 andin a decade
are not considered to be relevant. The Ô¨Årst one being
a generic TE and the second one being of granular-
ity superior to a year. The annotation of the relevant
TE has the attribute indicating that it time-stamps an
event realized by a reported speech verb. The nor-malized value of the TE corresponds to the 5th of
January 2005, which is a Wednesday. NEs are also
annotated.
In the entire AFP corpus, 11.5 millions temporal
expressions were detected, among which 845,000
absolute dates (7%) and 4.6 millions normalized
relative dates (40%). Although we have not yet
evaluated our tagging of relative dates, the system
on which our current date normalization is based
achieved good results in the TempEval (Verhagen et
al., 2007) campaign.
5 Experiments and Results
In Section 5.1, we propose two baseline approaches
in order to give a good idea of the difÔ¨Åculty of the
task (Section 5.4 also discusses this point). In Sec-
tion 5.2, we present our experiments using simple
Ô¨Åltering and statistics on dates calculated by Lucene.
Finally, Section 5.3 gives details of our experiments
with a learning approach. In our experiments, we
used three different values to rank dates:
occ(d)is the number of textual units (docu-
ments or sentences) containing the date d.
Lucene provides ranked documents together
with their relevance score. luc(d)is the sum of
Lucene scores for textual units containing the
dated.
An adaptation of classical tf.idf for dates:
tf:id f (d) =f(d):logN
d f(d)
where f(d)is the number of occurrences of
datedin the sentence (generally, f(d) = 1 ),N
is the number of indexed sentences and d f(d)
is the number of sentences containing date d.
In all experiments (including baselines), timelines
have been built by considering only dates between
the Ô¨Årst and the last dates of the corresponding man-
ual chronology. Processing runs were evaluated on
manually-written chronologies (see Section 3.2) ac-
cording to Mean Average Precision (MAP), which
is a widely accepted metric for ranked lists. MAP
gives a higher weight to higher ranked elements than
lower ranked elements. SigniÔ¨Åcance of evaluation
results are indicated by the p-value results of Stu-
dent‚Äôs t-test ( t(90) = 1 :9867 ).Baselines ‚Äúonly DCTs‚Äù
Model BLocc
DCT BLluc
DCT BLtf:id f
DCT
MAP Score 0.5036 0.5521 0.5523
Baselines ‚Äúonly absolute dates‚Äù
Model BLocc
abs BLluc
abs BLtf:id f
abs
MAP Score 0.2627 0.2782 0.2778
Baselines ‚Äúabsolute dates or alternatively DCTs‚Äù
Model BLocc
mix BLluc
mix BLtf:id f
mix
MAP Score 0.4005 0.4110 0.4135
Table 1: MAP results for baseline runs.
5.1 Baseline Runs
BLDCT.Indexing and search were done at docu-
ment level (i.e.each AFP article, with its title
and keywords, is a document). Given a query,
the top 10,000 documents were retrieved. In
these runs, only the DCT for each document
was considered. Dates were ranked by one of
the three values described above ( occ,lucor
tf:id f ) leading to runs BLocc
DCT,BLluc
DCT and
BLtfid f
DCT.
BLabs.Indexing and search were done at sentence
level (document title and keywords are added
to sentence text). Given a query, the top 10,000
sentences were retrieved. Only absolute dates
in these sentences were considered. We thus
obtained runs BLocc
abs,BLluc
absandBLtfid f
abs.
Note that in this baseline, as well as in all the
subsequent runs, the information unit was the
sentence because a date was associated to a
small part of the text. The rest of the document
generally contained text that was not related to
the speciÔ¨Åc date.
BLmix.Same as BLabs, except that sentences con-
taining no absolute dates were considered and
associated to the DCT.
Table 1 shows results for these baseline runs.
Using only DCTs with Lucene scores or tf.idf(d)
already yielded interesting results, with MAP
around 0.55.
5.2 Salient Date Extraction with XIP Results
and Simple Filtering
In these experiments, we considered a Lucene index
to be built as follows: each document was taken toModel MAP Score Model MAP Score
Salient date runs with all dates
SDluc0.6962 SDtf:id f0.6982
Salient dates runs with Ô¨Åltering
SDluc
R 0.6975 SDtf:id f
R 0.6996
SDluc
F 0.6967 SDtf:id f
F 0.6993
SDluc
M 0.6978 SDtf:id f
M 0.7005
SDluc
D 0.7066SDtf:id f
D 0.7091
SDluc
FMD 0.7086SDtf:id f
FMD 0.7112
SDluc
RFMD 0.7127SDtf:id f
RFMD 0.7146
Table 2: MAP results for salient date extraction with XIP
and simple Ô¨Åltering. SigniÔ¨Åcance of Ô¨Åltering improve-
ment is indicated by Student t-test against no Ô¨Åltering (:
p &lt; 0:05(signiÔ¨Åcant);:p &lt; 0:01(highly signiÔ¨Åcant)).
Improvement of using tf:id f (d)rather than occ(d)is also
highly signiÔ¨Åcant.
be a sentence containing a normalized date . This
sentence was indexed with the title and keywords of
the AFP article containing it. Given a query, the top
10,000 documents were retrieved. Combinations be-
tween the following Ô¨Åltering operations were pos-
sible, by removing all dates associated with a re-
ported speech verb ( R), a modal verb ( M) and/or
a future verb ( F). All these Ô¨Åltering operations were
intended to remove references to events that were
not certain, thereby minimizing noise in results.
These processing runs are named SDruns, with
indices representing the Ô¨Åltering operations. For ex-
ample, a run obtained by Ô¨Åltering modal and future
verbs is called SDM;F. In all combinations, dates
were ranked by the sum of Lucene scores for these
sentences ( luc) or by tf:id f4.
Table 2 presents the results for this series of exper-
iments. MAP values are much higher than for base-
lines. Using tf:id f (d)is only very slightly better
thanluc. Filtering operations bring some improve-
ment but the beneÔ¨Åts of these different techniques
have to be further investigated.
5.3 Machine-Learning Runs
We used our set of manually-written chronologies
as a training corpus to perform machine learning
experiments. We used IcsiBoost5, an implementa-
4We do not present runs where dates are ranked by the num-
ber of times they appear in retrieved sentences ( occ), as we did
for baselines, since results are systematically lower.
5http://code.google.com/p/icsiboost/tion of adaptative boosting (AdaBoost (Freund and
Schapire, 1997)).
In our approach, we consider two classes: salient
dates are dates that have an entry in the manual
chronologies, while non-salient dates are all other
dates. This choice does, however, represent an im-
portant bias. The choices of journalists are indeed
very subjective, and chronologies must not exceed a
certain length, which means that relevant dates can
be thrown away. These issues will be discussed in
Section 5.4.
The classiÔ¨Åer instances were not all sentences re-
trieved by the search engine. Using all sentences
would not yield a useful feature set. We rather ag-
gregated all sentences corresponding to the same
date before learning the classiÔ¨Åer. Therefore, each
instance corresponded to a single date, and features
were Ô¨Ågures concerning the set of sentences contain-
ing this date.
Features used in this series of runs are as follows:
1. Features representing the fact that the more
a date is mentioned, the more important it is
likely to be: 1) Sum of the Lucene scores for
all sentences containing the date 2) Number of
sentences containing the date 3) Ratio between
the total weights of the date and weights of all
returned dates 4) Ratio between the frequency
of the date and frequency of all returned dates;
2. Features representing the fact that an important
event is still written about, a long time after it
occurs: 1) Distance between the date and the
most recent mention of this date 2) Distance be-
tween the date and the DCT;
3. Other features: 1) Lucene‚Äôs best ranking of the
date 2) Number of times where the date is ab-
solute in the text 3) Number of times where
the date is relative (but normalized) in the text
4) Total number of keywords of the query in the
title, sentence and named entities of retrieved
documents 5) Number of times where the date
modiÔ¨Åes a reported speech verb or is extracted
from reported speech.
We did not aim to classify dates, but rather to rank
them. Instead, we used the predicted probability
P(d)returned by the classiÔ¨Åer, and mixed it with
the Lucene score of sentences, or with date tf.idf :Model MAP Score
Machine-Learning Runs
MLluc
base 0.7033
MLluc0.7905
MLtf:id f0.7918
Table 3: MAP results for salient date extraction with
machine-learning. MLluc
baseused Lucene scores and only
the Ô¨Årst set of features described above. MLlucand
MLtf:id fused the three sets of features. They are both
highly signiÔ¨Åcant under t-test (p6:10 4).
score (d) =P(d)val(d)
where val(d)is either luc(d)ortf:id f (d).
Because the task is very subjective and (above
all) because of the low quantity of learning data, we
prefered not to opt for a ‚Äúlearning to rank‚Äù approach.
We evaluated this approach with a classic 4-fold
cross-validation. Our 91 chronologies were ran-
domly divided into 4 sub-samples, each of them be-
ing used once as test data. The Ô¨Ånal scores, pre-
sented in Table 3, are the average of these 4 pro-
cesses. As shown in this table, the learning approach
improves MAP results by about 0.05 point.</body>
  <conclusion>This article presents a task of ‚Äúdate extraction‚Äù and
shows the importance of taking temporal informa-
tion into consideration and how with relatively sim-
ple temporal processing, we were able to indirectly
point to important events using the temporal infor-
mation associated with these events. Of course, as
our Ô¨Ånal goal consists in the detection of important
events, we need to take into account the textual con-
tent. In future work, we envisage providing, together
with the detection of salient dates, a semantic analy-
sis that will help determine the importance of events.
Another interesting direction in which we soon aim
to work is to consider all textual excerpts that are as-
sociated with salient dates, and use clustering tech-
niques to determine if textual excerpts correspond to
the same event or not. Finally, as our news corpus
is available both for English and French (compara-
ble corpus, not necessarily translations), we aim to
investigate cross-lingual extraction of salient dates and salient events.</conclusion>
  <discussion>Chronologies hand-written by journalists are a very
useful resources for evaluation of our system, as they
are completely dissociated from our research and are
an exact representation of the output we aim to ob-
tain. However, assembling such a chronology is a
very subjective task, and no clear method for evalu-
ation agreement between two journalists seems im-
mediately apparent. Only experts can build such
chronologies, and calculating this agreement would
require at least two experts from each domain, which
are hard to come by. One may then consider our sys-
tem as a useful tool for building a chronology more
objectively.
To illustrate this point, we chose four speciÔ¨Åc top-
ics6and showed one of our runs on each topic to an
AFP expert for these subjects. We asked him to as-
sess the Ô¨Årst 30 dates of these runs.
6Namely, ‚ÄúArab revolt timeline for Morocco‚Äù ,‚ÄúKyrgyzs-
tan unrest timeline‚Äù ,‚ÄúLebanon‚Äôs new government: a timeline‚Äù ,
‚ÄúLibya timeline‚Äù .Topic APC APE
Morocco 0.5847 0.5718
Kyrgyzstan 0.6125 0.9989
Libya 0.7856 1
Lebanon 0.4673 0.7652
Table 4: Average precision results for manual evaluation
on 4 topics, against the original chronologies ( APC), and
the expert assessment ( APE).
Table 4 presents results for this evaluation, com-
paring average precision values obtained 1) against
the original, manual chronologies ( APC), and 2)
against the expert assessment ( APE). These values
show that, for 3 runs out of 4, many dates returned
by the system are considered as valid by the expert,
even if not presented in the original chronology.
Even if this experiment is not strong enough to
lead to a formal conclusion ( post-hoc evaluation
with only 4 topics and a single assessor), this tends
to show that our system produces usable outputs and
that our system can be of help to journalists by pro-
viding them with chronologies that are as useful and
objective as possible.</discussion>
  <biblio>Salah A ¬®ƒ±t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond Shallowness: Incre-
mental Deep Parsing. Natural Language Engineering ,
8:121‚Äì144.
James Allan, Rahul Gupta, and Vikas Khandelwal. 2001.
Temporal summaries of new topics. In Proceedings of
the 24th annual international ACM SIGIR conference
on Research and development in information retrieval ,
SIGIR ‚Äô01, pages 10‚Äì18.
James Allan, editor. 2002. Topic Detection and Tracking .
Springer.
Omar Alonso, Ricardo Baeza-Yates, and Michael Gertz.
2007. Exploratory Search Using Timelines. In
SIGCHI 2007 Workshop on Exploratory Search and
HCI Workshop .
Omar Rogelio Alonso. 2008. Temporal information re-
trieval . Ph.D. thesis, University of California at Davis,
Davis, CA, USA. Adviser-Gertz, Michael.
Regina Barzilay and Noemie Elhadad. 2002. Infer-
ring Strategies for Sentence Ordering in Multidocu-
ment News Summarization. Journal of ArtiÔ¨Åcial In-
telligence Research , 17:35‚Äì55.
Thorsten Brants, Francine Chen, and Ayman Farahat.
2003. A system for new event detection. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval , SIGIR ‚Äô03, pages 330‚Äì337, New York, NY ,
USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval , SIGIR ‚Äô04, pages 425‚Äì432.
Yoav Freund and Robert E. Schapire. 1997. A Decision-
Theoretic Generalization of On-Line Learning and an
Application to Boosting. Journal of Computer and
System Sciences , 55(1):119‚Äì139.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In VLDB ‚Äô05: Proceedings
of the 31st international conference on Very large data
bases , pages 181‚Äì192.
Caroline Hag√®ge and Xavier Tannier. 2008. XTM: A Ro-
bust Temporal Text Processor. In Computational Lin-
guistics and Intelligent Text Processing, proceedings
of 9th International Conference CICLing 2008 , pages
231‚Äì240, Haifa, Israel, February. Springer Berlin /
Heidelberg.
Sanda Harabagiu and Cosmin Adrian Bejan. 2005.
Question Answering Based on Temporal Inference. In
Proceedings of the Workshop on Inference for TextualQuestion Answering , Pittsburg, Pennsylvania, USA,
July.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop ,
BioNLP ‚Äô11, pages 146‚Äì154, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Nattiya Kanhabua. 2009. Exploiting temporal infor-
mation in retrieval of archived documents. In Pro-
ceedings of the 32nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR 2009, Boston, MA, USA, July 19-
23, 2009 , page 848.
Youngho Kim and Jinwook Choi. 2011. Recogniz-
ing temporal information in korean clinical narra-
tives through text normalization. Healthc Inform Res ,
17(3):150‚Äì5.
Giridhar Kumaran and James Allen. 2004. Text clas-
siÔ¨Åcation and named entities for new event detection.
InSIGIR ‚Äô04: Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval , pages 297‚Äì304.
ACM.
Wei Li, Wenjie Li, Qin Lu, and Kam-Fai Wong. 2005a.
A Preliminary Work on Classifying Time Granulari-
ties of Temporal Questions. In Proceedings of Second
international joint conference in NLP (IJCNLP 2005) ,
Jeju Island, Korea, oct.
Zhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma.
2005b. A Probabilistic Model for Restrospective
News Event Detection. In Proceedings of the 28th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval , Sal-
vador, Brazil. ACM Press, New York City, NY , USA.
Thomas Mestl, Olga Cerrato, Jon √òlnes, Per Myrseth,
and Inger-Mette Gustavsen. 2009. Time Challenges -
Challenging Times for Future Information Search. D-
Lib Magazine , 15(5/6).
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. Iso-timeml: An international
standard for semantic annotation. In Nicoletta Calzo-
lari (Conference Chair), Khalid Choukri, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, Stelios Piperidis,
Mike Rosner, and Daniel Tapias, editors, Proceed-
ings of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC‚Äô10) , Valletta,
Malta, may. European Language Resources Associa-
tion (ELRA).
Claude Roux. 2004. Annoter les documents XML avec
un outil d‚Äôanalyse syntaxique. In 11√®me Confrence
annuelle de Traitement Automatique des Langues Na-
turelles , Fs, Maroc, April. ATALA.Estela Saquete, Jose L. Vicedo, Patricio Mart ¬¥ƒ±nez-Barco,
Rafael Mu Àúnoz, and Hector Llorens. 2009. Enhancing
QA Systems with Complex Temporal Question Pro-
cessing Capabilities. Journal of ArticiÔ¨Åal Intelligence
Research , 35:775‚Äì811.
David A. Smith. 2002. Detecting events with date and
place information in unstructured text. In JCDL ‚Äô02:
Proceedings of the 2nd ACM/IEEE-CS joint confer-
ence on Digital libraries , pages 191‚Äì196, New York,
NY , USA. ACM.
Russell Swan and James Allen. 2000. Automatic genera-
tion of overview timelines. In Proceedings of the 23rd
annual international ACM SIGIR conference on Re-
search and development in information retrieval , SI-
GIR ‚Äô00, pages 49‚Äì56, New York, NY , USA. ACM.
Marc Verhagen, Robert Gaizauskas, Franck Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 - 15: TempEval Temporal Rela-
tion IdentiÔ¨Åcation. In Proceedings of SemEval work-
shop at ACL 2007 , Prague, Czech Republic, June. As-
sociation for Computational Linguistics, Morristown,
NJ, USA.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xi-
aoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, 27-31 July 2011, Edinburgh, UK , pages
433‚Äì443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolution-
ary timeline summarization: a balanced optimization
framework via iterative substitution. In Proceeding
of the 34th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
SIGIR 2011, Beijing, China, July 25-29, 2011 , pages
745‚Äì754.
Y . Yang, T. Pierce, and J. G. Carbonell. 1998. A study on
retrospective and on-line event detection. In Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , Melbourne, Australia, August. ACM Press,
New York City, NY , USA.</biblio>


  <preamble>b0e5c43edf116ce2909ae009cc27a1546f09.pdf</preamble>
  <titre>Inclusive yet Selective: Supervised Distributional Hypernymy Detection</titre>
  <auteurs>
    <auteur>
      <name>Stephen Roller</name>
      <mail>roller@cs.utexas.edu</mail>
      <affiliation>Department of Computer Science
          The University of Texas at Austin</affiliation>
    </auteur>
    <auteur>
      <name>Katrin Erk</name>
      <mail>katrin.erk@mail.utexas.edu</mail>
      <affiliation>Department of Linguistics
The University of Texas at Austin</affiliation>
    </auteur>
    <auteur>
      <name>Gemma Boleda</name>
      <mail>gemma.boleda@upf.edu</mail>
      <affiliation>Department of Linguistics
The University of Texas at Austin</affiliation>
    </auteur>
  </auteurs>
  <abstract>We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in
a superset of contexts in which their hyponyms are found. We Ô¨Ånd that this hypothesis only
holds when it is applied to relevant dimensions. We propose a robust supervised approach that
achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting
the dimensions that are relevant for distributional inclusion.</abstract>
  <introduction>
One of the main criticisms of distributional models has been that they fail to distinguish between semantic
relations: Typical nearest neighbors of dogare words like cat,animal ,puppy ,tail, orowner , all obviously
related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002)
argues that distributional models cannot be a valid model of conceptual representation. Distinguishing
semantic relations are also crucial for drawing inferences from distributional data, as different semantic
relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as
Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004).
For these reasons, research has in recent years started to attempt the detection of speciÔ¨Åc semantic
relationships, and current results suggest that distributional models can, in fact, distinguish between
semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci
and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE
and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation
between a superordinate term in a taxonomy (e.g. animal ) and a subordinate term (e.g. dog).
Distributional approaches to date for detecting hypernymy, and the related but broader relation of
lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based
on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet
and Dagan, 2009), which states that more speciÔ¨Åc terms appear in a subset of the distributional contexts
in which more general terms appear. So, animal can occur in all the contexts in which dogcan occur,
plus some contexts in which dogcannot ‚Äì for instance, rights can be a typical cooccurrence for animal
(e.g. ‚Äúanimal rights‚Äù), but not so much for dog(e.g. #‚Äúdog rights‚Äù).
This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We
show that the current best unsupervised approach is brittle in that their performance depends on the space
they are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct,
and if so, under what circumstances it holds. We use a simple supervised approach to relation detection
that has good performance (accuracy .84 on B LESS , .85 on the lexical entailment dataset of Baroni et
al. (2012)) and works well across different spaces.1Furthermore, we show that it can be interpreted
as selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer is
to propose the Selective Distributional Inclusion Hypothesis : The Distributional Inclusion Hypothesis
holds, but only for relevant dimensions.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1Code and data are available at http://stephenroller.com/research/coling14 .</introduction>
  <body>Distributional models. Distributional models represent a word through the contexts in which it has
been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word
is represented as a vector in a high-dimensional space in which the dimensions are context items (for
example, other words) and the coordinates of the vector indicate the target‚Äôs degree of association with
each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not
stand for individual context items anymore.
Pattern-based approaches to inducing semantic relations. Early work on automatically inducing se-
mantic relations between words, starting with Hearst (1992), uses textual patterns. For example, ‚Äú[NP 1]
and other [NP 2]‚Äù implies that NP 2is a hypernym of NP 1. Pattern-based approaches have been applied
to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al.,
2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations be-
tween verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide
variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations.
A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al.,
2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors.
Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004)
introduce the notion of distributional generality , wherevis distributionally more general than uifu
appears in a subset of the contexts in which vis found, and speculate that hypernyms ( v) should be more
distributionally general than hyponyms ( u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the
term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy
or the more loosely deÔ¨Åned relation of lexical entailment .
Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and
in all equations below, uis the narrower term, and vthe more general one. Abusing notation, we write u
for both a word and its associated vector /angbracketleftu1,...,u n/angbracketright. Kotlerman et al. (2010) predict lexical entailment
with the balAPinc measure, a modiÔ¨Åcation of the Average Precision (AP) measure (eq. 2). The general
notion is that scores should increase with the number of dimensions of vthatushares, and also give more
weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is captured
inAPinc by computing precision P(r)at every rank ramongu‚Äôs dimensions ‚Äì where precision is the
fraction of dimensions shared with v‚Äì, and weighting by the rank of the same dimension in the broader
term,rel/prime(v,r,u ). The Ô¨Ånal measure, balAPinc , smooths using the LINsimilarity measure (Lin, 1998).
(We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).)
1(x) =/braceleftBigg
1ifx&gt; 0;
0otherwise
WeedsPrec (u,v) =/summationtextn
i=1ui¬∑1(vi)/summationtextn
i=1ui(1)
APinc (u,v) =/summationtext|1(u)|
r=1P(r)¬∑rel/prime(v,r,u ))
|1(u)|(2)
balAPinc (u,v) =/radicalbig
APinc (u,v)¬∑LIN(u,v)
TheClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the nar-
rower termuhas lower values than vacross all dimensions (eq. 3). Lenci and Benotto (2012) introduce
theinvCL measure, which uses ClarkeDE to measure both distributional inclusion of uinvand distri-
butional non-inclusion ofvinu(eq. 4). While all other measures interpret the Distributional Inclusion
Hypothesis as the degree to which a ‚äÜrelation holds, Lenci and Benotto test the degree to which proper
inclusion /subsetnoteqlholds. They consider not only the degree to which the contexts of the narrower terms are
included in the contexts of the wider term, but also determine the degree to which the wider term has
contexts that the narrower term does not have. 1027 CL(u,v) =/summationtextn
i=1min(ui,vi)/summationtextn
i=1ui(3)
invCL (u,v) =/radicalbig
CL(u,v)¬∑(1‚àíCL(v,u)) (4)
Like Lenci and Benotto, we focus on the stricter hypernymy relation, rather than lexical entailment.
We believe that the different relations that make up lexical entailment have different distributional indi-
cations and that, for that reason, it will be easier to detect the relations separately than together.
Baroni et al. (2012) proposes a supervised approach to hypernymy detection that represents two words
as the concatenation of their vectors. They also mention in passing another supervised approach that
represents two words as the component-wise difference of their vectors. These are broadly the two
approaches that we test, though we introduce signiÔ¨Åcant modiÔ¨Åcations.
3 Data
3.1 Distributional Vector Spaces
We use three standard types of distributional spaces.
U+W2: This space is based on a concatenation of the Gigaword, BNC, English Wackypedia and
ukWaC corpora (Baroni et al., 2009). The corpora are POS-tagged and lemmatized. We keep only
content words (nouns, proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The
resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by
counting co-occurrences of these word types within a window of two words on the left and the right,
using the top 20k most frequent content words as dimensions. The space was transformed using Positive
Pointwise Mutual Information (PPMI).
U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts
instead of 2-word windows.
TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM con-
tains a list of weighted tuples, /angbracketleft/angbracketleftw1,l,w 2/angbracketright,œÉ/angbracketright, wherew1andw2are content words, lis a corpus-derived
syntagmatic relationship between the words, and œÉis a weight estimating saliency of the relationship. We
construct vectors for every unique w1using the set of/angbracketleftl,w 2/angbracketrightpairs as dimensions and corresponding œÉ
values as dimension weights. We select TypeDM for its excellent performance in previous comparisons
of distributional hypernymy measures (Lenci and Benotto, 2012).
Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three
spaces to 300 dimensions using Singular Value Decomposition. We use a subscript to denote reduced
spaces, e.g. U+W2 300. When necessary, we use the term original dimensions to refer to the vector
dimensions from the original, non-reduced spaces (e.g. U+W2); the term latent dimensions refers to the
dimensions in the reduced spaces (e.g. U+W2 300).
3.2 Evaluation Data Sets
BLESS :The B LESS data set (Baroni and Lenci, 2011) covers 200 concepts , or concrete and unambigu-
ous terms (divided into 17 different general concept classes , including vehicle andground mammal ), and
their relationships to other nouns, called relata . Example concepts include vanandhorse . Each concept
is related to several relata through different semantic relations . Following Lenci and Benotto (2012), we
focus on the four semantic relations where both concepts and relata are nouns, for a total 14K data points:
Hypernymy, denoting a superset relationship (e.g. animal -dog); Co-hyponymy, denoting words that share
a common hypernym (e.g. dog-cat); Meronymy, denoting a part-whole relationship (e.g. tail-dog); and
Random, denoting no relationship between the words (e.g. dog-computer ). 1028 ‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzU+W2, invCL
‚óè
‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè ‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzU+Sent, invCL
‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzTypeDM, invCLFigure 1: Distributions of relata invCL scores for the U+W2, U+Sent, and TypeDM spaces for each of
the semantic relations, after per-concept z-normalization.
ENTAILMENT :(Baroni et al., 2012): The E NTAILMENT data set consists of 2,770 word pairs, bal-
anced between positive ( house -building ) and negative ( leader -rider ) examples of hypernymy, with 1376
unique hyponyms and 1016 unique hypernyms. The positive examples were generated by selecting direct
hypernym relationships from WordNet, the negative examples by randomly permuting the hypernyms of
the positive examples, and then manually checking correctness.
4 Distributional Inclusion across Spaces
We test several unsupervised distributional approaches to hypernymy detection from the literature, fo-
cusing on the underlying vector space representation as the main parameter that we vary. We use the
three spaces described in Section 3. We test four hypernymy detection approaches, all of them similarity
measures based on the Distributional Inclusion Hypothesis: WeedsPrec ,balAPinc ,ClarkeDE , and invCL .
Our baseline is the standard cosine measure. We evaluate on the B LESS dataset.
To evaluate on B LESS , we follow the evaluation scheme laid out in Baroni and Lenci (2011). Given a
space and similarity measure, we compute similarity for each concept and relatum. For each concept, we
select its nearest neighbors (according to the given similarity measure) in each of the four relations (C O-
HYP, HYPER , MERO, RANDOM ), and transform the corresponding four similarities to z-scores. Across
all concepts, this yields four sets of z-normalized similarity scores, one for each relation. These four sets
describe the relative similarity of concepts to their nearest neighbors in different relations. Tukey‚Äôs Hon-
estly SigniÔ¨Åcant Difference test is used for testing whether scores differ signiÔ¨Åcantly between relations
(threshold:p&lt; 0.05).
Figure 1 shows the distributions of z-scores for invCL for the four relations, with one graph for each
of the three spaces we consider. For this illustration, we focus on invCL because it shows the overall best
performance at identifying hypernymy. The rightmost plot in Figure 1 replicates the analysis of Lenci
and Benotto (2012), who used the TypeDM space. It conÔ¨Årms their Ô¨Ånding that invCL gives signiÔ¨Åcantly
higher values to hypernyms than co-hyponyms ‚Äì at least on this space. However, in the U+W2 and
U+Sent spaces (leftmost and middle plot), invCL clearly loses any ability to rank hypernyms the highest;
indeed, in both spaces, co-hyponymy and meronymy both have signiÔ¨Åcantly higher z-scores than hyper-
nymy. Concerning the other measures, we found that they patterned with invCL . On TypeDM, ClarkeDE
andWeedsPrec had signiÔ¨Åcantly higher nearest-neighbor values for hypernyms than co-hyponyms.2On
U+W2 and U+Sent, all measures ranked co-hyponyms signiÔ¨Åcantly higher than hypernyms. With the
baseline measure, cosine , the similarity ratings for the C O-HYP relation are always the highest, no matter
the space, followed by H YPER , MERO, RANDOM in this order.
Following Kotlerman et al. (2010) and Lenci and Benotto (2012), we also report the performance of
the measures using Mean Average Precision (MAP). Average Precision (AP) is a measure often used in
2balAPinc could not be evaluated on TypeDM due to computational issues. 1028 Measure CO-HYP HYPER MERO RANDOM
U+W2
cosine .68 .20 .27 .27
ClarkeDE .66 .19 .28 .28
invCL .60 .18 .31 .28
U+Sent
cosine .66 .18 .28 .28
ClarkeDE .66 .15 .29 .28
invCL .59 .13 .34 .29
TypeDM
cosine .78 .19 .20 .29
ClarkeDE .45 .35 .25 .32
invCL .38 .36 .27 .33
Table 1: Mean Average Precision for the unsupervised measures on three spaces.
the Information Retrieval community with a maximal AP score of 1 when all relevant documents (relata
with the right relationship, in our case) are ranked at the top. We compute AP on a per-concept basis and
report the mean over all 200 AP values. An advantage of MAP is that, while the B LESS analysis method
focuses on nearest neighbors, MAP evaluates the ranking of all relata. A disadvantage of MAP is that it
does not test the degree to which a similarity measure separates different semantic relations, like Tukey
does, so it may overstate the discriminative power of a particular measure. However, it provides a more
intuitive accuracy-like number compared to the B LESS evaluation.
Table 1 shows the Mean Average Precision values for cosine ,ClarkeDE , and invCL on all three spaces.
We also computed WeedsPrec andbalAPinc results, obtaining the same picture; we focus on ClarkeDE
andinvCL because ClarkeDE is a component of invCL , and invCL is the current best measure. The results
corresponding to Lenci and Benotto‚Äôs are shown in the lowest part of Table 1, where we report numbers
for TypeDM. Like Lenci and Benotto, we Ô¨Ånd that unsupervised measures other than invCL rank co-
hyponyms the highest, and obtain relatively low results for hypernyms. For invCL in TypeDM, Lenci
and Benotto obtain 0.38 MAP for co-hyponyms and a slightly higher 0.40 for hypernyms, though they
do not report signiÔ¨Åcance testing results. We obtain 0.38 for co-hyponyms and 0.36 for hypernyms, and
the difference is not signiÔ¨Åcant.3Even though our results are slightly different from those in Lenci and
Benotto (2012), both our results and theirs point to at most a weak preference of invCL for hypernyms
over co-hyponyms. Moreover, in the U+W2 and U+Sent spaces we see that all three measures are very
poor at identifying hypernyms, and the co-hyponymy relation stubbornly persists as most relevant to all
three measures, by a large margin.
Our results thus constitute a puzzle for the Distributional Inclusion Hypothesis. It seems that there
must be some merit to the hypothesis: On one particular space, namely TypeDM, the nearest neighbors
in the hypernymy relation had higher similarity scores than any other relation by a signiÔ¨Åcant margin.
This was true for all the hypernymy detectors we studied. But even on TypeDM, the MAP evaluation
showed at most a weak hypernymy signal, and when spaces other than TypeDM were used, the effect
vanished altogether. So how strong an indication for hypernymy can we expect from distributional
inclusion measures in general? We will return to this question below, where our answer will be: The
Distributional Inclusion Hypothesis seems to hold after all, but it needs to be applied to the right kind of
dimensions ‚Äì and a supervised approach can help in picking the right dimensions.
As the unsupervised approaches struggle to detect hypernymy and do not seem robust to changes in
standard space parameters, we think it is time to consider supervised approaches. In the next section, we
explore two simple supervised approaches that show good performance and are robust to changes in the
underlying space.
3Wilcoxon signed-rank test. 1030 5 Supervised Hypernymy Detection
We use two simple, supervised models for predicting B LESS and E NTAILMENT relations. The Ô¨Årst
(Concat) is a model previously proposed by Baroni et al. (2012). The second (Diff) takes up an idea
from a footnote in Baroni et al. (2012), but while that footnote stated that the approach in question did
not work, we Ô¨Ånd that, with a few modiÔ¨Åcations, it obtains the best performance ‚Äì and can be interpreted
as a supervised version of the Distributional Inclusion Hypothesis. Note that while we used unreduced
spaces in the previous section, we now use reduced spaces throughout (these are the spaces with the 300
subscript), in order not to have more features than data points.
5.1 Models, Features, and Method
Concat: We use a standard Support Vector Machine (SVM) classiÔ¨Åer with a concatenation of vectors as
input features. SVMs are binary classiÔ¨Åers which learn the maximum margin hyperplane separating the
two classes. SVMs employ kernel functions to Ô¨Ånd the hyperplanes in higher dimensional spaces which
are nonlinear in the original space. As feature vectors for the classiÔ¨Åer, we follow Baroni et al. (2012)
and use the concatenation of the latent dimension vectors representing words. For the E NTAILMENT
dataset, we use the concatenation of the hyponym latent vector and the hypernym latent vector for each
word pair as training features, and the entails/doesn‚Äôt entail annotations as binary targets. For B LESS ,
we use the concatenation of the concept latent vector and the relatum latent vector as training features,
and the four relationship classes as targets. We choose the four-way task rather than a ‚Äúhypernymy vs.
other‚Äù classiÔ¨Åcation because B LESS contains many more co-hyponymy and random than hypernymy
pairs, which would give a very high baseline in the two-way task. Additionally, the other relations in
BLESS , in particular meronymy, may be interesting in their own right.
Since SVMs are binary classiÔ¨Åers, we use SciKit-Learn‚Äôs default setting to train 6 pairwise-relation
one-vs-one classiÔ¨Åers which vote on the Ô¨Ånal answer. We use a polynomial kernel with a degree of 3
and a penalty term of C= 1.0, and all other hyperparameters are chosen using the SciKit-Learn default
values (Pedregosa et al., 2011). No hyperparameters are tuned in any experiment.
Diff: Our second classiÔ¨Åer is a Logistic Regression (aka MaxEnt) model trained on difference vectors.
Logistic Regression is a statistical model for binary classiÔ¨Åcation. It learns a linear hyperplane sepa-
rating the classes and estimates a probability for classes using a logistic function. We selected Logistic
Regression over other possible linear classiÔ¨Åers for its natural ability to give likelihood estimates, which
we believe will be useful in future work in an application of hypernymy classiÔ¨Åcation to RTE.
As feature vectors, we use a Mikolov-inspired method of representing word pairs as the difference
vectors between the two words.4Baroni et al. (2012) suggested the use of difference vectors as input
to a classiÔ¨Åer, but reported them as unsuccessful. We found difference vectors to be excellent features,
with three important modiÔ¨Åcations: a linear classiÔ¨Åer is better than a nonlinear one; vectors must be
normalized to have a magnitude of 1 before taking the difference; and squared difference vectors must
also be included as features. So, we represent each word pair with latent vectors (u,v)as a two part
vector/angbracketleftf;g/angbracketright, where
fi=ui
/bardblu/bardbl‚àívi
/bardblv/bardbl,
gi=f2
i.
These differences features5are analogous to a supervised distributional inclusion measure. The dif-
ference between two words on a particular dimension captures the degree of distributional inclusion on
that dimension. The primary distinction between the difference features and the unsupervised measures
is that the supervised classiÔ¨Åer learns to weight the importance of different dimensions. The ffeatures
encode directional aspects of distributional inclusion: that the hyponym contexts should be included in
4After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013).
5We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting
the best. We also tried the Diff features with an SVM and other nonlinear classiÔ¨Åers, but they performed worse. 1031 Data set BLESS ENTAILMENT
Baseline .46 .50
ClassiÔ¨Åer Concat Diff Concat Diff
U+W2 300 .76 .84 .81 .85
U+Sent 300 .73 .80 .78 .82
TypeDM 300 - .82 .65 .85
Table 2: Average accuracy of Concat and Diff on B LESS and E NTAILMENT using different spaces for
feature generation.
those of the hypernym (the weight learned is positive), and the hypernym contexts should not be in-
cluded in those of the hyponym (the weight learned is negative). So like invCL , this model uses a ‚Äúproper
subset‚Äù interpretation of the Distributional Inclusion Hypothesis, but only considers selected dimensions
(i.e. those that the model assigns nonzero weights).
The difference-squared features ( g), on the other hand, typically identify dimensions that are notin-
dicative of hypernymy, by learning negative weights on them (more about this in Section 6). Thus, rather
than helping identify hypernyms, they help separate random relations from the rest.
We use a L1 regularizer with a strength of C= 1.0. All other hyperparameters are chosen using
the SciKit-Learn defaults. Since Diff is also a binary classiÔ¨Åer, we use SciKit-Learn‚Äôs default setting of
training 4 one-vs-all classiÔ¨Åers for B LESS , with the most conÔ¨Ådent classiÔ¨Åer choosing the Ô¨Ånal answer.
Method: For evaluation on B LESS , we hold out one concept and train on the remaining 199 concepts.
We also exclude from the training set any pair containing a relatum which appears in the test set. This
way, no word that appears in the test set has been seen in training. We report the average accuracy across
all concepts. We use the most frequent relation type (random) as our baseline. For the E NTAILMENT data
set, we hold out one hyponym and train on all remaining hyponyms. Again, we exclude from training
any pair containing a hypernym which appears in the test set. We report average accuracy across all
hyponyms. The data set is balanced, so the baseline is 0.5.
5.2 Results
Table 2 shows the performance of the two classiÔ¨Åers, Concat and Diff, on both the B LESS and E NTAIL -
MENT datasets, using three underlying spaces. We use the reduced versions of the three spaces, indicated
by the subscript 300. Note that the Concat classiÔ¨Åer could not converge using features from TypeDM 300,
so we omit the result. With both methods, we obtain a high accuracy on the two datasets, with results
around .8 against baselines around .5. Our best result is .84 on B LESS and .85 on E NTAILMENT . More-
over, both approaches are in general robust to changes in space parameters (with TypeDM/Concat an
outlier). Still, the U+W2 300space seems to be the best for this task: Its scores are signiÔ¨Åcantly6higher
than the rest, except for TypeDM on E NTAILMENT , which achieves the same score as U+W2 300. Diff
achieves signiÔ¨Åcantly higher results than Concat.
When provided more information, Concat outperforms Diff. For instance, if cross-validation is done
over all pairs in B LESS in the U+W2 300space, Concat achieves .98 accuracy, while Diff obtains .90.
However, in this setting the same words appear in the training and test sets (albeit in different pairs).
We take this to mean that Concat is memorizing, rather than learning the hypernymy relation. This
emphasizes the need for our stricter evaluation that prevents repetition between training and test sets.
Clearly, both classiÔ¨Åers do fairly well at predicting hypernymy relations between words, regardless
of space. Naturally, one should ask what are the classiÔ¨Åers capturing that the unsupervised measures
are missing? We propose that the supervised classiÔ¨Åers perform essentially the same operation as the
unsupervised measures, but are learning to determine the relevance of dimensions. In particular, Diff
is learning weights on vector difference features. This is equivalent to doing selective distributional
inclusion. In the next section, we test this Selective Distributional Inclusion Hypothesis.
6Wilcoxon signed-rank test, p &lt; . 001. 1032 ‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzU+W2 proj, cosine
‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzU+W2 proj, ClarkeDE
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè
‚àí1.5‚àí1.0‚àí0.50.00.51.01.5
Co‚àíhyp Hyper Mero RandomzU+W2 proj, invCLFigure 2: Distributions of relata scores across concepts using the cosine ,ClarkeDE , and invCL measures
(after per-concept z-normalization). Here we use the selected dimensions of the U+W2 projspace.
6 Selective Distributional Inclusion
In order to test how well our supervised model is capturing the notion of selective distributional inclusion,
we test each of the unsupervised measures on a smaller space, limited only to the dimensions preferred
by the classiÔ¨Åer. We emphasize that we do notaim to show that our supervised method outperforms
unsupervised methods, but rather that the unsupervised methods beneÔ¨Åt greatly from feature selection.
Additionally, we analyze which dimensions are selected by the classiÔ¨Åer to facilitate understanding of
why these dimensions are important.
6.1 Experiment
We train the Diff classiÔ¨Åer using the dimensionality-reduced U+W2 300space with the same method we
use in Section 5. We take the classiÔ¨Åer‚Äôs learned hyperplane separating hypernyms from other relations,
and project the hyperplane back into the original U+W2 space.7We select the 500 dimensions in the orig-
inal space that are most relevant according to the classiÔ¨Åer weights, and test the unsupervised measures
on this new space, which we denote as U+W2 proj.8
The 500 most relevant dimensions are selected as follows: We select the 250 most negatively weighted
original dimensions using the difference features f. These are the features that have smaller values for
hyponyms (e.g. dog) than for hypernyms (e.g. animal ), so they characterize hypernymy. We further select
the 250 most positively weighted original dimensions using the squared-differences features g. These
are the ones where a large difference does not indicate hypernymy.
Figure 2 shows the boxplots for the B LESS analysis: the distributions of nearest-neighbor similarity
scores for the four different semantic relations, for the measures cosine ,ClarkeDE , and invCL . We see
thatinvCL now easily discriminates hypernymy from the other relations in the backprojected space. (The
difference of H YPER and C O-HYP is signiÔ¨Åcant.) This is even though the space is based on U+W2, where
invCL failed to rate hypernyms higher than co-hypernyms in Section 4. Unsurprisingly, cosine , which
does not measure distributional inclusion, still prefers C O-HYP.
Table 3 shows the MAP scores for three of the measures in the new U+W2 projspace. (The results
forbalAPinc andWeedsPrec are slightly worse than ClarkeDE .) All measures except for cosine assign
higher scores to hypernyms than they did in the original space (compare to U+W2 part of Table 1). But
it is only invCL that ranks hypernyms signiÔ¨Åcantly higher than co-hyponyms.9
7Ideally we would train on the original space to inspect the relevant dimensions. However, there are more dimensions than
examples, so we train on the SVD space and backproject.
8Note that U+W2 proj varies slightly from concept to concept, since the hyperplane is learned on a per-concept basis. It is
important that we use the linear Diff classiÔ¨Åer for this reverse-projection procedure, as the separating hyperplane must be linear
in order to complete the projection. In particular, the hyperplane in the Concat classiÔ¨Åer cannot be easily backprojected, since
it exists in a higher dimensional space than the projection matrix. Furthermore, it is important that we use a classiÔ¨Åer trained
using the difference features because of its analogy to the Distributional Inclusion Hypothesis.
9Wilcoxon signed-rank test, p &lt; . 001. To check that the measures are being improved by the dimension selection and not 1033 Measure CO-HYP HYPER MERO RANDOM
U+W2 proj
cosine .69 .20 .24 .28
ClarkeDE .55 .39 .24 .29
invCL .42 .58 .24 .29
Table 3: Mean Average Precision for the unsupervised measures after selecting the top dimensions from
a supervised model.
For this experiment, we train on all of B LESS except for one concept and then evaluate the unsuper-
vised models on the held-out concept ‚Äì that is a setting that could, in principle, be used as a hypernymy
detector. If we instead train the supervised model on all of B LESS to determine an upper bound of how
well dimension selection can do on this dataset, MAP for invCL rises to .67.
Overall, these experiments provide strong evidence for the Selective Distributional Inclusion Hypoth-
esis: The Distributional Inclusion Hypothesis holds, but only for relevant dimensions. In addition, hy-
pernymy detectors need to test for ‚Äúproper inclusion‚Äù of distributional contexts in order to really Ô¨Ånd
hypernyms.
Analysis of Selected Dimensions. We examine the 500 dimensions selected by the above procedure,
in order to see what the classiÔ¨Åer is learning. As this is for analysis only, the dimensions were selected
by training on all data.
Recall that the difference-squared gfeatures can be interpreted as dimensions that the classiÔ¨Åer deems
not indicative of hypernymy. 200 out of the 250 most relevant dimensions by gare Computer Science
related terms like software ,conÔ¨Ågure , orLinux . Since ukWaC, the largest corpus we use, is web-based,
it makes sense that it has many CS-related terms, which are noise when it comes to hypernymy detection
for B LESS concepts. Also, we Ô¨Ånd that while the supervised approach needs the negative information
from thegfeatures (for Diff in the U+W2 300space, omitting gfeatures yields a drop from .84 to .8),
the unsupervised measures cannot use it. Dropping gfeatures improves invCL results from .58 to .61.
Theg-based dimensions are explicitly those for which distributional inclusion should nothold, so they
constitute noise to the unsupervised approaches.
Theffeatures can be interpreted as dimensions that characterize hypernyms. An inspection reveals
two clear patterns. First, the features are topically relevant for the B LESS dataset. The 17 concept classes
in the dataset belong to three broader groups: animals, plants, and artifacts. An annotation of the 250
dimensions by one of the authors showed that 58 dimensions are typical of animals ( parasite ,extinct ), 14
typical of vegetables ( Ô¨Çora,nutrient ), 80 typical of artifacts ( repair ,mechanical ), 49 are general terms
(Ô¨Ånd,worthy ), and 49 have no clear interpretation ( thee,enigmatic ). Second, the features are general
terms. For instance, for animals we Ô¨Ånd terms like animal ,insect ,creature ,fauna ,species ,evolutionary ,
pathogen ,nature ,ecology . We also Ô¨Ånd many hypernyms, including many concept class names.
Clearly, the selected features are domain dependent; most are directly related to the concepts and
concept classes of B LESS . We expect that our method should work well for other data sets, given its high
accuracy and the strict training procedure. However, these features are unlikely to be global indicators of
hypernymy. This emphasizes the need, in future work, to Ô¨Ånd a way to automatically determine relevance
on a per-word basis.</body>
  <conclusion>In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional ap-
proaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively applied
to a set of relevant dimensions.
just by restricting to a smaller space, we evaluated the similarity measures on a variation of the U+W2 space which uses 500
randomly selected dimensions from the original space. The results are approximately unchanged from those on the original
U+W2 space. 1034 We have tested two simple supervised approaches to distributional hypernymy detection and have
found that they show good performance, and are robust to changes in the underlying space. Our best
classiÔ¨Åer achieves .84 accuracy on B LESS and .85 on the E NTAILMENT dataset of Baroni et al. (2012). It
uses features that encode dimension-wise difference between vectors. This classiÔ¨Åer can be interpreted
as selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as an
effective way to implement selective distributional inclusion.
The next natural step is to use the supervised features to guide development of an unsupervised mea-
sure for hypernymy detection: Now that we have examples, we hope to propose a method which selects
relevant features automatically. We also would like to explore detection of other relationships, such
as meronymy. Finally, we would like to perform an extrinsic evaluation of our hypernymy detection
approach in an actual RTE system.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics , pages 1‚Äì10, Edinburgh,
UK, July. Association for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation ,
43(3):209‚Äì226.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics , pages 23‚Äì32, Avignon, France, April. Association for Computational
Linguistics.
Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics , pages 57‚Äì64, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005. Ontology Learning from Text: Methods, Evaluation
and Applications . Frontiers in ArtiÔ¨Åcial Intelligence and Applications Series. IOS Press, Amsterdam.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for Ô¨Åne-grained semantic verb relations.
InProceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , pages 33‚Äì40.
Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relations
from heterogeneous sources of evidence. Ontology Learning from Text: Methods, evaluation and applications .
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop on Geometrical Models of Natural Language Semantics , pages 112‚Äì119, Athens, Greece, March.
Association for Computational Linguistics.
Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of the
20th International Conference on Computational Linguistics , page 247. Association for Computational Linguis-
tics.
Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human Language Technology-Volume 1 , pages 1‚Äì8. Association
for Computational Linguistics.
10http://www.tacc.utexas.edu 1035 Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Com-
putational Linguistics , 32(1):83‚Äì135.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th
Conference on Computational Linguistics , pages 539‚Äì545, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Aur√©lie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectors. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers) , pages 440‚Äì445, SoÔ¨Åa, Bulgaria, August. Association for Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language Engineering , 16:359‚Äì389, 10.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In *SEM
2012: The First Joint Conference on Lexical and Computational Semantics ‚Äì Volume 1: Proceedings of the
main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012) , pages 75‚Äì79, Montr√©al, Canada, 7-8 June. Association for Computational
Linguistics.
Alessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Lin-
guistics , 20(1):1‚Äì31.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th international Joint Conference on ArtiÔ¨Åcial intelligence , pages 1492‚Äì
1493.
Dekang Lin. 1998. An information-theoretic deÔ¨Ånition of similarity. In Proceedings of the 15th International
Conference on Machine Learning , volume 98, pages 296‚Äì304.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , pages 746‚Äì751, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Gregory L. Murphy. 2002. The Big Book of Concepts . MIT Press, Boston, MA.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
44th annual meeting of the Association for Computational Linguistics .
Fabian Pedregosa, Ga ¬®el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna-
peau, Matthieu Brucher, MMatthieu Perrot, and √âdouard Duchesnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research , 12:2825‚Äì2830.
Enrico Santus. 2013. SLQS: An entropy measure. Master‚Äôs thesis, University of Pisa.
Rion Snow, Daniel Jurafsky, and Andrew Y . Ng. 2005. Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and L√©on Bottou, editors, Advances in Neural Information Processing
Systems 17 , pages 1297‚Äì1304, Cambridge, MA. MIT Press.
Rion Snow, Daniel Jurafsky, and Andrew Y . Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
InProceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics , ACL-44, pages 801‚Äì808, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of ArtiÔ¨Åcial Intelligence Research , 37:141‚Äì188.
Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics , 32(3):379‚Äì416.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing ,
pages 81‚Äì88. 1036 Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
InProceedings of the 20th International Conference on Computational Linguistics , pages 1015‚Äì1021, Geneva,
Switzerland, Aug 23‚ÄìAug 27. Association for Computational Linguistics, COLING.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment.
InProceedings of the 43rd Annual Meeting of the Association for Computational Linguistics , pages 107‚Äì114,
Ann Arbor, Michigan, June. Association for Computational Linguistics.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computa-
tional linguistics , 35(3):435‚Äì461.</biblio>


  <preamble>BLESS.pdf</preamble>
  <titre>How we BLESSed distributional semantic evaluation</titre>
  <auteurs>
    <auteur>
      <name>Marco Baroni</name>
      <mail>marco.baroni@unitn.it</mail>
      <affiliation>University of Trento
Trento, Italy</affiliation>
    </auteur>
    <auteur>
      <name>Alessandro Lenci</name>
      <mail>alessandro.lenci@ling.unipi.it</mail>
      <affiliation>University of Pisa
Pisa, Italy</affiliation>
    </auteur>
  </auteurs>
  <abstract>We introduce BLESS, a data set speciÔ¨Åcally
designed for the evaluation of distributional
semantic models. BLESS contains a set of tu-
ples instantiating different, explicitly typed se-
mantic relations, plus a number of controlled
random tuples. It is thus possible to assess the
ability of a model to detect truly related word
pairs, as well as to perform in-depth analy-
ses of the types of semantic relations that a
model favors. We discuss the motivations for
BLESS, describe its construction and struc-
ture, and present examples of its usage in the
evaluation of distributional semantic models.</abstract>
  <introduction>
In NLP, it is customary to distinguish between in-
trinsic evaluations , testing a system in itself, and
extrinsic evaluations , measuring its performance in
some task or application (Sparck Jones and Galliers,
1996). For instance, the intrinsic evaluation of a de-
pendency parser will measure its accuracy in identi-
fying speciÔ¨Åc syntactic relations, while its extrinsic
evaluation will focus on the impact of the parser on
tasks such as question answering or machine trans-
lation. Current approaches to the evaluation of Dis-
tributional Semantic Models (DSMs, also known
as semantic spaces, vector-space models, etc.; see
Turney and Pantel (2010) for a survey) are task-
oriented. Model performance is evaluated in ‚Äúse-
mantic tasks‚Äù, such as detecting synonyms, recog-
nizing analogies, modeling verb selectional prefer-
ences, ranking paraphrases, etc. Measuring the per-
formance of DSMs on such tasks represents an in-direct test of their ability to capture lexical mean-
ing. The task-oriented benchmarks adopted in dis-
tributional semantics have not speciÔ¨Åcally been de-
signed to evaluate DSMs. For instance, the widely
used TOEFL synonym detection task was designed
to test the learners‚Äô proÔ¨Åciency in English as a sec-
ond language, and not to investigate the structure of
their semantic representations (cf. Section 2).
To gain a real insight into the abilities of DSMs to
address lexical semantics, existing benchmarks must
be complemented with a more intrinsically oriented
approach, to perform direct tests on the speciÔ¨Åc as-
pects of lexical knowledge captured by the models.
In order to achieve this goal, three conditions must
be met: (i) to single out the particular aspects of
meaning that we want to focus on in the evaluation
of DSMs; (ii) to design a data set that is able to ex-
plicitly and reliably encode the target semantic infor-
mation; (iii) to specify the evaluation criteria of the
system performance on the data set, in order to get
an estimate of the intrinsic ability of DSMs to cope
with the selected semantic aspects. In this paper, we
address these three conditions by presenting BLESS
(Baroni and Lenci Evaluation of Semantic Spaces),
a new data set speciÔ¨Åcally geared towards the in-
trinsic evaluation of DSMs, downloadable from:
http://clic.cimec.unitn.it/distsem .
</introduction>
  <body>There are several benchmarks that have been widely
adopted for the evaluation of DSMs, all of them cap-
turing interesting challenges a DSM should meet.
We brieÔ¨Çy review here some commonly used and
representative benchmarks, and discuss why we felt 2 the need to add BLESS to the set. We notice at the
outset of this discussion that we want to carve out a
space for BLESS, and not to detract from the impor-
tance and usefulness of other data sets. We further
remark that we focus on data sets that, like BLESS,
are monolingual English and, while task-oriented,
not aimed at a speciÔ¨Åc application setting (such as
machine translation or ontology population).
Probably the most commonly used benchmark in
distributional semantics is the TOEFL synonym de-
tection task introduced to computational linguis-
tics by Landauer and Dumais (1997). It consists of
80 multiple-choice questions, each made of a target
word (a noun, verb, adjective or adverb) and 4 re-
sponse words, 1 of them a synonym of the target.
For example, given the target levied , the matched
words are imposed ,believed ,requested ,correlated ,
the Ô¨Årst one being the correct choice. The task for
a system is then to pick the true synonym among
the responses. The TOEFL task focuses on a single
semantic relation, namely synonymy. Synonymy is
actually not a common semantic relation and one of
the hardest to deÔ¨Åne, to the point that many lexi-
cal semanticists have concluded that true synonymy
does not exist (Cruse, 1986). Just looking at a few
examples of synonym pairs from the TOEFL set will
illustrate the problem: discrepancy /difference ,pro-
liÔ¨Åc/productive ,percentage /proportion ,to market /to
sell,color /hue. Moreover, the criteria adopted to
choose the distractors (probably motivated by the
language proÔ¨Åciency testing purposes of TOEFL)
are not known. By looking at the set, it is hard
to discern a coherent pattern. In certain cases, the
distractors are semantically close to the target word
(volume ,sample andproÔ¨Åt forpercentage ), whereas
in other cases they are not ( home ,trail, and song for
annals ). It it thus not clear whether we are asking the
models to distinguish a semantically related word
(the synonym) from random elements, or a more
tightly related word (the synonym, again) from other
related words. The TOEFL task, Ô¨Ånally, is based on
a discrete choice (either you get the right word, or
you don‚Äôt), with the result that evaluation is ‚Äúquan-
tized‚Äù, leading to large accuracy gains for small ac-
tual differences (one model that guesses one more
synonym right than another gets 1.25% more points
in percentage accuracy).
The WordSim 353 data set (Finkelstein et al.,2002) is a widely used example of semantic simi-
larity rating set (see also Rubenstein and Goode-
nough (1965) and Miller and Charles (1991)). Sub-
jects were asked to rate a set of 353 word pairs on a
‚Äúsimilarity‚Äù scale and average ratings for each pair
were computed. Models are then evaluated in terms
of correlation of their similarity scores with aver-
age ratings across pairs. From the point of view
of assessing the performance of a DSM, the Word-
Sim (and related) similarity ratings are a mixed bag,
in two senses. First, the data set contains a vari-
ety of different semantic relations. In a recent se-
mantic annotation of the WordSim performed by
Agirre et al. (2009) we Ô¨Ånd that, among the 174
pairs with above-median score (and thus presum-
ably related), there is 1 identical pair, 17 synonym
pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,
6 holo-/meronym pairs and 92 (more than half) pairs
that are ‚Äútopically related, but none of the above‚Äù.
Second, the scores are a mixture of intuitions about
which of these relations are more semantically tight
and intuitions about more or less connected pairs
within each of the relations. For example, among
the top-rated scores we Ô¨Ånd synonyms such as jour-
ney/voyage and coordinate concepts ( king/queen ).
If we look at the relations characterizing pairs
around the median rating, we Ô¨Ånd both less ‚Äúper-
fect‚Äù synonyms ( monk/brother , that are synonymous
only under an unusual sense of brother ) and less
close coordinates ( skin/eye ), as well as pairs in-
stantiating other, less taxonomically tight relations,
such as many syntagmatically connected items ( fam-
ily/planning ,disaster/area ,bread/butter ). Appar-
ently, a single scale is merging intuitions about se-
mantic similarity of speciÔ¨Åc pairs and semantic sim-
ilarity of different relations.
A perhaps more principled way to evaluate DSMs
that has recently gained some popularity is the con-
cept categorization task , where a DSM has to clus-
ter a set of nouns expressing basic-level concepts
into gold standard categories. A particularly care-
fully constructed example is the Almuhareb-Poesio
(AP) set of 402 concepts introduced in Almuhareb
(2006). Concept categorization sets also include the
Battig (Baroni et al., 2010) and ESSLLI 2008 (Ba-
roni et al., 2008) lists. The AP concepts must be
clustered into 21 classes, each represented by be-
tween 13 and 21 nouns. Examples include the ve- 3 hicle class ( helicopter ,motorcycle . . . ), the motiva-
tion class ( ethics ,incitement , . . . ), and the social
unitclass ( platoon ,branch ). The concepts are bal-
anced in terms of frequency and ambiguity, so that,
e.g., the treeclass contains a common concept such
aspine but also the casuarina tree, as well as the
samba tree, that is not only an ambiguous term, but
one where the non-arboreal sense dominates.
Concept categorization data sets, while interest-
ing to simulate one of the basic aspects of human
cognition, are limited to one kind of semantic re-
lation (discovering coordinates). More importantly,
the quality of the results will depend not only on the
underlying DSMs, but also on the clustering algo-
rithm being used (and on how this interacts with the
overall structure of the DSM), thus making it hard
to interpret the performance of DSMs. The forced
‚Äúhard‚Äù category choice is also problematic, and ex-
aggerates performance differences between models
especially in the presence of ambiguous terms (a
model that puts samba in the occasion class with
dance andball might be penalized as much as a
model that puts it in the monetary currency class).
A more general issue with all benchmarks is that
tasks are based on comparing a single quality score
for each considered model (accuracy for TOEFL,
correlation for WordSim, a clustering quality mea-
sure for AP, etc.). This gives little insight into how
andwhy the models differ. Moreover, there is no
well-established statistical procedure to assess sig-
niÔ¨Åcance of differences for most commonly used
measures. Finally, either because the data sets were
not originally intended as standard benchmarks, or
even on purpose, they all are likely to cause coverage
problems even for DSMs trained on very large cor-
pora. Think of the presence of extremely rare nouns
likecasuarina in AP, of proper nouns in WordSim (it
is not clear to us that DSMs are adequate semantic
models for referring expressions ‚Äì at the very least
they should not be mixed up lightly with common
nouns), or multi-word expressions in other data sets.
3 How we intend to BLESS distributional
semantic evaluation
DSMs measure the distributional similarity between
words, under the assumption that proximity in distri-
butional space models semantic relatedness, includ-ing, as a special case, semantic similarity (Budanit-
sky and Hirst, 2006). However, semantically related
words in turn differ for the type of relation hold-
ing between them: e.g., dogis strongly related to
both animal andtail, but with different types of re-
lations. Therefore, evaluating the intrinsic ability of
DSMs to represent the semantic space of a word en-
tails both (i) determining to what extent words close
in semantic space are actually semantically related,
and (ii) analyzing, among related words, which type
of semantic relation they tend to instantiate. Two
models can be equally very good in identifying se-
mantically related words, while greatly differing for
the type of related pairs they favor.
The BLESS data set complies with both these
constraints. The set is populated with tuples ex-
pressing a relation between a target concept (hence-
forth referred to as concept ) and a relatum concept
(henceforth referred to as relatum ). For instance, in
the BLESS tuple coyote-hyper-animal , the concept
coyote is linked to the relatum animal via the hy-
pernymy relation (the relatum is a hypernym of the
concept). BLESS focuses on a coherent set of basic-
level nominal concrete concepts and a small but ex-
plicit set of semantic relations, each instantiated by
multiple relata. Depending on the type of relation,
relata can be nouns, verbs or adjectives. Moreover,
BLESS also contains, for each concept, a number of
random ‚Äúrelatum‚Äù words that are not semantically
related to the concept. Thus, it also allows to evalu-
ate a model in terms of its ability to harvest related
words given a concept (by comparing true and ran-
dom relata), and to identify speciÔ¨Åc types of relata,
both in terms of semantic relation and part of speech.
A data set intending to represent a gold standard
for evaluation should include tests items that are as
little controversial as possible. The choice of re-
stricting BLESS to concrete concepts is motivated
by the fact that they are by far the most studied ones,
and there is better agreement about the relations that
characterize them (Murphy, 2002; Rogers and Mc-
Clelland, 2004).
As for the types of relation to include, we are
faced with a dilemma. On the one hand, there is
wide evidence that taxonomic relations, the best un-
derstood type, only represent a tiny portion of the
rich spectrum covered by semantic relatedness. On
the other hand, most of these wider semantic rela- 4 tions are also highly controversial, and may easily
lead to questionable classiÔ¨Åcations. For instance,
concepts are related to events, but often it is not clear
how to distinguish the events expressing a typical
function of nominal concepts (e.g., carandtrans-
port), from those events that are also strongly re-
lated to them but without representing their typical
function sensu stricto (e.g., carandÔ¨Åx). As will be
shown in Section 4, the BLESS data set tries to over-
come this dilemma by attempting a difÔ¨Åcult com-
promise: Semantic relations are not limited to tax-
onomic types and also include attributes and events
strongly related to a concept, but in these cases we
have resorted to underspeciÔ¨Åcation, rather than com-
mitting ourselves to questionable granular relations.
BLESS strives to capture those differences and
similarities among DSMs that do not depend on
coverage, processing choices or lexical preferences.
BLESS has been constructed using a publicly avail-
able collection of corpora for reference (see Section
4.4 below), which means that anybody can train a
DSM on the same data and be sure to have perfect
coverage (but this is not strictly necessary). For each
concept and relation, we pick a variety of relata (see
next section) in order to abstract away from inciden-
tal gaps of models or different lexical/topical prefer-
ences. For example, the concept robin has 7 hyper-
nyms including the very general and non-technical
animal andbird and the more speciÔ¨Åc and techni-
calpasserine . A model more geared toward techni-
cal terminology might assign a high similarity score
to the latter, whereas a commonsense-knowledge-
oriented DSM might pick bird. Both models have
captured similarity with a hypernym, and we have
no reason, in general semantic terms, to penalize one
or the other. To maximize coverage, we also make
sure that, for each concept and relation, a reason-
able number of relata are frequently attested in our
reference corpora (see statistics below), we only in-
clude single-word relata and, where appropriate, we
include multiple forms for the same relatum (both
sock andsocks as coordinates of scarf ‚Äì as discussed
in Section 4.1, we avoided similar ambiguous items
as target concepts).
Currently, distributional models for attributional
similarity and relational similarity (Turney, 2006)
are tested on different data sets, e.g., TOEFL and
SAT respectively (brieÔ¨Çy, attributional similaritypertains to similarity between a pair of concepts in
terms of shared properties, whereas relational sim-
ilarity measures the similarity of the relations in-
stantiated by couples of concept pairs ). Conversely,
BLESS is not biased towards any particular type of
semantic similarity and thus allows both families of
models to be evaluated on the same data set. Given
a concept, we can analyze the types of relata that are
selected by a model as more attributionally similar
to the target. Alternatively, given a concept-relatum
pair instantiating a speciÔ¨Åc semantic relation (e.g.,
hypernymy) we can evaluate a model ability to iden-
tify analogically similar pairs, i.e., others concept-
relatum pairs instantiating the same relation (we do
not illustrate this possibility here).
Finally, by collecting distributions of 200 similar-
ity values for each relation, BLESS allows reliable
statistical testing of the signiÔ¨Åcance of differences
in similarity within a DSM (for example, using the
procedure we present in Section 5 below), as well
as across DSMs (for example, via a linear/ANOV A
model with relations and DSMs as factors ‚Äì not il-
lustrated here).
4 Construction
4.1 Concepts
BLESS includes 200 distinct English concrete
nouns as target concepts, equally divided be-
tween living and non-living entities. Concepts
have been grouped into 17 broader classes: AM-
PHIBIAN REPTILE (including amphibians and rep-
tiles: alligator ),APPLIANCE (toaster ),BIRD
(crow ),BUILDING (cottage ),CLOTHING (sweater ),
CONTAINER (bottle ),FRUIT (banana ),FURNI -
TURE (chair ),GROUND MAMMAL (beaver ),IN-
SECT (cockroach ),MUSICAL INSTRUMENT (vio-
lin),TOOL (i.e., manipulable tools or devices: ham-
mer),TREE (birch ),VEGETABLE (cabbage ),VEHI -
CLE(bus),WATER ANIMAL (including Ô¨Åsh and sea
mammals: herring ),WEAPON (dagger ).
All 200 BLESS concepts are single-word nouns
in the singular form (we avoided concepts such as
socks whose surface form might change depending
on lemmatization choices). The major source we
used to select the concepts were the McRae Norms
(McRae et al., 2005), a collection of living and non-
living basic-level concepts described by 725 sub- 5 jects with semantic features, each tagged with its
property type. As further constraints guiding our
selection, we wanted concepts with a reasonably
high frequency (cf. Section 4.4), we avoided am-
biguous or highly polysemous concepts and we bal-
anced inter- and intra-class composition. Classes in-
clude both prototypical and atypical instances (e.g.,
robin andpenguin forBIRD ), and have a wide spec-
trum of internal variation (e.g., the class VEHICLE
contains wheeled, air and sea vehicles). 175 BLESS
concepts are attested in the McRae Norms, while the
remnants were selected by the authors according to
the above constraints. The average number of con-
cepts per class is 11.76 (median 11; min. 5 AMPHIB -
IAN REPTILE ; max. 21 GROUND MAMMAL ).
4.2 Relations
For each concept noun, BLESS includes several
relatum words, linked to the concept by one of
the following 5 relations. COORD : the relatum
is a noun that is a co-hyponym (coordinate) of
the concept, i.e., they belong to the same (nar-
rowly or broadly deÔ¨Åned) semantic class: alligator-
coord-lizard ;HYPER : the relatum is a noun that
is a hypernym of the concept: alligator-hyper-
animal ;MERO : the relatum is a noun referring
to a part/component/organ/member of the concept,
or something that the concept contains or is made
of:alligator-mero-mouth ;ATTRI : the relatum is
an adjective expressing an attribute of the concept:
alligator-attri-aquatic ;EVENT : the relatum is a
verb referring to an action/activity/happening/event
the concept is involved in or is performed by/with
the concept: alligator-event-swim . BLESS also
includes the relations RAN.N,RAN.Jand RAN.V,
which relate the target concepts to control tuples
with random noun, adjective and verb relata, respec-
tively.
The BLESS relations cover a wide spectrum of
information useful to describe a target concept and
to qualify the notion of semantic relatedness: taxo-
nomically related entities ( hyper andcoord ), typical
attributes ( attri), components ( mero ), and associated
events ( event ). However, except for hyper andco-
ord(corresponding to the standard relations of class
inclusion and co-hyponymy respectively), the other
BLESS relations are highly underspeciÔ¨Åed. For in-
stance, mero corresponds to a very broad notion ofmeronymy, including not only parts ( dog-tail ), but
also the material ( table-wood ) as well as the mem-
bers ( hospital-patient ) of the entity the target con-
cept refers to (Winston et al., 1987); event is used to
represent the behaviors of animals ( dog-bark ), typi-
cal functions of instruments ( violin-play ), and events
that are simply associated with the target concept
(car-park );attri captures a large range of attributes,
from physical ( elephant-big ) to evaluative ones ( car-
expensive ). As we said in section 3, we did not at-
tempt to further specify these relations to avoid any
commitment to controversial ontologies of property
types. Note that we exclude synonymy both because
of the inherent problems in this very notion (Cruse,
1986), and because it is impossible to Ô¨Ånd convinc-
ing synonyms for 200 concrete concepts.
In BLESS, we have adopted the simplifying as-
sumption that each relation type has relata belonging
to the same part of speech: nouns for hyper ,coord
andmero , verbs for event , and adjectives for attri.
Therefore, we abstract away from the fact that the
same semantic relation can be realized with different
parts of speech, e.g., a related event can be expressed
by a verb ( transport ) or by a noun ( transportation ).
4.3 Relata
The relata of the non-random relations are English
nouns, verbs and adjectives selected and validated
by both authors using two types of sources: se-
mantic sources (the McRae Norms (McRae et al.,
2005), WordNet (Fellbaum, 1998) and ConceptNet
(Liu and Singh, 2004)) and text sources (Wikipedia
and the Web-derived ukWaC corpus, see Section 4.4
below). These resources greatly differ in dimension,
origin and content and therefore provide comple-
mentary views on relata. Their relative contribution
to BLESS also depends on the type of relation and
the target concept. For instance, the rich taxonomic
structure of WordNet has been the main source of in-
formation for many technical hypernyms (e.g. gym-
nosperm ,oscine ), which instead are missing from
more commonsense-oriented resources such as the
McRae Norms and ConceptNet. Meronyms are
rarer in WordNet, and were collected mainly from
the latter two resources, with many technical terms
(e.g., parts of ships, weapons) harvested from the
Wikipedia entries for the target concepts.
Attributes and events were collected from McRae 6 Norms, ConceptNet and ukWaC. In the McRae
Norms, the number of features per concept is fairly
limited, but they correspond to highly distinctive,
prototypical and cognitively salient properties. Con-
ceptNet instead provides a much wider array of as-
sociated events and attributes that are part of our
commonsense knowledge about the target concepts
(e.g., the events park,steal andbreak , etc. for car).
ConceptNet relations such as Created by,Used for,
Capable ofetc. have been analyzed to identify po-
tential event relata, while the Has property relation
has been inspected to look for attributes. The most
salient adjectival and verbal collocates of the tar-
get nouns in the ukWaC corpus were also used to
identify associated attributes and events. For in-
stance, the target concept elephant is not attested in
the McRae Norms and has few properties in Con-
ceptNet. Thus, many of its related events have been
harvested from ukWaC. They include verbs such as
hunt,kill, etc. which are quite salient and frequent
with respect to elephants, although they can hardly
be deÔ¨Åned as prototypical properties of this animal.
As a result of the combined use of such different
types of sources, the BLESS relata are representative
of a wide spectrum of semantic information about
the target concepts: they include domain-speciÔ¨Åc
terms side by side to commonsense ones, very dis-
tinctive features of a concept (e.g., hoot forowl)
together with attributes and events that are instead
shared by a whole class of concepts (e.g., all animals
have relata such as eat,feed, and live), prototypical
features as well as events and attributes that are sta-
tistically salient for the target, etc.
In many cases, the concept properties contained
in semantic sources are expressed with phrases, e.g.,
lay eggs ,eat grass ,live in Africa , etc. We decided,
however, to keep only single-word relata in BLESS,
because DSMs are typically populated with single
words, and, when they are not, they differ in the
kinds of multi-word elements they store. There-
fore, phrasal relata have always been reduced to
their head: a verb for properties expressed by a verb
phrase, and a noun for properties expressed by a
noun phrase. For instance, from the property lay
eggs, we derived the event relatum lay.
To extract the random relata, we adopted the fol-
lowing procedure. For each relatum that instantiates
a true relation with the concept, we also randomlypicked from our combined corpus (cf. Section 4.4)
another lemma with the same part of speech, and
frequency within 1 absolute logarithmic unit from
the frequency of the corresponding true relatum.
Since picking a random term does not guarantee
that it will not be related to the concept, we Ô¨Åltered
the extracted list by crowdsourcing, using the Ama-
zon Mechanical Turk via the CrowdFlower interface
(CF).1We presented CF workers with the list of
about 15K concept+random-term pairs selected with
the procedure we just described, plus a manually
checked validation set (a ‚Äúgold set‚Äù in CF terminol-
ogy) comprised of 500 concept+true-relatum pairs
and 500 concept+random-term pairs (these elements
are used by CF to determine the reliability of work-
ers, and discard the ratings of unreliable ones), plus a
further set of 1.5K manually checked concept+true-
relatum pairs to make the random-true distribution
less skewed. The workers‚Äô task was, for each pair,
to check a YES radio button if they thought there is
a relation between the words, NO otherwise. The
words were annotated with their part of speech, and
workers were instructed to pay attention to this in-
formation when making their choices. Extensive
commented examples of both related pairs and un-
related ones were also provided in the instruction
page. A minimum of 2 CF workers rated each pair,
and, conservatively, we preserved only those items
(about 12K) that were unanimously rated as unre-
lated to their concept by the judges. See Table 1 for
summary statistics about the preserved random sets
(nouns: RAND .N, adjectives: RAN.J, verbs: RAN.V).
4.4 BLESS statistics
For frequency information, we rely on the combi-
nation of the freely available ukWaC and Wackype-
dia corpora (size: 1.915B and 820M tokens, respec-
tively).2The data set contains 200 concepts that
have a mean corpus frequency of 53K occurrences
(min. 1416 chisel , max. 793K car). The relata of
these concepts (26,554 in total) are distributed as re-
ported in Table 1.
Note that the distributions reÔ¨Çect certain ‚Äúnatural‚Äù
differences between relations (hypernyms tend to be
more frequent words than coordinates, but there are
1http://crowdflower.com/
2http://wacky.sslmit.unibo.it/ frequency cardinality
relation min avg max min avg max
COORD 0 37K 1.7M 6 17.1 35
HYPER 31 138K 1.9M 2 6.7 15
MERO 0 133K 2M 2 14.7 53
ATTRI 0 501K 3.7M 4 13.6 27
EVENT 0 517K 5.4M 6 19.1 40
RAN.N 0 92K 2.4M 16 32.9 67
RAN.J 1 472K 4.5M 3 10.9 24
RAN.V 1 508K 7.7M 4 16.3 34
Table 1: Distribution (minimum, mean and maximum) of
the relata of all BLESS concepts: the frequency columns
report summary statistics for corpus counts across relata
instantiating a relation; the cardinality columns report
summary statistics for number of relata instantiating a
relation across the 200 concepts, only considering relata
with corpus frequency ‚â•100.
more coordinates than hypernyms, etc.). Instead of
trying to artiÔ¨Åcially control for these differences, we
assess their impact in Section 5 by looking at the
behavior of baselines that exploit the frequency and
cardinality of relations as proxies to semantic simi-
larity (such factors could also be entered as regres-
sors in a linear model).
5 Evaluation
This section illustrates one possible way to use
BLESS to explore and evaluate DSMs. Given the
similarity scores provided by a model for a concept
with all its relata across all relations, we pick the re-
latum with the highest score (nearest neighbour) for
each relation (see discussion in Section 3 above on
why we allow models to pick their favorite from a
set of relata instantiating the same relation). In this
way, for each of the 200 BLESS concepts, we obtain
8 similarity scores, one per relation. In order to fac-
tor out concept-speciÔ¨Åc effects that might add to the
overall score variance (for example, a frequent con-
cept might have a denser neighborhood than a rarer
one, and consequently the nearest relatum scores of
the former are trivially higher than those of the lat-
ter), we transform the 8 similarity scores of each
concept onto standardized zscores (mean: 0; s.d: 1)
by subtracting from each their mean, and dividing by
their standard deviation. After this transformation,
we produce a boxplot summarizing the distribution
of scores per relation across the 200 concepts (i.e.,each box of the plot summarizes the distribution of
the 200 standardized scores picked for each rela-
tion). Our boxplots (see examples in Fig. 1 below)
display the median of a distribution as a thick hori-
zontal line within a box extending from the Ô¨Årst to
the third quartile, with whiskers covering 1.5 of the
interquartile range in each direction from the box,
and values outside this extended range ‚Äì extreme
outliers ‚Äì plotted as circles (these are the default
boxplotting option of the R statistical package).3
While the boxplots are extremely informative about
the relation types that are best captured by models,
we expect some degree of overlap among the distri-
butions of different relations, and in such cases we
might want to ask whether a certain model assigns
signiÔ¨Åcantly higher scores to one relation rather than
another (for example, to coordinates rather than ran-
dom nouns ). It is difÔ¨Åcult to decide a priori which
pairwise statistical comparisons will be interesting.
We thus take a conservative approach in which we
perform allpairwise comparisons using the Tukey
Honestly SigniÔ¨Åcant Difference test, that is simi-
lar to the standard ttest, but accounts for the greater
likelihood of Type I errors when multiple compar-
isons are performed (Abdi and Williams, 2010). We
only report the Tukey test results for those com-
parisons that are of interest in the analysis of the
boxplots, using the standard Œ±=0.05signiÔ¨Åcance
threshold.
5.1 Models
Occurrence and co-occurrence statistics for all mod-
els are extracted from the combined ukWaC and
Wackypedia corpora (see Section 4.4 above). We ex-
ploit the automated morphosyntactic annotation of
the corpora by building our DSMs out of lemmas
(instead of inÔ¨Çected words), and relying on part of
speech information.
Baselines. TheRelatumFrequency baseline uses
the frequency of occurrence of a relatum as a sur-
rogate of its cosine with the concept. With this ap-
proach, we want to verify that the unequal frequency
distribution across relations (see Table 1 above) is
not trivially sufÔ¨Åcient to differentiate relation classes
in a semantically interesting way. For our second
baseline, we assign a random number as cosine sur-
3http://www.r-project.org/ 8 rogate to each relatum (to smooth these random val-
ues, we generate them by Ô¨Årst sampling, for each
relatum, 10K random variates from a uniform distri-
bution, and then averaging them). If the set of relata
instantiating a certain relation is larger, it is more
likely that it will contain the highest random value.
Thus, this RelationCardinality baseline will favor
relations that tend to have large relata set across con-
cepts, controlling for effects due to different cardi-
nalities across semantic relations (again, see Table 1
above).
DSMs. We choose a few ways to construct DSMs
for illustrative purposes only. All the models contain
vector representations for the same words, namely,
approximately, the top 20K most frequent nouns, 5K
most frequent adjectives and 5K most frequent verbs
in the combined corpora. All the models use Local
Mutual Information (Evert, 2005; Baroni and Lenci,
2010) to weight raw co-occurrence counts (this asso-
ciation measure is obtained by multiplying the raw
count by Pointwise Mutual Information, and it is a
close approximation to the Log-Likelihood Ratio).
Three DSMs are based on counting co-occurrences
with collocates within a window of Ô¨Åxed width,
in the tradition of HAL (Lund and Burgess, 1996)
and many later models. The ContentWindow2
model records sentence-internal co-occurrence with
the nearest 2 content words to the left and right
of each target concept (the same 30K target nouns,
verbs and adjectives are also employed as context
content words). ContentWindow20 is like Con-
tentWindow2, but considers a larger window of 20
words to the left and right of the target. AllWin-
dow2 adopts the same window of ContentWindow2,
but considers all co-occurrences, not only those with
content words. The Document model, Ô¨Ånally, is
based on a (Local-Mutual-Information transformed)
word-by-document matrix, recording the distribu-
tion of the 30K target words across the documents in
the concatenated corpus. This DSM is thus akin to
traditional Latent Semantic Analysis (Landauer and
Dumais, 1997), without dimensionality reduction.
The content-window-based models have, by con-
struction, about 30K dimensions. The other models
are much larger, and for practical reasons we only
keep 1 million dimensions (those that account, cu-
mulatively, for the largest proportion of the overallLocal Mutual Information mass).
5.2 Results
The concept-by-concept z-normalized distributions
of cosines of relata instantiating each of our rela-
tions are presented, for each of the example mod-
els, in Fig. 1. The RelatumFrequency baseline
shows a preference for adjectives and verbs in gen-
eral, independently of whether they are meaningful
(attributes, events) or not (random adjectives and
verbs), reÔ¨Çecting the higher frequencies of adjec-
tives and verbs in BLESS (Table 1). The Relation-
Cardinality baseline produces even less interesting
results, with a strong preference for random nouns,
followed by coordinates, events and random verbs
(as predicted by the distribution in Table 1). We can
conclude that the semantically meaningful patterns
produced by the other models cannot be explained
by trivial differences in relatum frequency or rela-
tion cardinality in the BLESS data set.
Moving then to the real DSMs, ContentWindow2
essentially partitions the relations into 3 groups: co-
ordinates are the closest relata, which makes sense
since they are, taxonomically, the most similar en-
tities to target concepts. They are followed by (but
signiÔ¨Åcantly closer to the concept than) events, hy-
pernyms and meronyms (events and hypernyms sig-
niÔ¨Åcantly above meronyms). Next come the at-
tributes (signiÔ¨Åcantly lower cosines than all relation
types above). All the meaningful relata are signif-
icantly closer to the concepts than the random re-
lata. Similar patterns can be observed in the Con-
tentWindow20 distribution, however in this case the
events, while still signiÔ¨Åcantly below the coordi-
nates, are signiÔ¨Åcantly above the (statistically in-
distinguishable) hypernym, meronym and attribute
set. Again, all meaningful relata are above the ran-
dom ones. Both content-window-based models pro-
vide reasonable results, with ContentWindow2 be-
ing probably closer to our ‚Äúontological‚Äù intuitions.
The high ranking of events is probably explained
by the fact that a nominal concept will often ap-
pear as subject or object of verbs expressing asso-
ciated events ( dog barks ,Ô¨Åshing tuna ), and thus the
corresponding verbs will share even relatively nar-
row context windows with the concept noun. The
AllWindow2 distribution probably reÔ¨Çects the fact
that many contexts picked by this DSM are function 9 ‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
COORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012RelatumFrequency
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
COORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012RelationCardinality
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
COORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012ContentWindow2
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óèCOORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012ContentWindow20
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
COORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012AllWindow2
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè
COORDHYPERMEROATTRIEVENTRAN.NRAN.JRAN.V‚àí2‚àí1012Document</body>
  <conclusion>We introduced BLESS, the Ô¨Årst data set speciÔ¨Åcallydesigned for the intrinsic evaluation of DSMs. Thedata set contains tuples instantiating different, ex-plicitly typed semantic relations, plus a number ofcontrolled random tuples. Thus, BLESS can be usedto evaluate both the ability of DSMs to discriminatetruly related word pairs, and to perform in-depthanalyses of the types of semantic relata that differentmodels tend to favor among the nearest neighbors ofa target concept. Even a simple comparison of theperformance of a few DSMs on BLESS - like theone we have shown here - is able to highlight inter-esting differences in the semantic spaces producedby the various models. The success of BLESS willobviously depend on whether it will become a refer-ence model for the evaluation of DSMs, somethingthat can not be foreseena priori. Whatever its des-tiny, we believe that the BLESS approach can boostand innovate evaluation in distributional semantics,as a key condition to get at a deeper understandingof its potentialities as a viable model for meaning.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Herv Abdi and Lynne Williams. 2010. Newman-Keuls
and Tukey test. In N.J. Salkind, D.M. Dougherty, and
B. Frey, editors, Encyclopedia of Research Design .
Sage, Thousand Oaks, CA.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas√ßa, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and WordNet-based approaches. In Proceedings of
HLT-NAACL , pages 19‚Äì27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition . Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics ,
36(4):673‚Äì721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-
tors. 2008. Bridging the Gap between Semantic The-
ory and Computational Simulations: Proceedings of
the ESSLLI Workshop on Distributional Lexical Se-
mantic . FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222‚Äì254.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating wordnet-based measures of lexical semantic re-
latedness. Computational Linguistics , 32:13‚Äì47.
D. A. Cruse. 1986. Lexical Semantics . Cambridge Uni-
versity Press, Cambridge.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences . Dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database . MIT Press, Cambridge, MA.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The concept
revisited. ACM Transactions on Information Systems ,
20(1):116‚Äì131.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato‚Äôs problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review , 104(2):211‚Äì240.
Hugo Liu and Push Singh. 2004. ConceptNet: A prac-
tical commonsense reasoning toolkit. BT Technology
Journal , pages 211‚Äì226.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods , 28:203‚Äì208.
Ken McRae, George Cree, Mark Seidenberg, and Chris
McNorgan. 2005. Semantic feature production norms
for a large set of living and nonliving things. Behavior
Research Methods , 37(4):547‚Äì559.George Miller and Walter Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes , 6(1):1‚Äì28.
Gregory Murphy. 2002. The Big Book of Concepts . MIT
Press, Cambridge, MA.
Timothy Rogers and James McClelland. 2004. Seman-
tic Cognition: A Parallel Distributed Processing Ap-
proach . MIT Press, Cambridge, MA.
Herbert Rubenstein and John Goodenough. 1965. Con-
textual correlates of synonymy. Communications of
the ACM , 8(10):627‚Äì633.
Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-
ing Natural Language Processing Systems: An Analy-
sis and Review . Springer Verlag, Berlin.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of ArtiÔ¨Åcial Intelligence Research , 37:141‚Äì188.
Peter Turney. 2006. Similarity of semantic relations.
Computational Linguistics , 32(3):379‚Äì416.
Morton E. Winston, Roger ChafÔ¨Ån, and Douglas Her-
rmann. 1987. A taxonomy of part-whole relations.
Cognitive Science , 11:417‚Äì444.</biblio>


  <preamble>C14-1212.pdf</preamble>
  <titre>Learning to Distinguish Hypernyms and Co-Hyponyms</titre>
  <auteurs>
    <auteur>
      <name>Julie Weeds</name>
      <mail>juliewe@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Daoud Clarke</name>
      <mail>D.Clarke@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Jeremy Reffin</name>
      <mail>J.P.Reffin@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>David Weir</name>
      <mail>davidw@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Bill Keller</name>
      <mail>billk@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
  </auteurs>
  <abstract>This work is concerned with distinguishing different semantic relations which exist between
distributionally similar words. We compare a novel approach based on training a linear Support
Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional
similarity. We show that the new supervised approach does better even when there is minimal
information about the target words in the training data, giving a 15% reduction in error rate over
unsupervised approaches.</abstract>
  <introduction>
Over recent years there has been much interest in the Ô¨Åeld of distributional semantics, drawing on the
distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,
1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and
Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of ‚Äúnearest neighbours‚Äù)
automatically and applied them in a variety of applications, generally with a good deal of success.
In early research there was much interest in how these automatically generated thesauri compare with
human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).
More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-
tributional thesauri have been used in a wide variety of areas including sentiment classiÔ¨Åcation (Bollegala
et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-
dicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,
2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-
onomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).
A primary focus of distributional semantics has been on identifying words which are similar to each
other. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-
lations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix
of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related
words. A central problem here is that whilst most measures of distributional similarity are symmetric,
some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)
which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and
determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-
hyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is
symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the
words cat,animal anddog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms1.
1We read cat in the sense domestic cat rather than big cat , hence tiger is a co-hyponym rather than hyponym
ofcat.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2250 cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,
snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23
animal bird 0.36, Ô¨Åsh 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,
human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25
dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, Ô¨Åsh
0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23
Table 1: Top 15 neighbours of cat,animal anddog generated using Lin‚Äôs similarity measure (Lin,
1998) considering all words and dependency features occurring 100 or more times in Wikipedia.
Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can
be useful to be able to distinguish between these different relationships. Consider the following two
sentences.
The cat ran across the road. (1)
The animal ran across the road. (2)
Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability
to determine whether entailment holds between the sentences, and in which direction, depends on the
ability to identify hyponymy. Given a similarity score of 0.29 between cat andanimal , how do we
know which is the hyponym and which is the hypernym?
In applying distributional semantics to the problem of textual entailment, there is a need to generalise
lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations
is crucial if approaches to the composition of distributional representations of meaning that are currently
receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual
entailment problem.
We formulate the challenge as follows: Consider a set of pairs of similar words /angbracketleftA, B/angbracketrightwhere one of
three relationships hold between AandB:Alexically entails B,Blexically entails AorAandBare
related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section
2, we discuss existing attempts to address this problem through the use of various directional measures
of distributional similarity.
This paper considers the effectiveness of various supervised approaches, and makes the following
contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,
achieving a signiÔ¨Åcant reduction in error rate in comparison to existing state-of-the-art methods based
on the notion of distributional generality. Second, by comparing two different data sets, one built from
BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important
insights into the requirements of a valid evaluation of supervised approaches, and provide a data set
for further research in this area. Third, we show that when learning how to determine an ontological
relationship between a pair of similar words by means of the word‚Äôs distributional vectors, quite different
vector operations are useful when identifying different ontological relationships. In particular, using the
difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding
the vectors works well for the co-hyponym task.
</introduction>
  <body>Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the
alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She
found that this measure improved results in language modelling, when a word‚Äôs distribution is smoothed
using the distributions of its nearest neighbours.
Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words
tend to occur in a larger variety of contexts than more speciÔ¨Åc words. For example, we would expect to be
able to replace any occurrence of cat withanimal and so all of the contexts of cat must be plausible 2251 contexts for animal . However, not all of the contexts of animal would be plausible for cat, e.g.,
‚Äúthe monstrous animal barked at the intruder‚Äù. Weeds et al. (2004) attempt to capture this asymmetry
by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision
and recall are deÔ¨Åned as:
Pww(u, v) =Œ£f‚ààF(u)‚à©F(v)I(u, f)
Œ£f‚ààF(u)I(u, f)andRww(u, v) =Œ£f‚ààF(u)‚à©F(v)I(v, f)
Œ£f‚ààF(v)I(v, f)
where I(n, f)is the pointwise mutual information (PMI) between noun nand feature fand F(n) is the
set of all features ffor which I(n, f)&gt;0.
By comparing the precision and recall of one word‚Äôs retrieval of another word‚Äôs contexts, they were
able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.
However, this was not signiÔ¨Åcantly better than a baseline which proposed that the most frequent word
was the most general.
Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space.
He also argued for using a variation of co-occurrence retrieval where precision and recall are deÔ¨Åned as:
Pcl(u, v) =Œ£f‚ààF(u)‚à©F(v)min(I(u, f), I(v, f))
Œ£f‚ààF(u)I(u, f)andRcl(u, v) =Œ£f‚ààF(u)‚à©F(v)min(I(u, f), I(v, f))
Œ£f‚ààF(v)I(v, f)
Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should
have high recall and low precision, which would thus make it possible to distinguish them from other
related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure
to identify hypernyms:
invCL (u, v) =2/radicalBig
Pcl(u, v)‚àó(1‚àíRcl(u, v))
Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-
guishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).
Geffet and Dagan (2005) proposed an approach based on feature inclusion , which extends the rationale
of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-
relation between complete inclusion of prominent features and lexical entailment. However, they were
unable to assess this using an off-line corpus due to data sparseness.
Szpektor and Dagan (2008) found that the Pwwmeasure tends to promote relationships between infre-
quent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed
using the geometric average of Pwwand the symmetric similarity measure of Lin (1998) in order to
penalise low frequency words.
Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-
fying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-
ities for narrow feature vectors; their measure is called balAPinc . They show that all of the asymmetric
similarity measures previously proposed perform much better than symmetric similarity measures on
a directionality detection experiment, and that their method and that of Clarke (2009) outperform the
others with statistical signiÔ¨Åcance. They also show that their measure is superior when used for term
expansion in an event detection task.
Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate
that support vector machines can generalise entailment relations between quantiÔ¨Åer phrases to entailment
involving unseen quantiÔ¨Åers. They compare the performance of their system with the balAPinc measure.
The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large
corpora to Ô¨Ånd patterns that are indicative of hyponymy. For example, the pattern ‚Äú NPXand other NPY‚Äù
is an indication that NPXis aNPY, i.e. that NPXis a hyponym of NPY. They use machine learning
to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to Ô¨Ånd
new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over
valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow 2252 et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic
relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms
themselves. Our approach is therefore more generally applicable to systems which compose distribu-
tional representations of meaning.
Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.
In their experiments with smoothing edge scores for parser lexicalisation, they Ô¨Ånd that a directional
similarity measure, WeightedCosine2, performs best. Also of note, Mikolov et al. (2013) propose a vector
offset method to capture syntactic and semantic regularities between word representations learnt by a
recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms
and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.
Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which
is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative
than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS
dataset (Baroni and Lenci, 2011), this measure outperforms Pwwat both discriminating hypernym test
pairs from other types of relation and at determining the direction of the entailment relation.
3 Methodology
The code used to perform our experiments has been open sourced, and is available online.3
3.1 Vector Representations
Distributional information was collected for all of the nouns from Wikipedia provided they had oc-
curred 100or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted
text using wp2txt4. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt
Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, nnmod ) and also occurring 100or more times were ex-
tracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive
point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The
total number of noun vectors which can be harvested from Wikipedia with these parameters is 124,345.
Our goal is to build classiÔ¨Åers that establish whether or not a given semantic relation, rel, holds be-
tween two similar words AandB. Support vector machines (SVMs), which are effective across a variety
of classiÔ¨Åcation scenarios, learn a boundary between two classes from a set of positive and negative ex-
ample vectors. The two classes correspond to the relation relholding or not holding. Here, however, we
do not start with a single vector, but with two distributional vectors vAandvBfor the words AandB,
respectively. These vectors must be combined in some way to produce the SVM‚Äôs input, and a number
of ways were considered, deÔ¨Åned in Table 2. Of these operations, the vector difference (used by svm-
DIFF andknnDIFF ) and direct sum (used by svmCAT ) are asymmetric, whereas the sum and pointwise
multiplication (used by svmADD andsvmMULT ) are symmetric.
We now motivate the use of each of these operations. First, we note that pointwise multiplication
(svmMULT ) is intersective. Similar vectors will have a large intersection and it might be possible to
learn the features that nouns occurring in different semantic relations should share. However, it does
not retain any information about non-shared features and it is symmetric so it is difÔ¨Åcult to see how it
would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition ( svmADD )
effectively performs the union of the features, giving emphasis to the shared features. Whilst it does
retain information about the non-shared features, it is also symmetric, making it difÔ¨Åcult again to see
how it would be useful in determining the direction of an entailment relation
Vector difference (as used in svmDIFF andknnDIFF ), on the other hand, is asymmetric. Further,
we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.
Further, considering the majority sign of features in this difference vector might indicate the direction of
2The details of this measure are unpublished.
3https://github.com/SussexCompSem/learninghypernyms
4https://github.com/yohasebe/wp2txt 2253 entailment. Using an SVM, we might expect to be able to effectively learn which of these features should
be ignored and which should be combined, to decide the correct direction of entailment in the majority
number of cases in our training data. However, note that if one uses vector difference it is impossible to
distinguish between the case where a feature occurred with both nouns (to the same extent) and the case
where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both
nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct
sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from
the original vectors. Finally, we consider the use of the single vector corresponding to the second word
(svmSING ) as a baseline. High performance by this operation would indicate that we can learn features
of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the
putative relationship.
We also note that the behaviour of these methods may differ depending on the weighting used for vec-
tors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition
where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using
frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded
equal to zero), which is consistent with other work in this area, means that this correspondence is lost.
Because of the nature of our datasets, we were concerned that systems could learn information about
the taxonomy from the relations in the training data, without making use of information in the vectors
themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived
from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially
exceeds the average number ( 398) of non-zero features in the Wikipedia vectors.
3.2 ClassiÔ¨Åers
We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear
SVMs for speed and simplicity, since the point is to compare the different vector representations of
the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly
supervised classiÔ¨Åers. These are listed in Table 2. For the linear SVMs and kNN classiÔ¨Åer, we used the
scikit-learn implementations with default settings. For knearest neighbours, we performed a parameter
search, using nested cross-validation, varying kbetween 1 and 50.
For weakly supervised approaches, we evaluated the measure on the training set, then found the best
threshold pon the training set that best divides the two classes using that measure. When classifying, we
determine that the relation holds if the value of the measure exceeds p.
svmDIFF A linear SVM trained on the vector difference vB‚àívA
svmMULT A linear SVM trained on the pointwise product vector vB‚àóvA
svmADD A linear SVM trained on the vector sum vB+vA
svmCAT A linear SVM trained on the vector concatenation vB‚äïvA
svmSING A linear SVM trained on the vector vB
knnDIFF knearest neighbours (knn) trained on the vector difference vB‚àívA.1&lt; k &lt; 50
widthdiff width (B)&gt;width (A)‚Üírel(A, B)where width (A)is number of non-zero features in A
singlewidth width (B)&gt; p‚Üírel(A, B)
cosineP simcos(A, B)&gt; p‚Üírel(A, B)where simcos(A, B)is cosine similarity using PPMI
linP simlin(A, B)&gt; p‚Üírel(A, B)(Lin, 1998)
CRdiff Pww(A, B)&gt; R ww(A, B)‚Üírel(A, B)(Weeds et al., 2004)
clarkediff Pcl(A, B)&gt; R cl(A, B)‚Üírel(A, B)(Clarke, 2009)
invCLP invCL (A, B)&gt; p‚Üírel(A, B)(Lenci and Benotto, 2012)
balAPincP balAPinc (A, B)&gt; p‚Üírel(A, B)(Kotlerman et al., 2010)
most freq The most frequent label in the training data is assigned to every test point.
Table 2: Implemented classiÔ¨Åers 2254 3.3 Data Sets
One of key the challenges of this work has been to construct a data set which accurately and validly tests
our hypotheses. All four of our datasets detailed below are available online5.
In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the
supervised classiÔ¨Åers making use of artefacts of the data. This would not only make it unfair to compare
the supervised approaches with the unsupervised approaches, but also make it unlikely that our results
would be generalisable to other data. Here, we outline the requirements for the data sets, the importance
of which is demonstrated by our initial results for a data set which does not satisfy all of them.
There should be an equal number of positive and negative examples of a semantic relation. Thus,
random guessing or labelling with the most frequently seen label in the training data will yield 50%
accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in
terms of simple accuracy (or error rate).
It should not be possible to do well simply by considering the distributional similarity of the terms.
Hence, the negative examples need to be pairs of equally similar words, but where the relationship under
consideration does not hold.
It should not be possible to do well by pre-supposing an entailment relation and guessing the direction.
For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from
WordNet, over 70% of the time the more frequent word is also the entailed word.
It should not be possible to do well using ontological information learnt about one or both of the
words from the training data that is not generalisable to their distributional representations. For example,
it should not be possible for the classiÔ¨Åer simply to learn directly from the training pairs /angbracketleftcat ISA
mammal/angbracketrightand/angbracketleftmammal ISAanimal/angbracketrightthat/angbracketleftcat ISAanimal/angbracketright. Furthermore, we must ensure that
a classiÔ¨Åer cannot learn that a particular word is near the top of the ontological hierarchy, and, as a
result, do well by guessing that a particular pairing probably has an entailment relation. For example,
given many pairs such as /angbracketleftcat ISAanimal/angbracketright,/angbracketleftdog ISAanimal/angbracketright, a system which guessed /angbracketleftrabbit
ISAanimal/angbracketrightbut not/angbracketleftanimal ISArabbit/angbracketrightwould do better than random guessing. Whilst both
of these types of information could be useful in a hybrid system, they do not require any distributional
information and therefore we would not be learning anything about the distributional features of animal
which make it likely to be a hypernym.
3.3.1 BLESS
We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-
amples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200concrete,
largely monosemous nouns. We will refer to these 200nouns as the BLESS concepts.
hyponymBLESS is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms
were randomly selected to provide positive examples of entailment. The remaining hypernyms for the
given concept were reversed and taken with the same number of co-hyponyms, meronyms and random
words to form negative examples of entailment. A Ô¨Ålter was applied to ensure that duplicate pairs were
not included (e.g., if /angbracketleftcat,animal/angbracketrightis a positive pair then /angbracketleftanimal ,cat/angbracketrightcannot be a negative pair).
cohyponymBLESS is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms
were taken as positive examples of this relation. The same total number of (and split evenly between)
hypernyms, meronyms and random words was taken to form the negative examples. The order of 50%
of the pairs was reversed and again duplicate pairs were disallowed.
In both cases the pairs are labelled as positive or negative for the speciÔ¨Åed semantic relation and in
both cases there are equal ( ¬±1) numbers of positive and negative examples. For 99% of the generated
BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an
associated vector, the classiÔ¨Åers use a zero vector.
5https://github.com/SussexCompSem/learninghypernyms 2255 3.3.2 WordNet
We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS
data sets they more adequately satisfy the requirements laid out above6. We constructed a list of all non-
rare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to
have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet
package) and to have occurred in Wikipedia at least 100times. To be considered largely monosemous,
the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor
frequency information provided with WordNet. This led to a list of 7613 nouns.
hyponymWNis a set of 2564 labelled pairs of nouns constructed in the following way. Pairs /angbracketleftA, B/angbracketrightwere
found in the list of nouns where Bis an ancestor of A(i.e.,Alexically entails B). Each found pair is
added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already
been added and provided that each word has not previously been used in that position. Co-hyponym
pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found
pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and
provided that neither word has already been seen in that position in a pairing (either in the entailment
pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives
is selected. This provides a balanced data set where half of the pairs are positive examples of entailment
and the other half are semantically similar but not entailing.
cohyponymWNis a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponymWN
except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in
either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled
as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-
hyponyms and the other half, the negative examples, are entailment pairs (with direction unspeciÔ¨Åed)
In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance
between co-hyponym pairs is 2.
3.4 Experimental Setup
Most of our experiments were carried out using an implementation of Ô¨Åve-fold cross-validation using
each combination of data set, vector set and classiÔ¨Åer. In this setup, the pairs are randomly partitioned
into Ô¨Åve subsets, one subset is held out for testing whilst the classiÔ¨Åers are trained on the remaining four,
and this process is repeated using each subset as the test set.
In initial experiments with the BLESS datasets, the SVM classiÔ¨Åers were able to achieve classiÔ¨Åcation
accuracy of over 95% for hyponymBLESS and over 90% for cohyponymBLESS . However, the results us-
ing random vectors were not signiÔ¨Åcantly different from using the distributional vectors harvested from
Wikipedia. This indicated that the classiÔ¨Åers were learning ontological information implicit in the train-
ing data. In order to address this, when using the BLESS datasets, we removed any pair from the training
data if either word was present in the test data. In order to preserve a reasonable amount of training data,
we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all
datasets and classiÔ¨Åers, we found performance by the random vectors was no higher than 52%. This
indicates that the performance seen in Table 3 is due to learning from distributional features rather than
any ontological information implicit in the training set.
4 Results
In Table 3, we compare average accuracy for a number of different classiÔ¨Åers on each of two tasks,
distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.
Looking at the results for the hyponymBLESS data set, we can see that the SVM methods do generally
outperform the unsupervised methods. However, the best performing model is svmSING, suggesting
that, for this data set, it is best to try to learn the distributional features of more general terms, rather than
comparing the vector representations of the two terms under consideration.
6Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only
provided for 200nouns. 2256 dataset svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF
hyponymBLESS 0.74 0.56 0.66 0.68 0.75 0.54
cohyponymBLESS 0.62 0.39 0.41 0.40 0.40 0.58
hyponymWN 0.75 0.45 0.37 0.74 0.69 0.50
cohyponymWN 0.37 0.60 0.68 0.64 0.58 0.50
dataset most freq cosineP linP widthdiff singlewidth CRdiff invCLP balAPincP
hyponymBLESS 0.54 0.53 0.54 0.56 0.58 0.52 0.54 0.54
cohyponymBLESS 0.61 0.79 0.78 - - - - -
hyponymWN 0.50 0.53 0.52 0.70 0.65 0.70 0.66 0.53
cohyponymWN 0.50 0.50 0.55 - - - - -
Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors &lt;
0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also
omit results of clarkediff as these were consistently the same or less than CRdiff.
On the corresponding co-hyponym task, using the cohyponymBLESS data set, we see the best performing
classiÔ¨Åer is the cosine measure. The cosine measure is able to perform relatively well here because a
substantial proportion of the negative examples (25%) are random unrelated words which will have low
cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests
that measures such as the cosine measure ‚Äúprefer‚Äù words in symmetric semantic relationships such as co-
hyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of
the training data in this experimental set up with this data set. If, for example, our test concept is robin ,
our approach requires that we will not have any training pairs containing robin , or any training pairs
containing any of the words to which robin is related in the test set. In a dataset as small as BLESS,
this requirement effectively removes all knowledge of the distributional features of words in the target
domain. Hence, the need for a larger dataset as we have extracted from WordNet.
Looking at the results for the hyponymWNdata set, the directional SVM methods ( svmDIFF andsvm-
CAT) substantially outperform the symmetric SVM methods, and their performance is signiÔ¨Åcantly better
(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between
svmDIFF andknnDIFF . Both of these methods are trained on the differences of vectors. However, the
linear SVM outperforms kNN by 19‚Äì25%. This may suggest that the shape of the vector space inhabited
by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative
pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.
Looking at the results for the cohyponymWNdata set, it is clear that the unsupervised methods cannot
distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially
better, with the best performance achieved by svmADD andsvmCAT . Both of these methods essentially
retain information about all of the features of both words. svmMULT does much better than svmDIFF ,
which suggests that the shared features are more indicative than the non-shared features for this task.
The reasonably high performance of svmSING on both data sets suggests that words which have co-
hyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which
are included as entailed words in the data set. We hypothesise that there are speciÔ¨Åc features which more
general words tend to share (regardless of their topic) which makes it possible to identify more general
words from more speciÔ¨Åc words. This is completely consistent with very recent results using SLQS, a
new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical
contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,
with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns
typically have less informative contexts and are therefore likely to by hypernyms.
Given prior work, the performance of the balAPincP measure is lower than expected on the
hyponymWNdataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-
mining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is known that a relationship exists. It could be that the measure is particularly suited to the latter task.</body>
  <conclusion>We have shown that it is possible to predict to a large extent whether or not there is a speciÔ¨Åc semantic
relation between two words given their distributional vectors, using a supervised approach based on
linear SVMs. The increase in accuracy over unsupervised methods is signiÔ¨Åcant at the 0.01% level and
corresponds to a substantial absolute reduction in error rate (over 15%).
We have also shown that the choice of vector operation is signiÔ¨Åcant. Whilst concatenating the vectors,
and therefore retaining all of the information from both vectors including direction, generally performs
well, we have also shown that different vector operations are useful in establishing different relationships.
In particular, the vector difference operation, which loses information about the original vectors, achieved
performance indistinguishable from concatenation on the entailment task, where the classiÔ¨Åer is required
to distinguish hyponyms from other semantically related words including hypernyms. On the other
hand, the addition operation, which also loses information, outperformed concatenation by 4% (which
is statistically signiÔ¨Åcant at the 0.01% level) on the coordinate task, where the classiÔ¨Åer is required to
distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is
trying to establish between words determines the nature of the operation one should perform on their
associated vectors.
We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when
a data set has been constructed without ontological information, and when target words have not previ-
ously been seen in that position of a relationship in the training data. Hence, we believe the supervised
methods are learning characteristics of the underlying feature space which are generalisable to new words
(inhabiting the same feature space).
In future work, we intend to apply this approach to the problem of labelling the distributional neigh-
bours found for a given word with speciÔ¨Åc semantic relations. We also plan to investigate the use of
bag-of-words (windowed) vectors instead of grammatical relations for this task.
Finally, we believe that the data sets constructed from WordNet, which we publish alongside this
paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised
and unsupervised methods.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011 .
Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing .
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-
ation for Computational Linguistics , pages 23‚Äì32. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 1220‚Äì1229,
Uppsala, Sweden, July. Association for Computational Linguistics.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-
tionality of preÔ¨Åx verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing , pages 293‚Äì303, Cambridge, MA, October. Association for Computational Linguistics.
Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-
sitive thesaurus for cross-domain sentiment classiÔ¨Åcation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011) . 2258 Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.
InProceedings of the 27th Annual Meeting on Association for Computational Linguistics , ACL ‚Äô89, pages
76‚Äì83, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop of Geometric Models for Natural Language Semantics .
James Curran. 2004. From Distributional to Semantic Similarity . Ph.D. thesis, University of Edinburgh.
Georgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarity. In
Proceedings of the First Joint Conference on Lexical and Computational Semantics .
Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database . The MIT Press, Cambridge,
Massachusetts.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pages 466‚Äì476, Montr√©al, Canada, June.
Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-
ings of the 43rd meeting of the Association for Computational Liuguistics (ACL) , pages 107‚Äì114.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-
ference on Computational Semantics (IWCS 2011) , pages 125‚Äì134.
Zelig Harris. 1954. Distributional structure. Word , 10:146‚Äì162.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted
WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics , pages 1532‚Äì1541, Uppsala, Sweden, July.
Adam Kilgarriff and Colin Yallop. 2000. What‚Äôs in a thesaurus? In Proceedings of the 2nd International
Conference on Language Resources and Evaluation (LREC2000) .
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-
larity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics ,
4(16):359‚Äì389.
Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics , pages 25‚Äì32, College Park, Maryland, USA, June.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-
ings of the First Joint Conference on Lexical and Computational Semantics (*Sem) .
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International
Conference on Computational Linguistics (COLING 1998) .
Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing , pages 356‚Äì365, Cambridge, MA,
October. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies , pages 746‚Äì751, Atlanta, Georgia, June.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012 , pages
1781‚Äì1796, Mumbai, India, December.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:
HLT, pages 236‚Äì244, Columbus, Ohio, June. Association for Computational Linguistics.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on
Incremental Parsing , pages 50‚Äì57. 2259 Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies , pages 391‚Äì400, Atlanta, Georgia, June. Association for Computational Linguistics.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector
spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics , pages 38‚Äì42, Gothenburg, Sweden, April.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Processing Systems 17 .
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
InProceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics , pages 801‚Äì808. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning , pages 1201‚Äì1211.
Gy¬®orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using
delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pages 1131‚Äì1141, Atlanta, Georgia, June.
Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Linguistics (Coling 2008) , pages 849‚Äì856, Manchester, UK,
August.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing ,
pages 81‚Äì88.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
InProceedings of Coling 2004 , pages 1015‚Äì1021, Geneva, Switzerland, Aug 23‚ÄìAug 27. COLING.
Julie Weeds, David Weir, and Jeremy RefÔ¨Ån. 2014. Distributional composition using higher-order dependency
vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,
EACL 2014 , Gothenburg, Sweden, April.
Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second
Symposium on Quantum Interaction, Oxford, UK , pages 1‚Äì8.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning , pages 1212‚Äì1222, Jeju Island, Korea, July. Association for Computational Linguistics.
Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings
of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 756‚Äì766, Cambridge,
MA, October. Association for Computational Linguistics.</biblio>


  <preamble>Guy.pdf</preamble>
  <titre>Skill2vec: Machine Learning Approach for Determining the Relevant
Skills from Job Description</titre>
  <auteurs>
    <auteur>
      <name>Le Van-Duyet</name>
      <mail>me@duyet.net</mail>
      <affiliation>N/A</affiliation>
    </auteur>
    <auteur>
      <name>Vo Minh Quan</name>
      <mail>95.vominhquan@gmail.com</mail>
      <affiliation>N/A</affiliation>
    </auteur>
    <auteur>
      <name>Dang Quang An</name>
      <mail>an.dang1390@gmail.com</mail>
      <affiliation>N/A</affiliation>
    </auteur>
  </auteurs>
  <abstract> Unsupervise learned word embeddings have seen
tremendous success in numerous Natural Language Processing
(NLP) tasks in recent years. The main contribution of this
paper is to develop a technique called Skill2vec, which applies
machine learning techniques in recruitment to enhance the
search strategy to Ô¨Ånd candidates possessing the appropriate
skills. Skill2vec is a neural network architecture inspired by
Word2vec, developed by Mikolov et al. in 2013. It transforms
skills to new vector space, which has the characteristics of
calculation and presents skills relationships. We conducted an
experiment evaluation manually by a recruitment company‚Äôs
domain experts to demonstrate the effectiveness of our ap-
proach.</abstract>
  <introduction>
Recruiters in information technology domain have met the
problem Ô¨Ånding appropriate candidates by their skills. In the
resume, the candidate may describe one skill in different
ways or skills could be replaced by others. The recruiters
may not have the domain knowledge to know if one‚Äôs skills
are Ô¨Åt or not, so they can only Ô¨Ånd ones with matched skills.
In order to cope with the problem, one should try to
Ô¨Ånd the relatedness of skills. There are some approaches:
building a dictionary manually, ontology approach, natural
language processing methods, etc. In this study, we apply a
word embedding method Word2Vec, using skills from online
job post descriptions. We treat skills as terms, job posts as
documents and Ô¨Ånd the relatedness of these skills.
</introduction>
  <body>To Ô¨Ånd relatedness of skills, Simon Hughes [4] from Dice
proposed an approach using Latent Semantic Analysis with
an assumption that skills are related to skills which occur
in the same context, and here contexts are job posts. This
approach will build a term-document matrix and use Singular
Value Decomposition to reduce the dimensionality. The cons
of this approach is that when we have new data coming, we
can not update the old term-document matrix, this leads to
difÔ¨Åculties in maintaining the model, as the change of trend
in this domain is high.
Google‚Äôs Data Scientists also face the same problems
in Cloud Jobs API [7]. Their solution is to build a skill
ontology deÔ¨Åning around 50,000 skills in 30 job domain with
relationships such as is a, related to, etc. This approach can
represent complicate relationships between skills and jobs,
but building such an ontology costs so much time and effort.
To overcome the problem of relevant term, [3] present a
new, effective log-based approach to relevant term extraction
and term suggestion.The goal of [2] is to develop an automated system that
discovers additional names for an entity given just one of
its names, using Latent semantic analysis (LSA) [1]. In the
example of authors, the city in India referred to as Bombay
in some documents may be referred to as Mumbai in others
because its name ofÔ¨Åcially changed from the former to the
latter in 1995.
[5] is the introduction of an ontology-based skills man-
agement system at Swiss Life and the lessons learned from
the utilisation of the methodology, present a methodology
for application-driven development of ontologies that is
instantiated by a case study.
III. WORD2VEC ARCHITECTURE
Word2Vec is a group of models proposed by Mikolov et al
in 2013 [6]. It consists of 2 models: continuous bag-of-words
and continuous Skip-gram, both are shallow neural networks
that try to learn distributed representations of words with
the target is to maximize the accuracy while minimizing the
computational complexity. In the continuous bag-of-words
architecture, the model predicts the current word from a
window of surrounding context words. On the other hand,
Skip-gram model try to predict surrounding context words
based on the current word. In this work, we focus on Skip-
gram model as it is known to be better with infrequent words
and it also give slightly better result in our experiment.
The model consists of three layers: input layer, one hidden
layer and output layer. The input layer take a word encoded
using 1-of-V encoding (also known as one-hot encoding),
where V is size of the dictionary. The word is then fed
through the linear hidden layer to the output layer, which
is a softmax classiÔ¨Åer. The output layer will try to predict
words within window size before and after current word.
Using stochastic gradient descent and back propagation, we
train the model until it converges.
This model takes vector dimensionality and window size
as parameters. The author found that increasing the window
size improves the quality of the word vector, and yet it
increases the computational complexity.
IV. M ETHODOLOGY
A. Data collecting and processing
Choosing a universal data set for the model is extremely
important, the data should be large enough and should have
balanced distributions over words (i.e. skills).arXiv:1707.09751v3  [cs.CL]  9 Oct 2019There are two dataset we need to concern, one (1) is the
standard skills dictionary for the parser and another (2) is
skills for training model; follow the Ô¨Ågure 1.
Career
websites(1) Standard skillscollect
cleaning
Career
websitesJob descriptionscollect(*) parser
(2) Training
data
Fig. 1. Pipeline of data collecting and processing
First, we collected and prepared a large dictionary of
skills. With this dictionary, we can extract a set of skills
from raw job descriptions. Skillss need to be cleaned into
unique skills because there are many way to present a skill in
job description ( i.e. OOP or Object-oriented programming ).
Figure 2 brieÔ¨Çy depicts the concept of the cleaning process.
Raw
skill
(1) Remove
punctuation
(2) Dictionary
replace
(3) Regular
expression
(4) Lemmatization,
Stemming
Cleaned
skill
Fig. 2. Pipeline cleaning skills
After that, we had the dictionary of skills ready for
parsing. We collected a huge number of job descriptions
from Dice.com - one of the most popular career website
about Tech jobs in USA. From these job descriptions, we
extract skills for each one by using our skills dictionary
(1). Now, the dataset is presented by a list of collectionsof skills based on job descriptions. After crawling, we got
a total of 5GB with more than 1,400,000 job descriptions.
From these data, we extracted skills and performed as a
list of skills in the same context, the context here includes
skills in the same job description. The dataset is published
at https://github.com/duyetdev/skill2vec-dataset
The data structure is shown in table I.
TABLE I
DATA STRUCTURE
Job description Context skills
JD1 Java, Spark, Hadoop, Python
JD2 Python, Hive
JD3 Python, Flask, SQL
     
B. Skill2vec architecture
In this paper, for training the dataset, we used a neural
network inspired by Word2Vec model as mentioned above.
Here we treated our skills as words in Word2Vec model.
In this study, with the documents contain only the skills,
we chose the maximum window size, implied that every
skills in the same job description are related to each other.
For the vector dimensions, after some point, adding more
dimensions provides diminishing improvements, so we chose
this parameter empirically. To honour the work of Word2Vec
model as it holds a big part in our study, we name our model
Skill2Vec. Figure 3 brieÔ¨Çy describes our Skill2Vec model.
Fig. 3. Skill2vec architecture V. EXPERIMENTAL SETUP
To evaluate our method, we have an expert team assesses
the result following these steps:
1) Pick 200 skills randomly from our dictionary.
2) Our system will return top 5 ‚Äùnearest‚Äù skills for each.
3) The expert team will check if these top 5 ‚Äùnearest‚Äù
skills are relevant or not.
The experiment showed that 78% of skills returned by
our model is truly relevant to the input skill. We present the
experimental results in table II
TABLE II
QUERY TOP 5RELEVANT SKILLS
Query skill Top relevant skills
HTML5css3
bootstrap
front end
angular
responsive
OOPOOD
Objective
Java
Multithread
Software Debug
HadoopPig
Hive
HBase
Big Data
Spark
ScalaZookeeper
Spark
Data System
Sqoop
solrcloud
HivePig
HDFS
Hadoop
Spark
Impala</body>
  <conclusion>In this paper, we developed a relationship network between
skills in recruitment domain by using the neural net inspired
by Word2vec model. We observed that it is possible to
train high quality word vectors using very simple model
architectures due to lower cost of computation. Moreover,
it is possible to compute very accurate high dimensional
word vectors from a much larger dataset. Using Skip-gram
architecture and an advanced technique for preprocessing
data, the result seems to be impressive. The result of our
work can contribute to building the matching system betweencandidates and job post. In the other hand, candidates can
Ô¨Ånd the gap between the job post requirements and their
ability, so they can Ô¨Ånd the suitable trainings.
A direction we can follow in the future: adding domain
in training model, for example: Between Python ,Java, and
R, inData Science domain, Python andRare more relevant
than Java, however in Back End domain, Python andJava
are more relevant than R.</conclusion>
  <discussion>N/A</discussion>
  <biblio>[1] Michael W Berry and Ricardo D Fierro. ‚ÄúLow-rank
orthogonal decompositions for information retrieval ap-
plications‚Äù. In: Numerical linear algebra with applica-
tions 3.4 (1996), pp. 301‚Äì327.
[2] Vinay Bhat et al. ‚ÄúFinding aliases on the web using
latent semantic analysis‚Äù. In: Data &amp; Knowledge Engi-
neering 49.2 (2004), pp. 129‚Äì143.
[3] Chien-Kang Huang, Lee-Feng Chien, and Yen-Jen
Oyang. ‚ÄúRelevant term suggestion in interactive web
search based on contextual information in query session
logs‚Äù. In: Journal of the Association for Information
Science and Technology 54.7 (2003), pp. 638‚Äì649.
[4] Simon Hughes. How We Data-Mine Related Tech Skills .
2015. URL:http : / / insights . dice . com /
2015/03/16/how-we-data-mine-related-
tech-skills/ (visited on 09/12/2017).
[5] Thorsten Lau and York Sure. ‚ÄúIntroducing ontology-
based skills management at a large insurance com-
pany‚Äù. In: Proceedings of the Modellierung . 2002,
pp. 123‚Äì134.
[6] Tomas Mikolov et al. ‚ÄúEfÔ¨Åcient Estimation of
Word Representations in Vector Space‚Äù. In: CoRR
abs/1301.3781 (2013). URL:http://dblp.uni-
trier.de/db/journals/corr/corr1301.
html#abs-1301-3781 .
[7] Christian Posse. Cloud Jobs API: machine learning
goes to work on job search and discovery . 2016. URL:
https : / / cloud . google . com / blog / big -
data/2016/11/cloud-jobs-api-machine-
learning-goes-to-work-on-job-search-
and-discovery (visited on 03/03/2018)</biblio>


  <preamble>infoEmbeddings.pdf</preamble>
  <titre>An Empirical Evaluation of Text Representation Schemes on
Multilingual Social Web to Filter the Textual Aggression</titre>
  <auteurs>
    <auteur>
      <name>Sandip Modha</name>
      <mail>sjmodha@gmail.com</mail>
      <affiliation>DA-IICT Gandhinagar,India</affiliation>
    </auteur>
    <auteur>
      <name>Prasenjit Majumdera</name>
      <mail>prasenjit.majumder@gmail.comar</mail>
      <affiliation>DA-IICT Gandhinagar,India</affiliation>
    </auteur>
  </auteurs>
  <abstract>Due to an exponential rise in the social media user base, incidents like hate speech,
trolling, cyberbullying are also increasing and that lead hate speech detection prob-
lem reshaped into different tasks like Aggression detection, Fact detection. This
paper attempt to study the effectiveness of text representation schemes on two
tasks namely: User Aggression and Fact Detection from the social media contents.
In User Aggression detection, The aim is to identify the level of aggression from
the contents generated in the Social media and written in the English, Devanagari
Hindi and Romanized Hindi. Aggression levels are categorized into three predefined
classes namely: `Non-aggressive`, √ívertly Aggressive`, and `Covertly Aggressive`.
During the disaster-related incident, Social media like, Twitter is
ooded with mil-
lions of posts. In such emergency situations, identification of factual posts is impor-
tant for organizations involved in the relief operation. We anticipated this problem
as a combination of classification and Ranking problem. This paper presents a com-
parison of various text representation scheme based on BoW techniques, distributed
word/sentence representation, transfer learning on classifiers. Weighted F1score is
used as a primary evaluation metric. Results show that text representation using
BoW performs better than word embedding on machine learning classifiers. While
pre-trained Word embedding techniques perform better on classifiers based on deep
neural net. Recent transfer learning model like ELMO, ULMFiT are fine-tuned
for the Aggression classification task. However, results are not at par with pre-
trained word embedding model. Overall, word embedding using fastText produce
best weighted F1-score than Word2Vec and Glove. Results are further improved
using pre-trained vector model. Statistical significance tests are employed to en-
sure the significance of the classification results. In the case of lexically different
test Dataset, other than training Dataset, deep neural models are more robust and
perform substantially better than machine learning classifier.</abstract>
  <introduction>
The Social Web is a great source for studying human interaction and behavior. In
the last few years, there is an exponential growth in Social Media user base. Sensing
content of Social Media like Facebook, Twitter, by the smart autonomous application
empower its user community with real-time information which is unfolded across the
different part of the world. Social media provide the easiest and anonymous platform
for common people to voice their opinion or view on a various entity like celebrity,
politician, product, stock market etc or any social movement. Sometime such opinions
might be aggressive in nature and propagate hate in the social media community.
With the unprecedented increase in the user base of the social media and its avail-
ability on the Smartphones, incidents like Hate speech, trolling, Cyberbullying, and
Aggressive posts are increasing exponentially. A smart autonomous system is required
which enable surveillance on the social media platform and detect such incidents.
Some of the researchers look posts from the aspect like aggression Kumar Ritesh et
al. (2018) to filter the contents. some of the posts contain words which might be qual-
ified as either highly or overly aggressive or have hidden aggression. Sometimes posts
do not have any aggression. Based on these, posts or comments are categorized into
three classes namely: √ívertly Aggressive`, `Covertly Aggressive√†nd `Non-aggressive`
Kumar Ritesh et al. (2018). Henceforth, in the rest of the paper, we will denote these
classes by these abbreviations namely: OAG, CAG, NAG respectively.Table 1 shows
the sample posts belonging to these classes.
Social Media, specifically Microblog has proved its importance during the disaster-
related incidents like an earthquake, Hurricane and
oods1. Organizations involved
in relief operation actively track posts related to situational information posted on
Facebook and Twitter during the disaster. However, At the same time, social media
is
ooded with lots of prayer and condolence messages. Posts which contain factual
information are extremely important for the organization involved in post-disaster
relief operations for coordination. Filtering and Ranking of the posts containing factual
information will be very useful to them. We believe that this is the special problem
of the Sentiment Analysis task. We consider this problem as a combination of two-
class classification problem: factual posts and nob-factual posts plus Ranking. Table
2 shows the example of the posts of belong to these class.
The Text representation of social web content plays a pivotal role in any NLP task.
Bag-of-word is the oldest and simple technique to represent the document or post into
a fixed length vector. The BoW techniques generate very sparse and high dimensional
space vector. Text representation using distributed word/sentence representation or
word embedding is gain rapid momentum recently. In this paper, one of the objectives
1https://phys.org/news/2018-08-social-media-bad-disaster-zones.html
2Table 2. sample post for the each class.
Post text Class Label
1 #Nepal #Earthquake day four. Slowly in the capital valley
Internet and electricity beeing restored . A relief for at least
some onesFactual
2 PMOIndia Indian Government is doing every possible help to
the earthquake victims and they need money so plz contributeNon-factual
is to find the best text representation scheme to model social web content for the
machine learning classifier and deep neural net. Various Text representation scheme
based on BoW, word embedding and are studied empirically. We have reported result
on popular word embedding technique like Word2vec, Glove and fastText on stan-
dard machine learning classifier like Multinomial Naive Bayes (MNB), Logistic Re-
gression(LR), K-Nearest neighbors KNN, Support Vector Classifier (SVC), Decision
Tree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost,
Perceptron, Deep neural net based on LSTM, CNN and Bidirectional LSTM. Results
are also reported on Doc2vec embedding, a popular sentence or paragraph embedding
technique for the above classifiers.
Transfer Learning is well practiced in the area of computer vision. However, in the
NLP, transfer learning has limited application in the form of pre-trained word vector
which is used to initialize the weights of the embedding layer of the deep neural net-
work. With the advent of transfer learning method like ELMO (Peters et al. , 2018),
ULMFiT (Howaard et al., 2018) claimed substantial improvement in the performance
of various NLP tasks like Sentiment Analysis, Question/Answering, Textual Entail-
ment empirically. The main idea behind these methods is to train language model
on the large corpus and fine tune on the task-specific corpus. In this paper, We have
evaluated the performance of these methods in the Aggression classification tasks.
1.1. Research Questions
In this study, experiments are performed on the benchmark dataset with to answer
the following questions
Which is the best Text Representation scheme to model text from the Social
Web?
Does pre-trained language model based on transfer learning better than pre-
trained word embedding based on shallow transfer learning on Social media
data?
Does Making too Deep Neural net make sense?
To answer all research question listed above, experiments are performed on two
tasks namely: Aggression detection (Trolling Aggression and Cyberbullying (TRAC)
dataset) Kumar et al. (2018) and Fact detection (FIRE iRMDI Dataset)Basu et al.
(2018). In this paper, we present exhaustive benchmarking of text representation
schemes on these datasets. Our results reveal that fastText with pre-trained vector
along with CNN outperform standard machine learning classifiers based on BoW
Model and marginally perform better than Word2vec and Glove. Paragraph vector
or Doc2vec Le, Quoc et al. (2014) perform very poor on our dataset and turn out to
be the worst text representation scheme among all. We also found that model based
on the deep neural net is more robust than machine learning classifier when tested
on lexically different dataset than training Dataset. i.e. deep neural model substan-
3tially outperforms machine learning classifier on Twitter test Dataset while trained on
Facebook Dataset in this evaluation.
To validate our claims, statistical significance tests are performed on weighted F1-
score of the classifier for each text representing scheme. Statistical inference is used
to check evidence to support or reject these claims. Significance tests like Wilcoxon
signed-rank and Student t-test were carried out by comparing weighted F1score all
the text representation scheme with the fastText pre-trained vector. In most of the
cases, p-values are less than 0.05.
The rest of the paper is organized as follows: In section 2, we review the relevant
works in the area of Sentiment analysis and hate speech detection. Section 3 contains
the detail information about the various benchmark Datasets used in the experiments.
Various Text Representation schemes are described in section 4. We formally describe
the evaluation task and models in section 5. We report results in section 6 and present
detail result analysis in section 7. We conclude the discussion and provide insight for
the future work in section 8.
</introduction>
  <body>Bag-of-Words (BoW) (Harris et al. , 1954) is the oldest technique to represent the
text of the documents in fixed-length vectors with high dimensionality. Mikolov et.al
(2013) proposed two architecture namely: skip-gram(SG) and continuous-bag-of word
(CBOW) to learn high quality low dimensional word embedding. However, to gener-
ate sentence vector often, average or mean of word vector are considered. Doc2vec or
paragraph vector Le, Quoc et al. (2014) proposed Paragraph2vec (Doc2vec) which is
the extension of the Word2vec to learning document level embedding. It is an unsuper-
vised method which learns document vector from paragraph, sentence or document.
Pennington et.al. (2014) proposed word embedding based on the co-occurrence ma-
trix. Lau et al. (2016) have performed a comprehensive evaluation of Doc2Vec on two
tasks namely: Forum Question Duplication and Semantic Textual Similarity (STS)
task. Authors claimed that Doc2Vec performs better than Word2vec provided that
models trained on large external corpora, and can be further improved by using pre-
trained word embedding. They have published the hyper-parameter for the Doc2Vec
embedding. Our work is similar to this but we have reported the evaluation of all the
text representation scheme including doc2vec on TRAC dataset(Kumar et al. , 2018)
on each classifier.
Hate speech is a type of language which is used to incite or spread violence to-
wards the group of people based on the gender, community, race, religion. Sentiment
analysis and hate-speech are closely related in fact sentiment analysis techniques are
used in hate speech detection. Initially, Sentiment Analysis problem is formulated as
a binary classification problem for predicting the election results or detecting political
opinion (Conover et al. , 2011; Conover Micheal et al. , 2011; Maynard et al., 2011;
Tumasjan et al. , 2010) on Twitter. Then after, It turned into the multi-class classi-
fication problem with the introduction of the neutral label. Soon, Researchers come
with different notion like aggression (Kumar Ritesh et al., 2018), cyberbullying(xu et
al. , 2012), sarcasm, trolling. Semeval (International workshop on semantic evalua-
tion)(Rosenthal et al. , 2017) is one of the popular competition on sentiment analysis
which is started since 2013. TRAC2(Trolling, aggression, cyberbullying) workshop
2https://sites.google.com/view/trac1
4(Kumar Ritesh et al., 2018) co-located with the International Conference of Com-
putational Linguistics (COLING 2018) redefine hate speech detection task in terms
of three type of aggression namely: Non-Aggression (NAG), Overly-Aggression(OAG)
and Covertly Aggression (CAG).
2.1. Sentiment Analysis
During the initial year, there is a lack of standard dataset for comparative perfor-
mance analysis. International Workshop on Semantic Evaluation 2013 (SemEval-2013)
(Hltcoe et.al , 2013) was the first forum who developed standard tweet dataset for the
benchmarking of the various sentiment analysis system. Most of the team who had par-
ticipated in the competition used supervised approaches based on SVM, Naive Bayes,
and Maximum Entropy. some of the team had used ensemble classifier and rule-based
classifier. Mohammad et al. (2013) was the top team of the Semeval-2013 challenge.
They have incorporated various semantic and lexicon based sentiment features for the
experiment and SVM was used for the classification. Deep learning and word embed-
ding had shown its footprints in SemEval-2015 (Rosenthal et al. , 2015). Team UNITN
(Severyn et al. , 2015) was the second team in the message polarity task. They have
build convolution neural network for the sentiment classification. They have used an
unsupervised neural language model to initialize word embeddings that are further
tuned by deep learning model on a distant supervised corpus (Severyn et al. , 2015).
In fourth edition SemEval-2016 (Nakov et al. , 2016),Team SwissCheese (Deriu et al.
, 2016) was the first ranked team with F1score around 63.3 %. Their approach was
based on 2-layer convolution neural networks whose predictions are combined using a
random forest classifier. SemEval-2017 (Rosenthal et al. , 2017) was the fifth edition,
Team DataStories (Baziotis et al. , 2017) was the top-ranked team with AvgRec= 68.1
andF1around=67.7 %. They use Long Short-Term Memory (LSTM) networks aug-
mented with two kinds of attention mechanisms, on top of word embedding pre-trained
on a big collection of Twitter messages without using any hand-crafted features.
2.2. Hate Speech/Cyberbullying/Aggression Detection
Hate Speech Detection research attracts researchers from the diverse background
like Computational linguistic, computer science, social science. The actual term hate
speech was coined by Warner et al. (2012). Various Authors used different notion
like offensive language (Razavi et.al , 2010), Cyberbullying (xu et al. , 2012), Aggres-
sion (Kumar et al. , 2018). Davidson et al. (2017) studied tweet classification of hate
speech and offensive language and defined hate speech as following: language that is
used to expresses hatred towards a targeted group or is intended to be derogatory,
to humiliate, or to insult the members of the group. Authors observed that offensive
language often miss-classified as hate speech. They have trained a multi-class classi-
fier on N-gram features weighted by its TF-IDF weights and PoS tags. In addition
to these, features like sentiment score of each tweet, no of hashtags, URLS, mentions
are considered. Authors concluded that Logistic regression and Linear SVM perform
better than NB, Decision Tree, Random Forests. Schmidt et al. (2017) perform com-
prehensive survey on hate speech. They have identified features like Surface features,
sentiment, word generalization,lexical, linguistics etc. can be used by classifier.
Cyberbullying is the type bullying that occurs on social media platform or app via
cellphone or any internet enabled device. xu et al. (2012) introduces Cyberbullying to
5the NLP community. They have performed various binary classification on tweets text
with bullying perspective to determine whether the user is cyberbully or not. They
reported binary classification accuracy around 81%. Kwok et al. (2013(@), authors
performed classification using NB classifier on tweets based on two classes :racist and
non-racists and achieved accuracy around 76 %. Burnap et al. (2015), authors studied
cyber hate on Twitter. They have used various classifier like SVM, BLR, RFDT, Voting
base ensemble for the binary classification achieved best F1-score of 0.77 in the voted
ensemble.Malmasi, et al. (2017), authors have used NLP based lexical approach to
address the multi-class classification problem. They have used character N-gram, word
N-gram and word skip-gram feature for the classification.
Schmidt et al. (2017), have described the key areas that have been explored to detect
hate speech. They have surveyed different types of features used for hate speech classi-
fication. They have categorized features in Simple surface features, word generalization
features, sentiment features, linguistic features, lexical resources features, Knowledge-
based features, and Meta-Information features Simple surface features include features
like character level unigram/n-gram, word generalization features include features like
the bag-of-words, clustering, word embedding, paragraph embedding. Linguistic fea-
tures include PoS tag of tokens. list of bad words or hate words can be considered as
a lexical resource. Malmasi et al. (2018), tried to address the problem of discrimi-
nating profanity from the hate speech in the social media posts. n-grams, skip-gram
and clustering based word representation features are considered for the 3-class classi-
fication.The Author use SVM and advance ensemble based classifier for this task and
achieved 80 % accuracy.
Aroyehun et al. (2018) performed translation as data augmentation strategy. TRAC
Dataset (Kumar et al. , 2018) was also augmented using translation and pseudo labeled
using an external dataset on hate speech. they have reported best performance with
LSTM and F1 score around 0.6415 on TRAC English dataset (Kumar et al. , 2018).
Arroyo et al. (2018) implement ensemble of the Passive-Aggressive (PA) and SVM
classifiers with character n-grams. TF-IDF weighting used for feature representation.
FIRE initiative also gave importance text representation in Indian language since its
inception. (Majumder et al. , 2008)(Majumder et al. , 2007).
3. Dataset
Experiments are performed on standard benchmarked Datasets to evaluate the perfor-
mance of various text representation scheme. For User Aggression detection problem,
Trolling, Aggression and Cyberbullying TRAC (Kumar et al. , 2018) is considered for
the experiments which contain post in English and code-mixed Hindi. For the Factual
Detection task, experiments are performed on FIRE IRMiDis Dataset.
3.1. TRAC Dataset
TRAC (Trolling, Aggresion and Cyberbullying) consist of 15,001 aggression-annotated
Facebook Posts and Comments each in Hindi (Romanized and Devanagari script) and
English for training and validation Kumar et al. (2018).
6Table 3. Class distribution in the Training Dataset
English Corpus Hindi Corpus
# Training # Validation # Training # Validation
NAG 5 ;052 1 ;233 2 ;275 538
CAG 4240 1 ;057 4 ;869 1 ;246
OAG 2 ;708 711 4 ;856 1217
Total 12 ;000 3 ;001 12 ;000 3 ;001
Table 4. Test Data Corpus statistics
Test Dataset # of posts
Facebook English Corpus 916
Twitter English Corpus 1 ;257
Facebook mixed script Hindi Corpus 970
Twitter mixed script Hindi Corpus 1 ;194
3.2. FIRE IRMiDis Dataset
Forum for Information Retrieval Evaluation, have introduced Microblog track since
2016 as Information Retrieval from Microblogs during Disasters (IRMiDis). IRMiDis
track (Basu et al. , 2018) of FIRE is organized with the objective to extract factual
or fact-checkable tweets during the disaster which might be helpful to the victims
or the people who are involved in the relief operation. Dataset contain tweets which
are downloaded from the Twitter during Nepal earthquake 2015. Following are the
example of factual or fact-checkable and non-fact-checkable tweet.Table 5 shows a
detail statistics of FIRE IRMiDis Dataset. As we look at the table, There are only
83 tweets is annotated with objective class. not a single tweet is annotated from the
subjective class.
4. Text Representation Schemes
The main objective of this paper is an identification of the best text representation
scheme for the Social media text which is very sparse and noisy in nature. Text rep-
resentation is about representing documents in a numerical way so that they can be
feed as an input to the classifier. This numerical representation is in the form of the
vectors which together form matrices. Essentially, There are two types of text repre-
sentation scheme :(i) Bag-of-words(BoW) (ii) Distributed Word/sentence representa-
tion. BoW with count vector and TF/IDF weighting , various word embedding tech-
niques(Word2Vec, Glove, fastText), and sentence or paragraph embedding (Doc2Vec)
are studied.
Table 5. FIRE IRMiDis Dataset statistics
Particulars # tweets Remark
Number of Tweets 50000+
Labelled Tweets 83 only tweets belong to Factual class
Classes 2
74.1. Bag-of-Word Model for Text Representation
The Bag-of-words is the simple technique to represent the document or social media
posts in the vector form and also a very common feature extraction method from the
text. Word count or TF/IDF weight of each n-gram word can be used as a features.
The dimension of the vector is equal to the size of vocabulary of the text corpus
or dataset which results in very high dimensional sparse document vector. It is the
common method used for the text representation in order to perform various NLP task
like text classification, clustering. However,the BoW methods ignore the word order
which may lead to loss of the context.
4.2. Word Embedding for Text representation
Word Embedding is the text representation technique to represent the word in the low
dimensional space so that semantically similar word have similar representation. Major
word embedding techniques like Word2vec learn word embedding using shallow neural
network. The fastText, extension of Word2vec, consider the morphological structure
of the word.
4.2.1. Word2Vec
Word2vec (Mikolov et.al , 2013) is the unsupervised and predictive neural word em-
bedding technique to learn the word representation in the low dimensional space.
Word2vec is a two-layer neural net that take text corpus as an input and output is a
set of vectors. two novel model architectures: Skipgram and CBOW(Continuous bag
of words) are proposed for computing continuous vector representations of words from
very large data sets.
4.2.2. Glove
GloVe stands for Global vector for [Word Representation] (Pennington et.al. , 2014)is
an unsupervised method for learning word embedding. A Co-occurrence word matrix
is created from the text corpus for the training and is reduced in low dimensional space
which explain the variance of high dimensional data and provide word vector for each
word.
4.2.3. fastText
fastText (Bojanowski et al. , 2017) is the neural word embedding technique which
learn distributed low dimensional word embedding. Word2vec, Glove consider each
word as single unit and ignore the morphological structure of the word. They are not
able generate word embedding for the unseen or out of vocabulary word during the
training. fastText overcome this limitation of Word2vec and GLOVE by considering
each word as N-gram of characters. A word vector for a word is computed from the
sum of the n-gram characters. The range of N is typically 3 to 6. Since user on social
media often make spelling error, typos, fastText will be more effective then rest of two.
4.3. Paragraph vector/Doc2vec
Paragraph Vector is an unsupervised algorithm that learns fixed-length feature rep-
resentations from variable-length pieces of texts, such as sentences, paragraphs, and
8documents (Le, Quoc et al. , 2014). Paragraph vector represents each document by
a dense vector which is trained to predict words in the document. Authors believe
that Paragraph vector have the potential to overcome the weaknesses of bag-of-words
models and claimed that Paragraph Vectors outperform bag-of-words models as well
as other techniques for text representations. Paragraph vector model is also referred
as doc2vec model. Henceforth, we will refer paragraph and Doc2vec interchangeably.
Doc2vec model have two architecture namely : (i) DM: This is the Doc2Vec model
analogous to CBOW model in Word2vec. The paragraph vectors are obtained by train-
ing a neural network on the task of inferring a center word based on context words and
a context paragraph. (ii) DBOW: This is the Doc2Vec model analogous to Skip-gram
model in Word2Vec. The paragraph vectors are obtained by training a neural network
on the task of predicting a probability distribution of words in a paragraph given a
randomly-sampled word from the paragraph.
4.4. Transfer Learning
Transfer Learning in NLP is not as matured as compare to in Computer Vision. Trans-
fer learning is a method in which model is trained on large corpus for a particular task
and use this pre-trained model for the similar task. There are two way to use transfer
learning in NLP (i) Use of Pre-trained word embedding to initialize first layer of neural
network model which can be called as shallow representation. (ii) Use the full model
and fine tune for the task specific in supervise learning way.
Word2vec, Glove and fastText provide pre-trained word vector trained on the large
corpus. Google Word2vec pre-trained model have word vector for 3 million words with
size 300 and trained on Google news. Glove pre-trained model available with different
embed size and trained on common crawl, Twitter. We have use Glove pre-trained
model with vocabulary size 2.2 million and trained on common crawl. fastText pre-
trained models are available in 157 language. We have use fasttext pre-trained vector
for Englsih and Hindi language trained on commnon crawl and wikipedia.
Recently, transfer learning in NLP done in new way; First language model is trained
on large text corpus in unsupervise way and fine tune on specific task like text classifi-
cation on labeled data. Peters et al. (2018) author argued that word representation is
depend upon the context. So each word has different word vector depending upon the
position of the word in the sentence. Essentially Each word has dynamic word vector
with respect to the context as opposed to the traditional word embedding techniques
which always give same word vector ignoring the context. Embedding from Language
Models (ELMos) use languge model for the word embedding. Howaard et al. (2018)
author propose Universal Language Model Fine-Tuning for Text Classification (ULM-
FiT) which is bi-LSTM model that is trained on a general language modeling (LM)
task and then fine tuned on text classification. Results are reported on both transfer
learning model on TRAC dataset (Kumar et al. , 2018).
5. Evaluation Tasks
We have benchmarked various text representation scheme on two specialized NLP
task namely: aggression detection and fact detection. Text Representation scheme are
evaluated on machine learning and deep neural model.
95.1. Aggression Detection task
The objective of this task is to identify type of aggression present in the text in
both Englsih and code-mixed Hindi language. Aggression are classified into three
level namely: √ívertly Aggressive` (OAG), `Covertly Aggressive` (CAG) and `Non-
aggressive` (NAG). We have implemented all standard machine learning classifiers
like Multinomial Naive Bayes (MNB), Logistic Regression(LR), K-Nearest neighbors
(KNN), Support Vector Classifier (SVC), Decision Tree (DT),Stochastic Gradient De-
scent(SGD), Random forest (RF), Ridge, AdaBoost, Perceptron, and various voting
based ensemble with different text representation schemes like count based, TF/IDF
and word embedding to prepare baseline results. Various word embedding techniques
like Word2Vec, Glove, fastText, Paragraph2Vec are studied.
5.1.1. Problem statement
Basically Aggression detection is a Text classification problem. Formally, the task of
Text Classification is stated as follows. Given a set of social media feed and a set of
classes, We need to compute a function of the form:
C=f(T;
)
where f is the multi-class classifier that is computed using training data, T is the
numeric representation of the text of the dataset,
 is the set of parameters of the
classifier and C is the pre-define class-labels.
5.1.2. Model Architectures and Hyperparameters
In this subsection, we will discuss the architecture and hyperparameters of our deep
neural model used for the classification. Model learns feature from the input texts
Ther is no need to design hand-crafted features which used to encode text into feature
vector.
5.1.2.1. Bidirectional LSTM. The first model is based on the Bidirectional LSTM
include embedding layer with embed size 300, convert each word from the post into
a fixed length vector. short posts are padded with zero values. Subsequent layers
includes Bidirectional LSTM layer with 50 memory units followed by one-dimensional
global max pooling layer, a hidden layer with size 50 and output layer with softmax
activations. ReLU activation function is used for the hidden layer activation. A drop
out layer is added between the last two layers to counter the overfitting with parameter
0.1. Hyperparameters are as follows: Sequence length is fixed at 1073 word; maximum
length of posts in the dataset. No of features is equal to half of total vocabulary size.
Models are trained for 10 epoch with batch size 128. Adam optimization algorithm is
used to update network weights.
5.1.2.2. Single LSTM with higher dropout. This model is based on the Long
Short Term Memory, a type of recurrent neural network with higher dropout. This
model is having one embedding layer, one LSMT layer with a size 64 memory unit,
and one fully connected hidden layer with Relu activation and size 256 and an output
layer with softmax activation. Hyperparameters are same as discussed in the previous
model. A dropout layer is added between the hidden layer and an output layer with
10drop out rate 0.2 to address the overfitting issue.
5.1.2.3. CNN Model. This model includes one embedding layer whose weights
are initialized with fastText pre-trained vector with embed size is 300, followed by
one-dimensional convolution layer with 100 filters of height 2 and stride 1 to target
biagrams. In addition to this, Global Max Pooling layer added to fetch the maximum
value from the filters which are feed to the fully connected hidden layer with size
256, followed by output layer. ReLU and softmax activation function are used for the
hidden layer and output layer respectively.
5.1.2.4. CNN model with Multiple Convolution layer. This model includes
embedding layer with embed size 300. Three one dimensional convolution layers with
size 100 and different filters with height 2,3,4 to target bigrams, trigrams, and four-
grams features, followed by max pooling layer which concatenate max pooled result
from each of one-dimensional convolution layer. The final two layers include a fully con-
nected hidden layer with size 250 and output layer with ReLu and softmax activation.
A Drop out layer is added between the last two layer with rate 0.2. Hyperparameters
are same as discussed in the first model. This model is similar to proposed by (Zhang
et al. , 2015).
5.2. Factual Post/Tweet Detection from Social Media
During the emergency situation like earthquake or
oods, Microblog plays a very
important role as an anonymous communication medium. The various entity like,
Volunteers, NGOs involved in relief operation always look for real-time information
which contains facts instead of prayer and condolence messages. In more technical
term, these agencies are looking for factual information from Microblog instead of the
subjective information. In addition to this, the system should generate rank-list of the
tweets based upon the worthiness of facts. we considered this problem as a binary
classification problem plus pure IR Ranking problem. two classes can be labeled as
factual and non-factual.
the IRMiDis dataset3,which was prepared from the tweet posted during Nepal
earthquake 2015 Basu et al. (2018) is considered for the experiment. There are only
83 fact checkable tweets in the dataset. Non-factual tweets are not available. Total no
of tweets in the dataset is more than 50000.
5.2.1. Preparation of Training Data
Due to the unavailability of adequate training data, The first task is to prepare
training data to train the deep neural model. We randomly choose 100 tweets from
the dataset and labeled as a non-fact-checkable tweet and 83 fact-checkable tweets
present in the dataset labeled as fact-checkable. We have trained our Convolution
neural network on these training data and tested the model on the remaining 50000
tweets. At this stage we are not interested in the class but, we have sorted all the
tweets based upon the predicted probability of the fact-checkable class and selected
top 2000 tweets. We have randomly selected tweets and gave relevance judgment
based upon availability of factual information in first 1000 tweets and manually
3https://sites.google.com/site/irmidisfire2018/
11extracted 300 tweets as non-fact-checkable tweets to minimize the false positives.
Remaining 1700 tweets labeled as fact-checkable tweets. We selected the last 1700
tweets with the least probability of the class fact-checkable and labeled them as
non-fact-checkable tweets. So our Training corpus has 1783 fact-checkable and 2000
non-fact-checkable tweets.
5.2.2. Proposed Approach
We have used word embedding to represent the text instead of bags-of-words. fastText
(Mikolov et al. , 2018) pre-trained vector with 300 dimensions is used to initialize the
weight matrix of the embedding layer of the network. We trained our CNN model on
this training corpus with 10-fold cross-validation.The Model gives validation accuracy
around 94%. Finally, we run the model on the entire corpus and sorted the tweet based
upon the predicted probability of the Fact-checkable class. Essentially this approach
termed as weakly-supervise classification.
6. Results
In this section, we first present results of classifiers TRAC dataset Kumar et al.
(2018) with different text representation scheme. Latter we present result on FIRE
IRMiDis 2018 Dataset. Tweets are very noisy in nature contains user mentions, Hash-
tags, Emojis, and URLs. We do not perform any kind of text pre-processing on tweets
in experiments with deep neural models. In experiments with machine learning classi-
fier, before classification, Hashtag symbol # and User mentions are dropped from the
tweets. Non-ASCII characters and stop-words are removed from tweet text (Modha et
al. , 2016).
6.1. Results On TRAC Dataset
Precision, Recall, and F1-score are the standard metrics which are used to evaluate the
classifier performance. We have evaluated 16 classifiers performance on 4 Datasets (2
English+2 Hindi) with 10 Text Representation scheme(8 in the case of Hindi Dataset).
Looking at such massive experiment, it is difficult to report results in all the above
metrics. Therefore, Results are reported in terms of weighted F1-score only which is
the function of Precision and Recall. Classifiers results based on LSTM and CNN
on BoW text representation schemes are not possible due to the high dimensionality.
Bernoulli classifier is used instead of Naive Bayes Classifier in case of text represen-
tation schemes other than BoW. Since word vectors might have negative weights, it
is impossible to calculate probabilities with negative weights. Skip-gram variant of
Word2Vec and fastText is used in this experiment instead of continuous bag-of-word.
Table 6 and 7 shows results on Facebook and Twitter English Dataset with BoW and
word embedding while Table 8 present result with pre-trained word embedding with
same dataset. Table 9 and 10 shows results on Facebook and Twitter code-mixed Hindi
Dataset. Only fastText provide pre-trained word vector (Mikolov et al. , 2018) for the
Hindi language. Exhaustive evaluation is performed with all classifiers with respect
to each text representation schemes. Experiments are also performed with the new
transfer learning model like ELMO and ULMFIT. Table 11 presents results on both
Facebook and Twitter English Datasets. Figure 1 and figure 2 display the heatmap of
12Table 6. F1-score on TRAC Facebook English Dataset
Classifier Count-
vectorTF/IDF W2Vec Glove Fasttext doc2vec-
dmcdoc2vec-
dbow
NB 0.5571 0.5596 0.4870 0.3873 0.5035 0.4585 0.4634
LR 0.5953 0.6046 0.5675 0.5358 0.5400 0.5266 0.5139
KNN 0.5466 0.5428 0.5061 0.5130 0.5113 0.5114 0.5095
SVC 0.5801 0.5902 0.5369 0.5037 0.5137 0.5388 0.5033
DT 0.5269 0.5055 0.4468 0.4067 0.5002 0.4198 0.4198
SGD 0.5706 0.5938 0.4647 0.3571 0.5167 0.5060 0.3521
RF 0.5621 0.5582 0.5199 0.4752 0.5513 0.4230 0.4210
Ridge 0.6009 0.5999 0.5347 0.5336 0.5225 0.5385 0.5083
AdaB 0.6210 0.6141 0.5491 0.4932 0.5644 0.4689 0.4852
Perce. 0.5387 0.5491 0.5230 0.4020 0.4848 0.3800 0.3253
ANN 0.5703 0.5350 0.5350 0.5037 0.5380 0.4980 0.4401
Ensemble 0.58 0.5900 0.5558 0.4067 0.5617 0.4980 0.4401
LSTM 0.5649 0.5454 0.5062
BLSTM 0.5759 0.4760 0.5641
CNN 0.5515 0.5365 0.5638
NCNN 0.5919 0.5488 0.4849
Table 7. F1-score on TRAC Twitter English Dataset
Classifier Count-
vectorTF/IDF W2Vec Glove Fasttext doc2vec-
dmcdoc2vec-
dbow
NB 0.5102 0.4528 0.5551 0.3936 0.5495 0.3254 0.3536
LR 0.4849 0.4890 0.3457 0.3959 0.3871 0.3041 0.3274
KNN 0.3539 0.2891 0.3843 0.3607 0.3997 0.3225 0.3191
SVC 0.4642 0.4853 0.3078 0.3627 0.2858 0.3019 0.3274
DT 0.4229 0.4111 0.3884 0.3673 0.3948 0.3326 0.3326
SGD 0.4682 0.5020 0.4512 0.4182 0.3838 0.3251 0.3350
RF 0.4199 0.3917 0.4333 0.3634 0.4069 0.3301 0.3293
Ridge 0.4703 0.5003 0.3352 0.3877 0.3180 0.2994 0.3243
AdaB 0.3343 0.3696 0.4485 0.3552 0.4215 0.3288 0.3223
Perce. 0.4930 0.4778 0.3521 0.3938 0.3340 0.3015 0.2990
ANN 0.4912 0.5164 0.5111 0.3552 0.4532 0.3230 0.3281
Ensemble 0.495 0.4842 0.4500 0.3938 0.4471 0.3230 0.3281
LSTM 0.5385 0.5156 0.5335
BLSTM 0.5314 0.3860 0.4985
CNN 0.5012 0.5377 0.4849
NCNN 0.5120 0.4984 0.5179
the results achieved by classifiers on each text representation scheme.
6.2. Information Retrieval from Microblogs during Disasters (IRMiDis)
Dataset
As discussed in the previous section 1, This task is classification plus Ranking task.
Table 13 shows our system results on IRMiDis dataset (Basu et al. , 2018) along with
the rest of teams. nDCG overall is the primary metric for the evaluation. Our system
substantially outperforms rest of team in the most of the metrics which justifies our
claim established on TRAC dataset (Kumar et al. , 2018)
7. Result Analysis
In this section, we will present the comprehensive result analysis and try to answer
the research questions which framed before the experiments were performed. As we
look at the table 6 7, and 8, Overall, LSTM and CNN with pre-trained fastText word
embedding marginally outperform (around 2 % to 4%) standard machine learning
13Table 8. F1-score on TRAC Facebook and Twitter English Dataset:Using Pre-trained word vectors.
Facebook Test Dataset Twitter Test Dataset
Classifier p-Word2vec p-Glove p-Fasttext p-Word2vec p-Glove p-Fasttext
NB 0.5342 0.5373 0.5519 0.4152 0.4527 0.4276
LR 0.5799 0.6050 0.6045 0.4197 0.4527 0.4441
KNN 0.4981 0.5103 0.4819 0.3405 0.3959 0.3912
SVC 0.5832 0.5678 0.6120 0.4446 0.4581 0.4350
DT 0.4700 0.4515 0.4900 0.3640 0.3949 0.3632
SGD 0.5019 0.5521 0.5360 0.3692 0.3793 0.3852
RF 0.5402 0.5338 0.5505 0.3394 0.3716 0.3687
Ridge 0.5829 0.5952 0.6140 0.4092 0.4530 0.4461
AdaB 0.5713 0.5781 0.5907 0.4241 0.4261 0.4033
Perce. 0.5114 0.5201 0.5660 0.4224 0.4118 0.4049
ANN 0.5025 0.5498 0.5722 0.3728 0.3722 0.4842
Ensemble 0.5300 0.5500 0.5558 0.3728 0.3722 0.4500
LSTM 0.4979 0.4979 0.6178 0.5537 0.5518 0.5541
BLSTM 0.5501 0.6062 0.6000 0.5359 0.5466 0.5423
CNN 0.4749 0.5405 0.6407 0.5226 0.5667 0.5520
NCNN 0.5169 0.5883 0.5600 0.5384 0.5067 0.5407
Table 9. F1-score on TRAC Facebook Code-mixed Hindi Dataset
Classifier Count-
vectorTF/IDF W2Vec Glove Fasttext p-
fastTextdoc2vec-
dmcdoc2vec-
dbow
NB 0.5535 0.6031 0.3001 0.372 0.2959 0.3176 0.3459 0.4736
LR 0.5855 0.6134 0.5779 0.464 0.5457 0.5518 0.3894 0.4380
KNN 0.3340 0.1721 0.4998 0.425 0.5106 0.4909 0.3768 0.4038
SVC 0.5556 0.5862 0.4806 0.373 0.5186 0.5442 0.3879 0.4344
DT 0.5307 0.5025 0.4629 0.388 0.4392 0.4288 0.3485 0.3485
SGD 0.5533 0.5922 0.3912 0.393 0.3670 0.4746 0.3331 0.4134
RF 0.5473 0.5473 0.5374 0.440 0.5047 0.4788 0.3512 0.3477
Ridge 0.5780 0.5850 0.5293 0.381 0.5092 0.5544 0.3866 0.4292
AdaB 0.5373 0.5233 0.5342 0.479 0.5336 0.4913 0.3751 0.4214
Perce. 0.5213 0.5598 0.4232 0.364 0.3763 0.4873 0.2661 0.3282
ANN 0.5703 0.5350 0.5455 0.5037 0.5842 0.5190 0.4091 0.4440
Ensemble 0.5700 0.6087 0.5558 0.4067 0.534 0.5612 0.4980 0.4401
LSTM 0.5649 0.590 0.6021 0.5916
BLSTM 0.5759 0.527 0.5770 0.5900
CNN 0.5515 0.566 0.5950 0.6081
NCNN 0.5919 0.573 0.5912 0.5965
Table 10. F1-score on TRAC Twitter Code-mixed Hindi Dataset
Classifier Count-
vectorTF/IDF W2Vec Glove Fasttext p-fastText doc2vec-
dmcdoc2vec-
dbow
NB 0.2970 0.2902 0.3215 0.273 0.3359 0.2897 0.3270 0.3205
LR 0.3787 0.3724 0.2819 0.279 0.3184 0.3524 0.2438 0.2833
KNN 0.2527 0.2553 0.3704 0.334 0.3381 0.2917 0.3051 0.3299
SVC 0.3781 0.3886 0.2821 0.261 0.3087 0.3472 0.2580 0.2905
DT 0.3685 0.3936 0.3572 0.326 0.3475 0.3473 0.2988 0.2988
SGD 0.3996 0.3993 0.2822 0.287 0.2739 0.3163 0.2605 0.2588
RF 0.3585 0.3737 0.3286 0.344 0.3449 0.3288 0.2981 0.2988
Ridge 0.3616 0.3872 0.2811 0.242 0.3346 0.3361 0.2549 0.2875
AdaB 0.1886 0.1903 0.3256 0.362 0.3261 0.3441 0.2614 0.2933
Perce. 0.3931 0.3868 0.2802 0.329 0.2787 0.3835 0.2616 0.2752
ANN 0.430 0.44 0.3163 0.3552 0.2399 0.3593 0.2419 0.3132
Ensemble 0.4400 0.4600 0.4500 0.3938 0.3426 0.3555 0.3230 0.3281
LSTM 0.3840 0.376 0.3667 0.4600
BLSTM 0.2846 0.318 0.3005 0.4600
CNN 0.3323 0.317 0.2669 0.4992
NCNN 0.3338 0.380 0.3494 0.4600
14Table 11. F1 score on TRAC Facebook English Test Dataset using
Transfer Learning methods
Transfer learning Model English
Facebook Dataset Twitter Dataset
ELMO 0 :3699 0 :3854
ULMFiT 0 :4725 0 :4664
Table 12. weighted F1-score TRAC Test Dataset: comparison with peers
System English Hindi
Facebook
DatasetTwitter
DatasetFacebook
DatasetTwitter
Dataset
Our system result 0.6407 0.5541 0.6081 0.4992
DA-LD-hildesheim 0.6178 0.552 0.6081 0.4992
saroyehun 0.6425 0.5920 NA NA
EBSI-LIA-UNAM 0.6315 0.5715 NA NA
TakeLab 0.5920 0.5651 NA NA
taraka rama 0.6008 0.5656 0.6420 0.40
vista.ue 0.5812 0.6008 0.5951 0.4829
na14 0.5920 0.5663 0.6450 0.4853
Figure 1. Heatmap on English Facebook Test Dataset Results.
Table 13. Results Comparison with rest of team on FIRE 2018 IRMiDis Dataset.
System p@100 R@1000 MAP@100 MAP nDCG @100 nDCG
Our System 0.4 0 :2002 0 :0129 0 :1471 0 :4021 0 :7492
MIDAS-semiauto 0 :9600 0 :1148 0 :0740 0 :1345 0 :6007 0 :6899
MIDAS-1 0 :8800 0 :1292 0.0581 0.1329 0.5649 0.6835
FAST NURun2 0.7000 0.0885 0.0396 0.0801 0.5723 0.6676
UEM DataMining 0.6800 0.1427 0.0378 0.1178 0.5332 0.6396
iitbhu irlab2 0.3900 0.0447 0.0144 0.0401 0.3272 0.6200
15Figure 2. Heatmap on English Twitter Test Dataset Results.
Table 14. Results Comparison with CNN model and Logistic Regression TRAC
Facebook English Test Dataset
Class CNN model Logistic Regression #Posts
P R Weighted F1 P R Weighted F1
NAG 0.86 0.64 0.73 0.83 0.60 0.70 630
CAG 0.28 0.46 0.35 0.23 0.54 0.32 142
OAG 0.42 0.61 0.50 0.46 0.39 0.42 144
overall 0.70 0.61 0.64 0.68 0.56 0.60 916
classifiers and ensemble of classifier with respect to weighted F1- score on Facebook
English corpus and substantially outperforms on Twitter English corpus. By and large
similar results observed on code-mixed Hindi corpus as shown in table 9 and 10.
Table 14 present the detailed comparative results of two classifiers: CNN model
with fastText pre-trained vector and the logistic regression with TF/IDF weighting
on TRAC Facebook English dataset. The CNN Model classify Facebook posts better
than logistic regression at the individual class level and overall. It has been quite
evident that posts belong to CAG class are hard to classify and Malmasi, et al. (2017)
reported that the same observation. Table 15 show posts which are miss-classified by
logistic regression however, CNN model correctly classified them into the CAG class.
7.1. Significance Test
To support our claim drawn in the previous section, significance tests, like Wilcoxon
signed-rank test and Student t-test were carried out by comparing Weighted F1score
of each classifier for each text representation scheme with fastText pre-trained vector
scheme. Table 16 and 17 summarizes the p-values of statistical significance tests on
16Table 15. sample post for the CAG class.
no Post text Gold Label CNN LR
1 Mauni singh trying very hard to convince himself what
is written in script... body language says it allCAG CAG NAG
2 Indian govt is all Abt giving money to Bangladesh on
the terms of Bangladesh ll give that all projects to
amabani n adani for thier benefits lol who cares Abt
soldiers or India they r just puppets of thier owners
feku or pappuCAG CAG NAG
3 When asked to speak in Parliament ran away. Speaks
only in TV,radio or in election rally. Can we expect
Another crying drama after Demonetisation disaster ?
#cryBaby"CAG CAG NAG
Table 16. p-values of Significance test on F1-score on TRAC Facebook English Dataset
Text Rep. scheme Facebook English Twitter English
Wilcoxon T-test Wilcoxon T-test
Count Vector 0.001 0.12 0.004 0.016
TF/IDF 0.0001 0.1465 0.0061 0.0391
Word2vec 0.00002 0.0001 0.69 0.30
Glove 0.00001 0.000002 0.01 0.0009
fastText 0.00003 0.0004 0.12 0.08
doc2vec-dmc 0.000009 0.0001 0.0004 0.000004
doc2vec-dbow 0.000009 0.00003 0.0004 0.000006
p-Word2vec 0.00001 0.0006 0.02 0.01
P-Glove 0.0002 0.02 0.08 0.30
English and Hindi Dataset respectively. In Wilcoxon signed-rank test, p-values of the
results is less than 0.05 for Facebook English dataset and Twitter Hindi Dataset.
However, On Twitter English dataset and Facebook Hindi Dataset, some of the p-
values are higher than 0.05. In student t-test, we get mixed bag results. By and large,
our results are statistically significant.
In the following subsection, we will try to answer all the research questions framed
during the experiments were planned.
7.2. Best Text Representation scheme to model the text from Social web
Text Representation is the primary task for to address any NLP task like Ques-
tion/answering, classification etc. As dicusses in section 4, There are basically two
text representing scheme:Bag-of-Word(BoW) with countvector, TF/IDF weighting
and word embedding. Word2Vec (Mikolov et.al , 2013), Glove (Pennington et.al. ,
2014), and fastText (Mikolov et al. , 2018), an extension of Word2vec are popular
word embedding techniques.
Table 17. p-values of Significance test on F1-score on TRAC code-mixed Hindi Test Dataset
Text Rep. scheme Facebook Code-mixed Hindi Twitter Code-mixed Hindi
Wilcoxon T-test Wilcoxon T-test
Count Vector 0.003 0.05 0.01 0.20
TF/IDF 0.003 0.14 0.007 0.12
Word2vec 0.40 0.17 0.043 0.017
Glove 0.001 0.0005 0.015 0.004
fastText 0.35 0.15 0.022 0.006
doc2vec-dmc 0.005 0.00001 0.001 0.0008
doc2vec-dbow 0.003 0.002 0.001 0.002
17Results clearly show that models with fastText pre-trained vector outperform Glove
pre-trained vector on Facebook test dataset as well as the Twitter test dataset. the
main reason behind the outperformance of fastText over Glove and Word2vec is that
The fastText consider each word as N-gram characters. A word vector for a word
is computed from the sum of the n-gram characters. Glove and Word2vec consider
each word as a single unit and provide a word vector for each word. Since Facebook
users make a lot of mistakes in spelling, typos, fastText is more convenient than Glove
(Majumder et al., 2018). from Figure 1 and 2 shows that BoW is still effective text
representation scheme for the standard machine learning classier which takes hand-
crafted feature and n-grams as inputs. Logistic Regression and Support Vector perform
better than other classifiers in English as well as Hindi Dataset. Adaboost performs
better than LR and SVC on Facebook English Dataset but substantially underperform
them on rest of three Datsets. Our participation (Majumder et al., 2018) in TRAC
competition (Kumar Ritesh et al., 2018) FIRE Information Retrieval from Microblogs
during Disasters (Basu et al. , 2018) track where our team performed well and secured
top position.
7.3. Transfer Learning Model vs Pre-trained Word Embedding Model
Transfer learning is focused on storing knowledge gained while solving one problem and
applying it to a different but related problem. On many occasion, NLP researchers face
the problem of unavailability of sufficient labeled data to train the model. With the
advent of new transfer learning method like ELMO (Peters et al. , 2018) and Universal
language model fine-tuning for Text Classification (ULMFiT) (Howaard et al., 2018)
attract interest among NLP Researchers. These models are trained or large text corpus.
Howaard et al. (2018) claimed that these model can be fine-tuned on the task-specific
corpus. We have used these transfer learning model on TRAC English dataset Kumar
et al. (2018) and results are presented in table 11. one can observe that results are
substantially lower than the results reported in Table 6, 7, and 8 where pre-trained
word vectors are used to initialize the first layer of deep neural model and rest of the
network is trained from scratch achieve better results than transfer learning model.
Howaard et al. (2018) termed use of pre-trained vector as shallow representation.
In these experiments, we trained different classifier models on Facebook posts. Table
7 10 shows the results on Twitter dataset Kumar et al. (2018). There is lexical
difference between Facebook and Twitter posts. From the results shown in Table 7 8
and table 10, one can conclude that weighted F1score of standard machine learning
classifiers are substantially lower in Twitter Dataset as compare to Facebook Dataset.
While deep learning models perform better than machine learning classifiers for the
Twitter Dataset. Thus, Deep learning models are more robust than machine learning
classifier across diverse datasets.
7.4. Does Deeper Neural Net make Sense
To answer this question, we designed first CNN model with one convolution layer and
other CNN model with 3 convolution layer with different filters height. As we look
at results shown in Table 6,7,8, and 9, one can conclude that by and large weighted
F1score lower for CNN model with multiple convolution layer than CNN model with
single convolution layer.</body>
  <conclusion>In this Paper, Multilingual Social media stream is studied with special kind of text
features: Aggression and fact perspective. Exhaustive experiments are performed to
benchmark the text representation scheme on machine learning classifiers and deep
neural nets. From the results, we conclude that deep Neural model with pre-trained
word embedding is the better choice than machine earning classifier and transfer learn-
ing model. Word embedding is the better text representative scheme than Bag-of-words
for the deep neural models. In fact, performance can be improved with the help of fast-
Text pre-trained vector. However, machine learning classifiers perform better in BoW
with TF/IDF weighting than word embedding. We also concluded that higher drop
out will help to counter model overfitting and improvise a standard evaluation metrics.
CNN and LSTM are the better models for these datasets. On the English test corpus,
we obtained a better weighted F1score for NAG class and poor weighted F1score
for CAG class which supports the previous (Malmasi, et al. , 2017) findings. For the
Facebook Hindi test corpus, the same seems not to be true. We obtained a better F1
score for CAG class than NAG class. It is also to be noted that the model leads to
poor result on Twitter test data since the training corpus was created from Facebook.
In such cases, deep neural models substantially outperform machine learning classi-
fiers. Significance test confirms these claims with 95 % confidence interval in most the
cases. Our work shows what kind of problems are moving into the center of attention
for research in machine learning. Using deep learning models, there is great potential
to solve some of these problems, yet still, the performance is far from perfect. Model
transfer between problems and the application of derived knowledge in user interfaces
are areas directions for future work.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media:
Using deep neural networks, data augmentation, and pseudolabeling .Proceedings of the First
Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018) , pp.90{97.
Arroyo-Fern√© andez, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel and
Carrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). Cyberbullying
Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING'18 TRAC-1 .Proceedings
of the First Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018) , pp 140{149.
Basu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the FIRE
2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis) .Proceedings
of FIRE 2018 - Forum for Information Retrieval Evaluation , Gujrat, India, December .
Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval-
2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis .
Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) ,
pp.747{754
Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) En-
riching word vectors with subword information ,Transactions of the Association for Compu-
tational Linguistics , vol-5,pp.135{146, MIT Press.
Burnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An applica-
tion of machine classification and statistical modeling for policy and decision makingPolicy
&amp; Internet , vol-7 number 2, pp.223{242, Wiley Online Library.
Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gon calves , Bruno and
Menczer, Filippo and Flammini, Alessandro (2011). Political polarization on twitter. ,Icwsm ,
vol-133, pp.89{96.
19Conover, Michael D and Gon calves, Bruno and Ratkiewicz, Jacob and Flammini, Alessandro
and Menczer, Filippo (2011). Predicting the political alignment of twitter users ,Privacy,
Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social
Computing (SocialCom), 2011 IEEE Third International Conference on , pp.192{199.
Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar (2017). Au-
tomated Hate Speech Detection and the Problem of Offensive Langemuage .Proceedings of
ICWSM .
Deriu, Jan and Gonzenbach, Maurice and Uzdilli, Fatih and Lucchi, Aurelien and Luca, Valeria
De and Jaggi, Martin (2016). Swisscheese at semeval-2016 task 4: Sentiment classification
using an ensemble of convolutional neural networks with distant supervision .Proceedings of
the 10th international workshop on semantic evaluation ,pp.1124{1128.
Hltcoe, J (2013). Semeval-2013 task 2: Sentiment analysis in Twitter ,vol-312 Atlanta, Georgia,
USA.
Howard, Jeremy &amp; Ruder, Sebastian 2018. Universal language model fine-tuning for text clas-
sification" ,arXiv preprint arXiv:1801.06146 ,
Harris, Zellig S (1954), Distributional structure Word,10, number=2-3,pp.146{162, 1954, Taylor
&amp; Francis.
Kumar, Ritesh and Reganti, Aishwarya N. and Bhatia, Akshit and Maheshwari,Tushar (2018),
Aggression-annotated Corpus of Hindi-English Code-mixed Data ,Proceedings of the 11th
Language Resources and Evaluation Conference (LREC) , Miyazaki, Japan.
Kumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri Marcos (2018). Bench-
marking Aggression Identification in Social Media, Proceedings of the First Workshop on
Trolling, Aggression and Cyberbulling (TRAC), Santa Fe, USA
Kwok, Irene and Wang, Yuzhou (2013),Locate the hate: Detecting Tweets Against Blacks
,Twenty-Seventh AAAI Conference on Artificial Intelligence .
Lau, Jey Han and Baldwin, Timothy (2016). An empirical evaluation of doc2vec with practical
insights into document embedding generation .arXiv preprint arXiv:1607.05368 .
Le, Quoc and Mikolov, Tomas (2014) Distributed representations of sentences and docu-
ments .International Conference on Machine Learning , pp.1188{1196
Majumder, Prasenjit and Mandl, Thomas and Modha Sandip (2018) Filtering Aggression from
the Multilingual Social Media Feed Proceedings of the First Workshop on Trolling, Aggres-
sion and Cyberbullying (TRAC-2018) , pp. 199{207
Malmasi, Shervin and Zampieri, Marcos (2017) Detecting Hate Speech in Social Media Pro-
ceedings of the International Conference Recent Advances in Natural Language Processing
(RANLP) , pp.467{472.
Malmasi, Shervin and Zampieri, Marcos (2018). Challenges in Discriminating Profanity from
Hate Speech Journal of Experimental &amp; Theoretical Artificial Intelligence pp.1{16, vol-30,
issue-2,Taylor &amp; Francis.
Maynard, Diana and Funk, Adam (2011) Automatic detection of political opinionsin tweets ,
Extended Semantic Web Conference ,pp. 88{99, Springer .
Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey (2013). Efficient estima-
tion of word representations in vector space ,arXiv preprint arXiv:1301.3781 .
Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin,
Armand (2018). Advances in Pre-Training Distributed Word Representations ,Proceedings of
the International Conference on Language Resources and Evaluation (LREC 2018) .
Modha, Sandip and Agrawal, Krati and Verma, Deepali and Majumder, Prasenjit and Man-
dalia, Chintak 2016. DAIICT at TREC RTS 2016: Live Push Notification and Email Di-
gest.,TREC.
Mohammad, Saif M and Kiritchenko, Svetlana and Zhu, Xiaodan (2013). NRC-Canada: Build-
ing the state-of-the-art in sentiment analysis of tweets.arXiv preprint arXiv:1308.6242 .
Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov,
Veselin (2016). SemEval-2016 task 4: Sentiment analysis in Twitter ,Proceedings of the 10th
international workshop on semantic evaluation (semeval-2016) , pp. 1{18.
Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for
20word representation Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP) , pp.1532{1543.
Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark,
Christopher and Lee, Kenton and Zettlemoyer, Luke (2018). Deep contextualized word rep-
resentations arXiv preprint arXiv:1802.05365 .
Razavi, Amir H and Inkpen, Diana and Uritsky, Sasha and Matwin, Stan (2010) Offensive
language detection using multi-level classification ,Canadian Conference on Artificial Intel-
ligence pp.16{27, Springer
Rosenthal, Sara and Nakov, Preslav and Kiritchenko, Svetlana and Mohammad, Saif and Rit-
ter, Alan and Stoyanov, Veselin (2015). Semeval-2015 task 10: Sentiment analysis in twit-
ter Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015) ,
pp.451{463.
Rosenthal, Sara and Farra, Noura and Nakov, Preslav (2017). SemEval-2017 task 4: Sentiment
analysis in Twitter Proceedings of the 11th International Workshop on Semantic Evaluation
(SemEval-2017) , pp.502{518.
Schmidt, Anna and Wiegand, Michael (2017). A Survey on Hate Speech Detection Using Natural
Language Processing ,Proceedings of the Fifth International Workshop on Natural Language
Processing for Social Media. Association for Computational Linguistics Valencia, Spain,
pp.1{10,
Severyn, Aliaksei and Moschitti, Alessandro (2015) Unitn: Training deep convolutional neural
network for twitter sentiment classification ,Proceedings of the 9th international workshop
on semantic evaluation (SemEval 2015) , pp. 464{469.
Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp G and Welpe, Isabell
M (2010). Predicting elections with twitter: What 140 characters reveal about political sen-
timent. ,Icwsm , vol-10, number-1, pp. 178{185.
Warner, William and Hirschberg, Julia (2012) Detecting hate speech on the world wide web ,
Proceedings of the Second Workshop on Language in Social Media ,pp 19{26,Association for
Computational Linguistics.
Xu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy (2012). Learning
from bullying traces in social media ,Proceedings of the 2012 conference of the North Amer-
ican chapter of the association for computational linguistics: Human language technologies ,
pp.656{666, Association for Computational Linguistics
Zhang, Ye and Wallace, Byron (2015). A Sensitivity Analysis of (and Practitioners' Guide to)
Convolutional Neural Networks for Sentence Classification ,arXiv preprint arXiv:1510.03820 .
Majumder, Prasenjit and Mitra, Mandar and Pal, Dipasree and Bandyopadhyay, Ayan and
Maiti, Samaresh and Mitra, Sukanya and Sen, Aparajita and Pal, Sukomal. Text collections
for FIRE ,Proceedings of the 31st annual international ACM SIGIR conference on research
and development in information retrieval ,pp.699{700,ACM
Majumdar, P and Mitra, Mandar and Parui, Swapan K and Bhattacharya, Initiative for indian
language ir evaluation ,The First International Workshop on Evaluating Information Access
(EVIA)
21</biblio>


  <preamble>L18-1504.pdf</preamble>
  <titre>A New Annotated Portuguese/Spanish Corpus for the Multi-Sentence
Compression Task</titre>
  <auteurs>
    <auteur>
      <name>Elvys Linhares Pontes</name>
      <mail>elvys.linhares-pontes@univ-avignon.fr</mail>
      <affiliation>CERI/LIA, Universit√© d‚ÄôAvignon et des Pays de Vaucluse, Avignon, France</affiliation>
    </auteur>
      <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>CERI/LIA, Universit√© d‚ÄôAvignon et des Pays de Vaucluse, Avignon, France
√âcole Polytechnique de Montr√©al, Montr√©al, Canada</affiliation>
    </auteur>
      <auteur>
      <name>St√©phane Huet</name>
      <mail>stephane.huet@univ-avignon.fr</mail>
      <affiliation>CERI/LIA, Universit√© d‚ÄôAvignon et des Pays de Vaucluse, Avignon, France</affiliation>
    </auteur>
    <auteur>
      <name>Andr√©a Carneiro Linhares</name>
      <mail>andrea.linhares@ufc.br</mail>
      <affiliation>Universidade Federal do Cear√°, Sobral-CE, Brasil</affiliation>
    </auteur>
  </auteurs>
  <abstract>Multi-sentence compression aims to generate a short and informative compression from several source sentences that deal with the same
topic. In this work, we present a new corpus for the Multi-Sentence Compression (MSC) task in Portuguese and Spanish. We also
provide on this corpus a comparison of two state-of-the-art MSC systems.</abstract>
  <introduction>
Among the various applications of Natural Language Pro-
cessing, Automatic Text Summarization (ATS) aims at
summarizing one or more texts automatically. Summariza-
tion systems identify relevant data and create a summary
from key information. The (Multi-)Sentence Compression
task can be seen as a subproblem of ATS with the objective
to generate a shorter, informative and correct sentence from
source sentence(s).
In many cases, state-of-the-art NLP systems are evaluated
with experiments restrained to the English language, in part
because there are a lot of available English resources for
most NLP tasks. As regards Multi-Sentence Compression
(MSC), the available resources are unfortunately limited; to
our knowledge, only one dataset is freely available and it is
conÔ¨Åned to the French language (Boudin and Morin, 2013).
In this work, we present a new annotated corpus in the Por-
tuguese and Spanish languages for the MSC task. Using
this corpus, we evaluate two state-of-the-art systems and
show that the use of several languages leads to more miti-
gated results on the superiority of one system than the use
of the French corpus alone.
The remainder of this paper is organized as follows. In Sec-
tion 2, we characterize MSC with respect to related tasks
from the perspective of the available corpora. Section 3
describes the creation and the features of our corpus. In
Section 4 we analyze the results achieved by state-of-the-
art methods using our dataset. Finally, conclusions are set
out in Section 5.
</introduction>
  <body>Sentence Compression (SC) aims at producing a reduced
grammatically correct sentence from a source sentence. SC
can be used in the context of the abstractive summarization
of documents, the generation of article titles or the simpli-
Ô¨Åcation of complex sentences, using diverse methods (opti-
mization, syntactic structure, deletion of words and/or gen-
eration of sentences). The corpora for SC can be dividedin two categories: deletion-based and summarization-based
SC.
In the case of SC by deletion of words, sentences are
compressed by removing irrelevant words (Filippova et al.,
2015; Ive and Yvon, 2016). Knight and Marcu (2002) de-
veloped a SC corpus by aligning abstracts and sentences
extracted from the Ziff-Davis corpus, which is a collec-
tion of newspaper articles announcing computer products.
Clarke and Lapata (2008) provided two manually created
two-reference corpora for deletion-based compression. Fil-
ippova and Altun (2013), and Filippova et al. (2015) ex-
tracted and released deletion-based compressions by align-
ing news headlines to the Ô¨Årst sentences. Finally, Ive and
Yvon (2016) developed an English-French parallel corpus
for the compression and simpliÔ¨Åcation tasks.
SC by generations of sentences analyzes a whole sentence
and generates a new shorter sentence with the core infor-
mation of the source sentence (Rush et al., 2015; Gan-
itkevitch et al., 2011; Cohn and Lapata, 2008; Toutanova
et al., 2016). Ganitkevitch et al. (2011) created a corpus
of compression paraphrases composed of parallel English-
English sentences obtained from multiple reference transla-
tions. Rush et al. (2015) produced compression pairs made
up of the headline of each article and its Ô¨Årst sentence; they
released their code to extract data from the annotated Gi-
gaword (Graff et al., 2011). Cohn and Lapata (2008) and
Toutanova et al. (2016) describe two manually created ab-
stractive compression corpora that are publicly available.
The dataset presented in Cohn and Lapata (2008) comprises
a single-reference sentence pairs for abstractive summary,
while the corpus developed by Toutanova et al. (2016) has
multiple references for short paragraph compressions.
Multi-Sentence Compression (MSC), also known as Multi-
Sentence Fusion, is a variation of SC. MSC aims at ana-
lyzing a cluster of similar sentences to generate a new sen-
tence, which is shorter than the average length of source
sentences and has the key information of the cluster (Barzi-
lay and McKeown, 2005; Filippova, 2010). MSC enables
summarization and question-answering systems to gener-
3192CharacteristicsFrench Portuguese Spanish
Source Reference Source Reference Source Reference
#tokens 20,224 2,362 17,998 1,425 30,588 3,694
#vocabulary (tokens) 2,867 636 2,438 533 4,390 881
#sentences 618 120 544 80 800 160
avg. sentence length (tokens) 33.0 19.7 33.1 17.8 38.2 23.1
type-token ratio 38.8% 50.1% 33.7% 67.9% 35.2% 43.4%
sentence similarity [0,1] 0.46 0.67 0.51 0.59 0.47 0.64
Table 1: Statistics of the corpora.
ate outputs combining fully formed sentences from one or
several documents. Various corpora have been developed
for MSC and are composed of clusters of similar sentences
from different source news in English, French, Spanish or
Vietnamese languages (Barzilay and McKeown, 2005; Fil-
ippova, 2010; Boudin and Morin, 2013; Thadani and McK-
eown, 2013; Luong et al., 2015). Filippova‚Äôs corpus as
well as Boudin and Morin‚Äôs contain clusters of similar sen-
tences, each cluster composed of at least 7 or 8 sentences,
whereas the datasets introduced in (McKeown et al., 2010)
and (Luong et al., 2015) have only a pair of source sen-
tences per cluster. McKeown et al. (2010) collected 300
English sentence pairs taken from newswire clusters using
Amazon‚Äôs Mechanical Turk. Likewise, the dataset built by
Luong et al. (2015) contains 250 Vietnamese sentences di-
vided into 115 groups of similar sentences with 2 sentences
per group. Thadani and McKeown (2013) presented an En-
glish corpus with 1,858 clusters having between 2 and 4
sentences; this dataset was built using automatic methods
from annotations made for the DUC1and TAC2evalua-
tions. The corpora presented in (McKeown et al., 2010),
(Boudin and Morin, 2013) and (Luong et al., 2015) are pub-
licly available, but among these three datasets only the sec-
ond one is more suited to multi-document summarization
or question-answering tasks because the documents to ana-
lyze are usually composed of many similar sentences.
3. Dataset Description
We introduce a novel annotated corpus collected from Por-
tuguese and Spanish Google News.3This corpus is com-
posed of clusters of similar sentences along with reference
compressions for each cluster. The data are described in the
following subsections. Table 1 summarizes the characteris-
tics of the corpus and Table 2 shows a small example of our
Portuguese dataset.
3.1. Source Sentences
In keeping with the methodology introduced by Filippova
(2010), we collected links from Google News in Spanish
and Portuguese between July and September 2016. These
links redirect international news sites in Spanish ( La Jor-
nada, Milenio, El Economista, BBC Mundo, El Colom-
biano, El Pa ¬¥ƒ±s, CNN en espa Àúnol, etc.) and in Portuguese
1http://duc.nist.gov
2http://www.nist.gov/tac
3The Spanish and Portuguese MSC datasets are freely avail-
able, under GPL license on the DOI website: http://dev.
termwatch.es/ Àúfresa/CORPUS/MSF2/ .(G1, Uol Not ¬¥ƒ±cias, Estad Àúao, O Globo, etc. ). Each cluster is
composed of related sentences describing a speciÔ¨Åc event
and was chosen among the Ô¨Årst sentence from different ar-
ticles about Science, Sports, Economy, Health, Business,
Technology, Accidents/Catastrophes, General Information
and other subjects. During the collection period, sentences
were gathered among news threads that had at least 8 dif-
ferent sources. The source sentences of each cluster were
manually selected so that they best describe the news, while
sentences dealing with less relevant information were dis-
carded. Each source sentence is composed of at least 8 to-
kens and a verb. In order to ensure the variability of source
sentences inside a cluster, we removed all duplicated sen-
tences, by assuming that sentences were too similar when
the cosine similarity4computed from one-hot vectors was
higher that 0.8. We used the TreeTagger system5to tag the
source sentences with Parts-of-Speech.
3.2. Reference Compressions
Like in (Filippova, 2010; Boudin and Morin, 2013), ref-
erence compressions are edited by human annotators, all
native Portuguese or Spanish speakers, who analyzed the
most relevant facts of a cluster and generated a condensed
sentence of this cluster. We suggested that the annotators
should use the same vocabulary and n-grams as the source
sentences and only select the most relevant information
about the topic. We also recommended that they should
generate compressions that are shorter than the length aver-
age of the source sentences. The following sections provide
details about the Portuguese and Spanish parts of the cor-
pus and, as a matter of comparison, brieÔ¨Çy recalls the main
characteristics of the French corpus built by Boudin and
Morin.
3.2.1. Portuguese Dataset
The Portuguese corpus is composed of 40 clusters. Each
cluster has at least 10 similar sentences by topic and 2 refer-
ence compressions made by 2 human annotators. This cor-
pus contains 17,998 tokens and has a vocabulary of 2,438
tokens. Source sentences have an average of 33.1 tokens
per sentence with a standard deviation of 9.9 tokens. The
Type-Token Ratio (TTR) indicates the reuse of tokens in
the cluster and is deÔ¨Åned by the number of unique tokens
divided by the number of tokens in each cluster; the lower
4The cosine similarity between two vectors uandvassociated
with two sentences is deÔ¨Åned byuv
jjujjjjvjjin the [0,1] range.
5Website: http://www.cis.uni-muenchen.de/
Àúschmid/tools/TreeTagger/
3193Source sentences :
A Tesla fez uma oferta de compra√† empresa de servi√ßos de energia solar SolarCity por mais de 2300 milh Àúoes de euros.
A Tesla Motors , fabricante de carros el√©tricos , anunciou aquisi√ß Àúao da SolarCity por US$ 2,6 bilh Àúoes .
A fabricante de carros el√©tricos e baterias Tesla Motors disse nesta segunda-feira ( 1 ) que chegou a um acordo com a
SolarCity para comprar a instaladora de pain√©is solares por US$ 2,6 bilh Àúoes , em um grande passo do bilion√°rio Elon
Musk para oferecer aos consumidores um neg√≥cio totalmente especializado em energia limpa , informou a Reuters .
Reference compressions :
A Tesla Motors anunciou acordo para comprar a SolarCity por US$ 2,6 bilh Àúoes .
A fabricante Tesla Motors vai adquirir a instaladora de pain√©is solares da SolarCity .
Table 2: Small example of our Portuguese dataset.
the TTR, the greater the reuse of tokens in the cluster. The
sentence similarity represents the average cosine similarity
of the sentences in a cluster. Using these metrics, references
have an average length of 17.8 tokens and a standard devi-
ation of 1.5 tokens, while the Portuguese source corpus has
a TTR of 33.7%. The Portuguese annotators generated the
compressions with a TTR of 67.9% and a sentence similar-
ity of 0.59. Finally, the average compression ratio between
the reference and source sentences is 54%.
3.2.2. Spanish Dataset
The Spanish part is also composed of 40 clusters. It has
30,588 tokens and a vocabulary of 4,390 tokens. Each clus-
ter has 20 similar sentences on the same topic and 4 refer-
ence compressions made by 4 human annotators. Source
sentences have an average of 38.2 tokens per sentence with
a standard deviation of 10.7 tokens and an average TTR of
35.2%. Reference compressions contain the same vocab-
ulary as source sentences while keeping an average size of
23.1 tokens, a standard deviation of 2.4 tokens and a TTR of
43.4%. The sentence similarity between the compressions
is 0.64. The average compression rate is 61%.
3.2.3. French Dataset
We used in the following experiments the French corpus
developed by Boudin and Morin (2013). This corpus also
has 40 clusters composed of 618 sentences (33 tokens on
average). The clusters are composed of 15 sentences on
average and the TTR of the corpus is 38.8%. Reference
compressions have a compression rate of 60%.
4. Experimental Evaluation
We used our corpus to provide a more thorough evalua-
tion of state-of-the-art approaches for MSC than the study
on the French corpus alone. We tested on our dataset a
simple baseline, as well as (Filippova, 2010) and (Boudin
and Morin, 2013) methods. Filippova modeled clusters of
similar sentences as Word Graphs based on the cohesion
of tokens and their Part-of-Speech (PoS). Inspired by the
good results of the Filippova‚Äôs method, Boudin and Morin
used the TextRank method as a re-rank method to analyze
the sentences generated by Filippova‚Äôs method in order to
produce well punctuated and hopefully more informative
compressions. The baseline system creates a Word Graph
(WG) like Filippova‚Äôs method, but this time all arcs have
the same weight. Then, the system generates a compres-
sion represented by the shortest path in the WG that hasat least 8 tokens. Algorithms were implemented using the
Python programming language and the takahe6library.
4.1. Automatic and Manual Metrics
The most important features of MSC are informativeness
and grammaticality. Informativeness is the percentage of
the main information retained in the compression, while
grammaticality analyzes whether a sentence is correct or
not.
References are assumed to contain the most important
information. Thus we calculated informativeness scores
based on the common information between the output of
the MSC system and the references using ROUGE (Lin,
2004). In particular, we used the f-measure metrics
ROUGE-1, ROUGE-2 and ROUGE-SU4. Like in Boudin
and Morin (Boudin and Morin, 2013), ROUGE metrics are
calculated using stop words removal and stemming.7
We also led a manual evaluation with 4 native speakers
for each language. The native speakers of each language
judged the compression in two aspects: informativeness
and grammaticality. In the same way as (Filippova, 2010;
Boudin and Morin, 2013), the native speakers evaluated the
grammaticality in a 3-point scale: 0 point for an ungram-
matical compression, 1 point for compression with minor
mistakes; and 2 points for a correct compression. The in-
formativeness evaluation process is similar for grammati-
cality: 0 point if the compression is not related to the main
topic, 1 point if the compression misses some relevant in-
formation and 2 points if the compression conveys the gist
of the main event.
4.2. Results with Automatic Metrics
Table 3 shows f-score ROUGE scores for the French, Por-
tuguese and Spanish datasets.8Boudin and Morin‚Äôs system
generated better compressions with higher ROUGE scores
than Filippova‚Äôs and the baseline for all datasets.
6Website: http://www.florianboudin.org/
publications.html
7http://snowball.tartarus.org/
8Although we used the same system and data as (Boudin and
Morin, 2013) for the French corpus, we were not able to repro-
duce exactly their results. The ROUGE scores given in their arti-
cle are close to ours for their system: 0.6568 (ROUGE-1), 0.4414
(ROUGE-2) and 0.4344 (ROUGE-SU4), but using Filippova‚Äôs
system we measured higher scores than them: 0.5744 (ROUGE-
1), 0.3921 (ROUGE-2) and 0.3700 (ROUGE-SU4).
3194MethodFrench Portuguese Spanish
RG-1 RG-2 RG-SU4 RG-1 RG-2 RG-SU4 RG-1 RG-2 RG-SU4
Baseline 0.3681 0.1904 0.1758 0.3199 0.1273 0.1309 0.2700 0.0990 0.0984
Filippova (2010) 0.6384 0.4423 0.4297 0.5388 0.2971 0.2938 0.5004 0.2983 0.2847
Boudin and Morin (2013) 0.6674 0.4672 0.4602 0.5532 0.3029 0.2868 0.5140 0.2960 0.2801
Table 3: ROUGE f-scores measured on the French, Portuguese and Spanish datasets. The best ROUGE results are in bold.
Table 4 provides statistics on the length and the compres-
sion ratio of the sentences generated by the systems. The
baseline system output the shortest compressions, which
translated into the worst ROUGE scores. For the three
tested datasets, Filippova‚Äôs method generated shorter com-
pressions with a smaller standard deviation than Boudin
and Morin‚Äôs system. Let us note that for this last system
the lengths of the outputs are less regular across the three
languages.
The Portuguese and Spanish languages derive from Latin
and are closely related languages. However, they differ in
many details of their grammar and lexicon. Moreover, the
datasets produced for the three languages are unlike accord-
ing to several features. First, our corpus contains a smaller
(Portuguese corpus) and a larger (Spanish corpus) dataset
in terms of sentences than the original French corpus. Be-
sides, the compression rates of the three datasets (see Sec-
tion 3.) indicates that the Portuguese source sentences have
more irrelevant tokens. The sentence similarity (Table 1,
last line) describes the variability of sentences in the source
sentences and in the references, and reÔ¨Çects here that the
sentences are slightly more diverse for the Portuguese cor-
pus. It can be noticed that the references are more similar
too each other than source sentences since they only retain
the main information. Finally, the French corpus has a TTR
of 38.8% whereas the Portuguese and Spanish datasets have
TTRs of 33.7% and 35.2%, respectively.
The baseline system generated the shortest compression be-
cause all arcs of the WG have the same weights. However,
this system analyzes neither the grammaticality nor the
most used n-grams in the clusters. Consequently, the base-
line system generated compressions with the worst ROUGE
scores.
4.3. Human Evaluation
ROUGE only analyzes the overlapping between the candi-
date compression and the references. Since this analysis is
not reliable enough, we led a further manual evaluation to
study the informativeness and grammaticality of compres-
sions, as described in Section 4.1.. Given the poor results
of the baseline with ROUGE, we only analyzed the Filip-
pova‚Äôs and Boudin and Morin‚Äôs methods (Table 5).
We measured inter-rater agreement on the judgments we
collected, obtaining values of Fleiss‚Äô kappa of 0.418, of
0.305 and 0.364 for French, Portuguese and Spanish re-
spectively. These results show that human evaluation is
rather subjective. Questioning evaluators on how they pro-
ceed to rate sentences reveals that they often made their
choice by comparing outputs for a given cluster. As the
differences of the grammaticality and the informativeness
scores for the methods are not statistically signiÔ¨Åcant, wemove our investigation on the average and standard de-
viation of the results. Both methods generated compres-
sions of good quality (scores higher than 1) for all datasets,
especially for the French and the Portuguese parts where
scores above 1.5 for grammaticality and above 1.2 for in-
formativeness were obtained. Filippova‚Äôs method gener-
ated more correct compressions (except for the Portuguese
corpus where both methods obtained almost the same re-
sults), which shows that the re-ranking step tends to mod-
erately deteriorate grammaticality. By contrast, Boudin
and Morin‚Äôs method consistently improves informative-
ness, which validates the interest of integrating the anal-
ysis of key phrases inside candidate compressions. This re-
ranking method combines the cohesion score of Filippova
and the relevance of key phrases9to generate more infor-
mative compression. This method selects the path of Word
Graph that has relevant key phrases even if this path has a
lower cohesion quality.
All in all, Boudin and Morin‚Äôs method generated more in-
formative but also longer compressions than Filippova‚Äôs,
CR showing a relative increase of 18% between both sys-
tems (Table 4).</body>
  <conclusion>Multi-Sentence Compression aims to generate a short infor-
mative text summary from several sentences with related
and redundant information. This task can be used in the
domain of multi-document summarization or question an-
swering to provide more informative and concise texts.
In this paper, we presented a new annotated corpus in
two languages that extends the French data made available
in (Boudin and Morin, 2013). We also compared two state-
of-the art systems on this new dataset. We hope this cor-
pus will help the NLP community to develop and validate
multi-language methods for multi-sentence compression.
In order to extend the multi-language resources to more di-
verse languages, we plan to create a similar MSC dataset
for Arabic. We also want to use our corpus to test other
competitive MSC systems, such as the one based on integer
linear programming we introduced in (Linhares Pontes et
al., 2016).</conclusion>
  <discussion>N/A</discussion>
  <biblio>Barzilay, R. and McKeown, K. R. (2005). Sentence fusion
for multidocument news summarization. Computational
Linguistics , 31(3):297‚Äì328, September.
Boudin, F. and Morin, E. (2013). Keyphrase extraction
for N-best reranking in multi-sentence compression. In
NAACL , pages 298‚Äì305.
Clarke, J. and Lapata, M. (2008). Global inference for
sentence compression: An integer linear programming
approach. Journal of ArtiÔ¨Åcial Intelligence Research
(JAIR , 31:399‚Äì429.
Cohn, T. and Lapata, M. (2008). Sentence compression
beyond word deletion. In COLING , pages 137‚Äì144.
Filippova, K. and Altun, Y . (2013). Overcoming the lack of
parallel data in sentence compression. In EMNLP , pages
1481‚Äì1491.
Filippova, K., Alfonseca, E., Colmenares, C. A., Kaiser,
L., and Vinyals, O. (2015). Sentence compression by
deletion with LSTMs. In EMNLP , pages 360‚Äì368.
Filippova, K. (2010). Multi-sentence compression: Find-
ing shortest paths in word graphs. In COLING , pages
322‚Äì330.
Ganitkevitch, J., Callison-Burch, C., Napoles, C., and
Durme, B. V . (2011). Learning sentential paraphrases
from bilingual parallel corpora for text-to-text genera-
tion. In EMNLP , pages 1168‚Äì1179.
Ive, J. and Yvon, F. (2016). Parallel sentence compression.
InCOLING, Technical Papers , page 1503‚Äì1513.
Knight, K. and Marcu, D. (2002). Summarization beyond
sentence extraction: A probabilistic approach to sentence
compression. ArtiÔ¨Åcial Intelligence , 139(1):91‚Äì107.
Lin, C.-Y . (2004). ROUGE: A Package for Automatic
Evaluation of Summaries. In Workshop Text Summariza-
tion Branches Out (ACL‚Äô04) , pages 74‚Äì81.
Linhares Pontes, E., Gouveia da Silva, T., Linhares,
A. C., Torres-Moreno, J.-M., and Huet, S. (2016).
M√©todos de otimiza√ß Àúao combinat√≥ria aplicados ao prob-
lema de compress Àúao multifrases. In Anais do XLVIII
Simp√≥sio Brasileiro de Pesquisa Operacional (SBPO) ,
pages 2278‚Äì2289.
Luong, A. V ., Tran, N. T., Ung, V . G., and Nghiem, M. Q.(2015). Word graph-based multi-sentence compression:
Re-ranking candidates using frequent words. In Sev-
enth International Conference on Knowledge and Sys-
tems Engineering (KSE) , pages 55‚Äì60.
McKeown, K., Rosenthal, S., Thadani, K., and Moore, C.
(2010). Time-efÔ¨Åcient creation of an accurate sentence
fusion corpus. In HLT-NAACL , pages 317‚Äì320.
Rush, A. M., Chopra, S., and Weston, J. (2015). A neural
attention model for abstractive sentence summarization.
InEMNLP , pages 379‚Äì389.
Thadani, K. and McKeown, K. (2013). Supervised sen-
tence fusion with single-stage inference. In Sixth Inter-
national Joint Conference on Natural Language Process-
ing, IJCNLP , pages 1410‚Äì1418.
Toutanova, K., Brockett, C., Tran, K. M., and Amershi,
S. (2016). A dataset and evaluation metrics for abstrac-
tive compression of sentences and short paragraphs. In
EMNLP , pages 340‚Äì350.
8. Language Resource References
Boudin, Florian and Morin, Emmanuel. (2013).
Keyphrase Extraction for N-best Reranking in Multi-
Sentence Compression . NAACL (2013). Available on
https://github.com/boudinÔ¨Ç/lina-msc.
Graff, David and Cieri, Christopher and Kong, Junbo and
Chen, Ke and Maeda, Kazuaki. (2011). English Gi-
gaword . Linguistic Data Consortium, 5th, ISLRN 911-
942-430-413-0.
319</biblio>


  <preamble>On_the_Morality_of_Artificial_Intelligence.pdf</preamble>
  <titre>On the Morality of ArtiÔ¨Åcial Intelligence</titre>
  <auteurs>
    <auteur>
      <name>Alexandra Luccioni</name>
      <mail>N/A</mail>
      <affiliation>Universit√© de Montr√©al, Mila</affiliation>
    </auteur>
    <auteur>
      <name>Yoshua Bengio</name>
      <mail>N/A</mail>
      <affiliation>Universit√© de Montr√©al, Mila</affiliation>
    </auteur>
  </auteurs>
  <abstract>Much of the existing research on the social and ethical impact of ArtiÔ¨Åcial Intelligence has been
focused on deÔ¨Åning ethical principles and guidelines surrounding Machine Learning (ML) and other
ArtiÔ¨Åcial Intelligence (AI) algorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for
helping deÔ¨Åne the appropriate social norms of AI, we believe that it is equally important to discuss both
the potential and risks of ML and to inspire the community to use ML for beneÔ¨Åcial objectives. In the
present article, which is speciÔ¨Åcally aimed at ML practitioners, we thus focus more on the latter, carrying
out an overview of existing high-level ethical frameworks and guidelines, but above all proposing both
conceptual and practical principles and guidelines for ML research and deployment, insisting on concrete
actions that can be taken by practitioners to pursue a more ethical and moral practice of ML aimed at
using AI for social good.</abstract>
  <introduction>N/A</introduction>
  <body>
Progress in ML in the last decade has been extraordinary and has rekindled the notion that AI systems could
eventually reach human levels of performance, which was abandoned for several decades. Even if we are
still currently far from this achievement, technological progress in ML has passed a threshold which enables
it to have a huge economic impact, estimated to be close to 16 trillion US dollars by 2030 [Szczepa ¬¥nski,
2019]. This contrasts with the Ô¨Årst few decades of ML progress, when researchers had the luxury of fo-
cusing purely on the fundamental aspects of their work, not worrying too much about its potential societal
impacts ‚Äì an object recognition algorithm could be tested on a common dataset like MNIST [LeCun et al.,
1998] or ImageNet [Deng et al., 2009], and an objective performance metric would be obtained in order to
measure progress, without having to think about the messiness and complexity of deployment and social
impact. Something crucial has changed in recent years, as algorithms initially developed in the lab are
increasingly being improved and deployed in society, in real-world applications such as healthcare, trans-
portation and industrial production with real-life consequences, and we are likely seeing just the tip of the
iceberg in terms of social impact. Along the way, this deployment in society has forced the realization that
these algorithms have social impacts which could be positive or negative. For example, we have realized
that biases hidden in data and algorithms could lead to more discrimination, in the simplest case simply
because of the data imbalance: facial recognition algorithms have been found to underperform on gender
and racial minorities [Buolamwini and Gebru, 2018]. Furthermore, above and beyond hidden biases, given
the high impact potential of ML research, the question stands of whether practitioners are acting with the
best interests of humanity and society in mind when developing their tools and applications.
As ML researchers and engineers, we believe that we have a shared responsibility to consider both ethics
and moral values when we choose what we work on, for what organization, and whether the products we
contribute to directly or indirectly will be beneÔ¨Åcial to humanity or more likely to end up hurting more
than helping. Unfortunately, very few of us have been trained to think about these questions. Instead,
most of us have focused from a very young age on mathematics and computer science and not so much
on philosophy and other humanities. A good step towards learning about these issues is to consult the
documents proposing ethical guidelines for AI, which we will cover in Section 2. Furthermore, in order
Also CIFAR Senior Fellow
1to offer a guiding direction for such debates and soul-searching within the scope of ML, we propose the
following self-directed questions:
1. How is the technology that I am working on going to be used?
2. Who will beneÔ¨Åt or suffer from it?
3. How much and what social impact will it have?
4. How does my job Ô¨Åt with my values?
We are conscious that the questions listed above are subjective and the answers will depend highly on the
values and ethics of the individual answering them. Nonetheless, we hope that work on some applications,
such as the design and deployment of lethal autonomous weapons and automatic surveillance, will clearly
be seen to contradict fundamental rights and dignity, as deÔ¨Åned in, among other places, the UN Declaration
of Human Rights [1948]. Other applications of ML, such as those increasing the efÔ¨Åciency of advertising or
beating the stock market, are less clear-cut in their moral value, and merit informed debate and discussion
within the scientiÔ¨Åc community and society at large. As some of us become more conscious of the potential
or deÔ¨Ånite social impact of ML, we have the opportunity, if not the duty, to make our voices heard. A good
example of this is a recent letter signed by numerous scientists calling for an international treaty banning
lethal autonomous weapon systems, e.g., killer drones which can decide to shoot at a person without human
involvement, which would make it possible to take the broad social, moral and psychological context into
account and potentially decide to abort the mission (for instance, when the target is in a school or at a family
dinner surrounded by women and children).
Finally, while the legal frameworks to oversee and limit research and development violating these prin-
ciples are often and unfortunately updated in a reactive rather than a proactive manner, we believe that
we should not wait until all of the dots between ML and ethics are formally connected by legislation and
regulation. We believe that we have a responsibility to educate ourselves, to think ahead about potential
consequences, to use our internal moral compasses and to consciously choose the direction of the research
or engineering that we practice. This is important because we believe that we are faced with a wisdom
race: as technology becomes more powerful, its impact can be proportionally greater, either positively or
negatively.
To curb the negative impact, we need to become wiser individually (as reÔ¨Çected in our personal deci-
sions) and collectively (through social norms, laws and regulations). Unfortunately, technological progress
in AI has accelerated faster than the current rate of progress of personal and social wisdom, ultimately mak-
ing it possible for unwise humans or organizations, even those with good intentions and acting legally, to
have large-scale, major destructive effects. This is comparable to a world in which nuclear bombs (i.e. very
powerful technology) were accessible and usable by children (i.e., persons with insufÔ¨Åcient maturity and
wisdom), which could easily result in global nuclear war. This highlights the importance of the discussions
still to be had by large numbers of ML practitioners about ethics and social impact, as well as the safeguards
that need to be put in place to protect especially the most vulnerable members of our society. We will dis-
cuss some of the most advanced efforts to introduce these safeguards in the next section, followed by some
examples of socially beneÔ¨Åcial applications of ML.
2 Ethics and AI - Existing Initiatives
In recent years, there have been numerous initiatives which have taken one of two major approaches to
fostering the ethical practice of AI: (1) Proposing principles guiding the socially responsible development
of AI or (2) Raising concerns about the social impact of AI. We will describe both approaches in the current
section, as well as giving examples of notable initiatives and projects which have adopted either of the
approaches.1
1For a more complete overview of different global ML ethics initiatives, see a recent review in Jobin et al. [2019]
2 2.1 DeÔ¨Åning Principles for Practicing AI Responsibly
The topic of ethical research and practice in technology has been gaining momentum in different corners of
the computing community in recent years, and the various initiatives that have been proposed are indicative
of the interest and the concern that many members share. For instance, in the United States, the Association
for Computing Machinery (ACM) has proposed a Code of Ethics and Professional Conduct, to be followed
by all members of the association and to guide them in their usage of computer science [Gotterbarn et al.,
2018]. A similar initiative was undertaken by the Royal Statistical Society (RSS) in the United Kingdom,
which has created a practical guide for practitioners regarding the ethical use of mathematics [RSS, 2019].
In the present section, we will address the two most relevant and extensive initiatives to establish ethical
guidelines for AI research and practice: the Montreal Declaration for Responsible Development of AI and
the IEEE report for Ethically Aligned Design.
2.1.1 The Montreal Declaration for a Responsible Development of ArtiÔ¨Åcial Intelligence
One of the most notable approaches to establishing guidelines for AI deployment is the Montreal Declara-
tion for a Responsible Development of ArtiÔ¨Åcial Intelligence, developed in 2017 and revised in 2018 based
on public feedback2. It was elaborated under the premise that given the assumption that since AI will even-
tually affect all sectors of society, it requires principles to guide its development to ensure its adherence to
human values and social progress. The resulting Declaration has ten principles, ranging from protection
of privacy to equal representation, with some principles touching responsibility and ethics directly; for in-
stance, the principle of Prudence stipulates that ‚ÄúEvery person involved in AI development must exercise
caution by anticipating, as far as possible, the adverse consequences of AIS [ArtiÔ¨Åcial Intelligent Systems]
use and by taking the appropriate measures to avoid them. ‚Äù These principles were deÔ¨Åned after extensive
debates and dialogue between both specialists and non-specialists from different domains and parts of the
world to ensure representability and cohesion. The overall aim of the declaration was to spark public debate
and to encourage a progressive and inclusive orientation to the development of AI.
However, the Montreal declaration goes further than theoretical ethical principles, proposing recom-
mendations to accomplish an ethical digital transition that includes all of the different levels of society,
from researchers to policy-makers. For instance, it includes a proposition for auditing and validating the
use of AIS using concrete frameworks and certiÔ¨Åcations in order to prevent biases and discrimination. Spe-
ciÔ¨Åc steps were also proposed for ensuring the protection of democracy and reducing the environmental
footprint of AI, all within the framework of a democratic and citizen-led process. This is important given
that the effects of AI will permeate all levels of society, from the programmers and engineers who write
the code, to the leaders who will legislate it, and the businesses who will make products with it that will be
used by all. The process of creation of the Montreal declaration was consequently the keystone to building
a way of including all of these different stakeholders in the elaboration of an ethical AI, and paves the way
for subsequent work on the topic.
2.1.2 IEEE Ethically Aligned Design
A more recent effort, initiated by the IEEE Global Initiative on Ethics of Autonomous and Intelligent Sys-
tems, carried out an in-depth study on the issue of the ethics surrounding the design of AI systems [IEEE,
2017]. In particular, aspects that are relevant to the topics covered in the present paper include: the usage of
A/IS [autonomous and intelligent systems] in service to sustainable development for all, and more specif-
ically for the attainment of the United Nations Sustainable Development Goals (SDGs) [UNHCR, 2017].
The authors of the study speciÔ¨Åcally underline the potential of AI to contribute to resolving some of the
world‚Äôs most urgent problems, such as climate change and poverty, given the necessary will and orientation
towards these problems. Furthermore, they highlight the fact that despite their great potential, current AI
deployment and development is currently not aligned with these goals and impacts [IEEE, 2017, p. 144],
which is unsettling given the myriad of ML project and initiatives worldwide.
The IEEE report also lays down principles to guide ‚Äúthe ethical and values-based design, development,
and implementation of autonomous and intelligent systems‚Äù , many of which are similar to those deÔ¨Åned
2 https://www :montrealdeclaration-responsibleai :com/
3 by the Montreal Declaration: respect of human rights, data agency, transparency, accountability, etc. They
go further in proposing that ‚ÄúA/IS creators shall adopt increased human well-being as a primary success
criterion for development‚Äù instead of focusing on isolated metrics such as accuracy, and, from a deployment
perspective, offering alternative metrics to quantify meaningful progress, for instance by evaluating social,
economic and environmental factors instead of proÔ¨Åt and other common success metrics. The report also
includes propositions for policymakers, legislators and other stakeholders from the extended AI community
and, as such, represents the most extensive effort of establishing ethical boundaries and guidelines for AI
research to date.
In a recent survey of the various global ethics guidelines proposed around AI, the authors observed that
despite a conceptual overlap between the many existing guidelines, including the two mentioned above,
there are major differences regarding how the principles are interpreted [Jobin et al., 2019]. This underlines
the complexity and nuance of applying theoretical, philosophical principles in practice, and raises questions
such as: what aspects of the AI research and deployment pipeline do ethics principles affect? how would it
be possible to resolve conÔ¨Çicts between, for instance, fairness and sustainability (i.e. training an algorithm
longer and with more data - thus potentially leading to more greenhouse gas emissions - to ensure that it
is not discriminatory and covers all demographic groups equally well)? And, above all, how is it possible
to translate ethical principles into a programming language? In any case, the bridge between theory and
practice has yet to be built and there are different ways in which that can happen. This underlines the
necessity of involving actors from different levels of the AI ecosystem (and neighboring ones) in order to
ensure that experts in policy-making work in tandem with experts in coding and engineering to create tools
and frameworks that are coherent and usable by all.
2.2 Identifying Ethical Concerns of AI Applications
There are several types of ethical concerns regarding AI applications and, in this paper, we will focus more
concretely on bias leading to potential discrimination. While it is true that on the one hand, AI-infused
technology such as computer vision can enhance public security, for instance by identifying crime in real-
time based on CCTV cameras, but the trade-off is that can also be abused to track individuals and to establish
a surveillance state where privacy is greatly threatened by those who control the technology. On the military
side, similar technology can be used to design autonomous drones which use computer vision to identity
their target, representing a grave threat to global security and democracy due to the lack of human oversight.
In addition to the security risk, such weapons would be moral and legal hazard: AI technology is not yet
capable to comprehend and represent the social and psychological context in which such a targeted attack
could take place in a manner that is coherent with international laws regarding war as well as with human
morality.
Unfortunately, the most common argument brought in favour of developing lethal autonomous weapons
is that they are needed as a precautionary measure (i.e. since other countries are undeniably working on
them, each country needs to do the same). In reality, the weapons needed to defend against killer drones
would be very different from the drones themselves, and do not need to be lethal autonomous weapons since
they would be designed to destroy weapons rather than to target people, similar to the Iron Dome used by
Israel. Another common argument is that an international treaty would be useless since some countries will
refuse to sign it. But we have seen in the past that even when major powers do not sign a treaty (such as
the one on anti-personnel mines, signed by 133 countries, excluding the US, in 1997), the treaty can still be
used to create a moral stigma, as well as a decline in demand; in the case of anti-personnel mines, the result
has been that U.S. companies have stopped building them, even though their government never signed the
treaty. Another Ô¨Çawed argument is that regulating lethal autonomous weapons could threaten the innovation
in AI, whereas in fact AI has been developed very successfully in a civilian setting (mostly in academia and
major technology companies) and its continued development does not require neither data nor engineering
which would come from AI military development.
Another potential threat to democracy stemming from AI could come not simply from the increased
ability to monitor and to target individuals, but also from the more subtle power to inÔ¨Çuence them, e.g. via
AI-driven advertising, automated online trolls and other psychological manipulations via the internet and
social media. The recent use of AI to inÔ¨Çuence political campaigns such as the 2016 US election or Brexit
is just the beginning of what can be done when machines learn how to ‚Äúpress our buttons‚Äù in a personalized
way. This is due to the fact that micro-targeting makes it possible for ads to be truly bespoke depending
4 on your political views, network of friends and personal history. While we may not mind being inÔ¨Çuenced
when it comes to choosing a brand of soft drinks, when the proÔ¨Åt or power motives of a corporation or
political organization go against our individual and collective interests, it becomes important to establish
social norms, laws and regulations to protect us from such psychological manipulation. But where should
the line be drawn between, for example, manipulation and education? These are difÔ¨Åcult questions but
there are clues which can be used (like whether the organization that stands to proÔ¨Åt is paying for the
advertisement or social network inÔ¨Çuence), so human judgement remains key for judging the ethical aspect,
e.g. in balancing different values (like autonomy vs well-being, when considering an ad campaign against
cigarettes, for example). In the case of advertising, what is interesting is that in addition to the moral hazard
associated with psychological manipulation, it is not even clear that advertising is beneÔ¨Åcial to society from
a purely economic perspective, as it tends to favour established brands and thus slow down innovation.
Closely related to the political misuse and manipulation with AI is also increasing concern about AI-
generated false images, videos and news. Thanks to rapid progress in generative neural networks such as
the GANs [Goodfellow et al., 2014], it is becoming possible to synthesize images and sounds in a controlled
way, e.g., using ‚Äúdeep fakes‚Äù for making a video of a president declaring war, or with the face of a celebrity
seamlessly integrated on the body and behavior of a pornography actor. Other commonly discussed concerns
of AI deployment include the effect on the job market [Perisic, 2018], which means that governments and
communities must prepare, e.g. by adapting the education system and the social safety net, which can
take decades, as well as the potential concentration of power which it may lead to in speciÔ¨Åc individuals,
corporations and countries, and the bias and discrimination it may contribute to increase, as we discuss next.
2.2.1 Identifying and Mitigating Bias
In recent years, we have been confronted numerous times with the fact that biased algorithmic systems can
perpetuate injustice and discrimination, whether we are aware of it or not. There are many different ways
that this kind of bias can creep into algorithms: it can be from the data itself, or the implicit bias that the
creator programmed into the system, and even the way the problem is framed3. Therefore, in order to ensure
that the models that we develop and the systems that they are later used in are as fair and ethical as possible,
there are steps to take to identify bias and to reduce it as much as possible.
Numerical Bias
A major challenge in designing ML systems is understanding how they work during training and deploy-
ment, and what factors and features they use to make decisions. However, diagnosing the presence of bias
in these systems is not a straightforward task, since it is not always obvious during a model‚Äôs construction
what the downstream impacts of design choices may be; therefore, upstream efforts are needed to reduce
this risk as much as possible. To this end, there have been several proposals to help practitioners identify
and mitigate bias in ML models, some of which we will describe in the current section.
More concretely, exploring, analyzing and visualizing the data used for training a model is a key part
of the ML process. But it is not straightforward to identify bias simply by looking at the data; often, more
in-depth probing is needed to Ô¨Ågure out what features and implicit information is present and, once a model
is developed, how this will inÔ¨Çuence the model‚Äôs behavior. For instance, it was recently found that the
COMPAS system, a criminal risk assessment tool developed widely used in the United States, is often
biased with respect to race [Angwin et al., 2016]. Whereas the bias in the COMPAS system was identiÔ¨Åed
after its deployment, once the data was made public, this bias is an aspect of the model that should have been
identiÔ¨Åed much earlier, during development and certainly before deployment. Similarly, off-the-shelf facial
recognition technology used by police forces has been shown to perform much worse on racial and gender
minorities, with a difference of up to 34.4% in error rate between lighter-skinned males and darker-skinned
females, mostly due to the lack of reliable training data [Buolamwini and Gebru, 2018].
To address these types of issues, several approaches exist: for instance, researchers have recently re-
leased a tool called ‚ÄòWhat-If‚Äô, an open-source application that lets practitioners not only visualize their
data, but also test the performance of their ML model in hypothetical situations, for instance modifying
3 For a more hands-on presentation of bias and fairness in AI, we suggest Google‚Äôs Online course designed speciÔ¨Åcally for ML
practitioners
5 some characteristics of data points and analyzing subsequent model behavior, by measuring fairness met-
rics such as Equal Opportunity and Demographic Parity [Wexler et al., 2019]. Other approaches address
bias by changing the training procedure or the structure of ML models themselves, for instance by trans-
forming the raw data in a space in which discriminatory information cannot be found [Zemel et al., 2013]
or using a variational autoencoder to learn the latent structure from the dataset and using this structure to
re-weight the importance of speciÔ¨Åc data points during model training [Ribeiro et al., 2016]. Whatever the
approach chosen, using these kinds of tools during ML model development and deployment can change the
life of individual people, who could go from unfairly spending decades in prison to having the chance of a
better life ‚Äì an immensely important difference when multiplied by the thousands of people whose lives can
be affected by the deployment of these tools. This multiplication of bias is especially important to consider
since ML is being used more and more, and therefore even edge cases and small minorities can be ampliÔ¨Åed
in real-world applications.
Textual Bias
Bias is not always in numbers, it can also manifest itself in the words that we use to describe the
world around us. For instance, in 2018, Reuters reported that Amazon was forced to decommission an
ML-powered recruiting engine when it was discovered that it penalized any mention of female-related
vocabulary, including applicants who attended all-women colleges [Dastin, 2018]. This is not surprising
given the gender disparity that exists in the technology sector and since the data used to develop this tool
was comprised of resumes submitted (and accepted) to Amazon over a 10-year period. It is nonetheless
disturbing in terms of algorithmic fairness, especially if algorithms such as this one make Ô¨Åltering or hiring
decisions that can ultimately affect an entire gender‚Äôs lives and careers. This can potentially create a negative
feedback loop, as such a system would reduce the number of female workers and thus the number of
positive role models for girls interested in technology. A similar type of gender bias was also found in
pretrained word embedding models, which were found to exhibit gender stereotypes in terms of higher
cosine similarity between, for instance, ‚Äòwoman‚Äô and ‚Äòhomemaker‚Äô or ‚Äòreceptionist‚Äô as opposed to ‚Äòwoman‚Äô
and ‚Äòdoctor‚Äô or ‚Äòlawyer‚Äô, notably due to these biases existing in the corpus that they were trained on, which
consisted of mainstream news articles [Bolukbasi et al., 2016].
In order to reduce and eventually remove gender bias in written text, researchers have proposed ap-
proaches such as identifying the gender subspace of vectors and adjusting the dimensions in a way that
either neutralizes or entirely removes gender bias [Bolukbasi et al., 2016]. Others have deÔ¨Åned a formal
gender bias taxonomy in order to capture gender bias and to train ML models to later identify this bias in
texts [Hitti et al., 2019]. Debiasing the computational representation of language, notably word embedding
models, is especially important because of the extent of their usage; pretrained embedding models trained
on corpora such as Google News and the Common Crawl are used in a variety of applications and systems,
and can therefore continue perpetuating gender bias in downstream usages in Natural Language Processing
(NLP) applications such as dialogue systems. This is a challenge given the complex and sub-symbolic na-
ture of modern NLP, which makes it difÔ¨Åcult to analyze speciÔ¨Åc features and aspects of data and identify
latent connections and bias between words and concepts. Therefore, more work is needed to explore and
analyse these issues, which constitutes an interesting research direction in itself, and one that is important
to pursue and to integrate into mainstream ML research.
Despite the research initiatives described above to carve appropriate social norms about AI, there re-
mains a noticeable gap between the recommendations they make and ways to ensure that these are respected.
Legislation of AI is still catching up to the progress made in research and practice, and there have not yet
been any country-level laws governing AI research speciÔ¨Åcally. However, there have been, on the one hand,
more high-level legislative frameworks such as the European Union (EU) General Data Protection Regula-
tion (GDPR), which aims to ensure data privacy and protection and, on the other hand, more local initiatives
such as San Francisco‚Äôs Facial Recognition Software Ban. Nonetheless, more complete legal frameworks
are needed to control nefarious use of AI and to ensure that the principles deÔ¨Åned in theory are applied and
enforced in practice.
6 3 AI for Good Initiatives
Whereas the proÔ¨Åt motive is the main driver behind much of the commercial deployment of AI today, there
are nonetheless many projects going on in academia, government organizations, civil society and industry
labs motivated by more noble objectives, often called AI for Social Good (AISG) projects. In addition
to the speciÔ¨Åc projects being undertaken in areas such as healthcare, education or the environment, it is
interesting to highlight higher-level efforts which aim to foster and facilitate these projects. For example,
the AI Commons projec aims to construct a hub where different kinds of actors can connect and collaborate
on AISG projects, e.g., ML graduate students or engineers, problem owners in NGOs or local governments,
philanthropy organizations, or startups which could deploy the ML solutions. Their interaction is to be
facilitated by online tools and datasets as well as a standardized description of the status, progress and
expected impact of each project. We hope that initiatives like this will help solidify and amplify the impact
of AISG; in the meantime, there are also many profoundly positive uses of AI that are emerging and we
would like to highlight and applaud such efforts in the present section.
3.1 AI in Healthcare
Achieving universal health coverage is one of the 17 UN Sustainable Development Goals [UNHCR, 2017]
and although major progress has been made in numerous domains, such as maternal health as well as
HIV/AIDS reduction, there are still many problems that are far from being solved. While ML is not a cure-
all, there are many challenges that it can help with such as personalized medicine, diagnosis of medical
imagery, and improved drug discovery [Ghassemi et al., 2018]. ML in the health sector is in fact a thriving
domain of research, with its own workshops at major ML conferences and research published in major
medical journals read by practitioners worldwide. In the last Ô¨Åve years alone, groundbreaking work has
been done in improving the diagnosis of diabetic retinopathy from a single visit [Arcadu et al., 2019],
detecting breast cancer in lymph nodes [Golden, 2017] and large-scale discovery of diseases based on health
records [Pivovarov et al., 2015]. There is also an increasing number of startups and companies working in
the space, either by commercializing research done in academia or by developing products speciÔ¨Åcally
catered to the medical sector, with the most advanced applications harnessing the power of deep learning
for analyzing and classifying medical imagery.
Despite the many exciting advances that are being made, there are many hurdles in ML research in
healthcare, starting from data privacy and control (who owns the data? Can patients share their own data, or
should the process be centralized? How to Ô¨Ånd the right balance between privacy and the lives which will
be saved by applying ML on the aggregated health records from many different sources?), to the manner
in which medical data should be processed (Should it contain information such as race and postal/zip code,
which can impact diagnoses, be included in electronic heath records, or does that open the door to discrim-
ination and bias?) and how should such systems be deployed (human-in-the-loop or fully automated?)4.
There are also often questions of responsibility and interpretability that arise, given the high stakes of de-
ploying ML systems in situations of life and death. In order to make meaningful progress in this sector, it is
therefore important to continue existing research on fair and ethical usage of ML in healthcare [Wiens et al.,
2019] and to ensure that Hippocratic principles are a solid part of the research and development process,
as well as working with stakeholders of the domain (e.g. radiologists, clinicians, patient organizations and
hospital administrators) to propose solutions to the hurdles proposed above.
3.2 AI for Education
The promise of using adaptive intelligent systems and agents for education has been around since the
1960s [Suppes and Morningstar, 1969], but access to personalized digital education tools has yet to be-
come a reality in most countries, especially in the developing world, where it could have the most impact
to democratize education and knowledge [Nye, 2015]. In recent years, given the increasing global shortage
of qualiÔ¨Åed teachers along with the increasing number of students, the issue of access to education has
become a global one, a fact highlighted by its presence among the UN SDGs. And yet, the usage of ML
in the education sector has been limited to speciÔ¨Åc, narrow applications such as predicting the probability
4 For a more extensive overview of the opportunities and challenges of using ML in healthcare, see Ghassemi et al. [2018]
7 of learner attrition [Chaplot et al., 2015] or improving learner evaluation [Abbott, 2006]. There are many
reasons for this, starting from the difÔ¨Åculty in representing learning content in a domain-agnostic way to
facilitate scalability, to overcoming cultural and linguistic barriers to deploying tutors worldwide, but also
more fundamental issues such as the lack of large-scale educational datasets and the inherent technological
constraints in developing countries.
Despite these hurdles, there are many new and longstanding efforts to create intelligent tutors, be it using
symbolic AI approaches such as ontologies and knowledge modeling [Nkambou et al., 2010], educational
data mining [Dutt et al., 2017] or, more recently, ML-driven approaches [Conati et al., 2018]. However,
there are very high stakes in the Ô¨Åeld, since technological interventions have the potential to make consider-
able, long-term impact on human livelihoods, for example lifting people out of poverty by endowing them
with linguistic and numerical literacy, but these can be hindered by bias and technological constraints. We
therefore agree with recent proposals to improve and support human learning at scale and believe that ML
has a key role to play in this endeavour. This can be done, for instance, by partnering up with existing
education initiatives and organizations in order to learn what their speciÔ¨Åc needs are and how ML can be
used to meet them, or else by collaborating with Massive Open Online Course (MOOC) creators in order
to gather data and make it available to the ML community, and Ô¨Ånally by sharing learning materials and
activities used in local education initiatives (e.g. university courses in Machine Learning) so that they proÔ¨Åt
learners in places where access to high-quality technical education is limited.
3.3 AI for the Environment
Climate change is, without a doubt, one of the biggest challenges that humanity has faced, and we are at
an important point in history when we are both aware of the issue and still have the possibility to change
its course. Climate change has been described as a ‚Äòwicked‚Äô problem, due to features such as the difÔ¨Åculty
in deÔ¨Åning the problem itself and in developing and deploying solutions to it, the lack of central authority
that can solve it, the incentives for individual countries or companies to not do their share, and the cognitive
biases that discount the future impacts of our actions [Head et al., 2008, Levin et al., 2012]. Furthermore,
while we do not know of any single technological silver bullet as solution to climate change, there are
nonetheless numerous technical challenges for which ML can be helpful, and which can be combined to
make a signiÔ¨Åcant impact on the overall issue. These challenges and the ongoing ML approaches to tackle
them were presented in a recent survey paper [Rolnick et al., 2019]. We will not go into all of these at
length, but we will focus on a few examples that are particularly salient and that we hope will give an idea
of both the relevance of deploying ML in environmental applications and the opportunities that this can
generate.
Energy and Transportation
Together, electricity and transportation systems are estimated to produce close to half of anthropogenic
greenhouse gas (GHG) emissions [Allen et al., 2019] and both sectors have their own unique challenges for
decarbonization. For instance, one of the major obstacles to building and using renewable energy sources
such as solar and wind is the variability of their output, which is inherently problematic since the power
generated by an energy grid must equal the power used by its consumers at any given moment. Currently,
this means that despite the existence of solar panels and wind turbines, these must be complemented by
controllable but highly polluting energy sources such as coal and natural gas plants. ML methods that are
appropriate for time-series predictions, such as Recurrent Neural Networks are particularly suited for these
types of tasks [V oyant et al., 2017] and can dramatically lower the barrier to entry for renewable energy
globally. Furthermore, even in cases where controllable energy sources are used, demand on the energy
grid will still Ô¨Çuctuate based on usage; in this case, ML techniques such as Reinforcement Learning and
Dynamic Scheduling can be used to balance the grid in real time [V√°zquez-Canteli and Nagy, 2019].
In transportation, reducing activity is a key part in reducing GHG emissions; however, given the highly
regional nature of transportation methods (i.e. high-speed trains are only an option in Europe, whereas
many major US cities have limited public transportation), custom solutions are needed to make a signiÔ¨Åcant
impact. ML can be of particular help in estimating and predicting vehicle Ô¨Çow to minimize it, for example
by helping to optimize the design of new roads and hubs [Sommer et al., 2017] and monitoring trafÔ¨Åc [Kaack
8 et al., 2019], as well as estimating carbon emissions in real-time [Nocera et al., 2018]. ML can also be used
for designing more energy-efÔ¨Åcient batteries [Hoffmann et al., 2019] which will become an increasingly
important concern as more people switch to electric vehicles. In both cases of energy and transportation,
ML can be used to make systems more efÔ¨Åcient and to improve predictions of complex phenomena based
on large amounts of data; nonetheless, it remains only one part of the solution, and as tempting as it is to halt
research projects once a theoretically plausible solution has been found (and a research paper published),
what is key here is working with domain experts to bring projects towards deployment, where concrete
impact can be made. Transversal connections between disciplines are therefore key, and must be established
and fostered for projects to Ô¨Çourish.
Individuals and Societies
While changes in our climate can be abstract, quantiÔ¨Åed in degrees of warming or tons of CO2, climate
change will also have very concrete impacts on society, for instance by decreasing crop yield, increasing
the frequency of extreme weather events such as hurricanes and storms, and impacting biodiversity. There
are a myriad of ways in which ML can help face these, whether it be by analyzing real-time images and
recordings of ecosystems to detect species [Duhart et al., 2018] and deforestation [McDowell et al., 2015],
improving disaster preparation and response by generating real-time maps from satellite imagery [V oigt
et al., 2007] and even setting an optimal price on carbon to accelerate the transition to a low-carbon energy
economy [Wei et al., 2018]. Finally, while we are far from being able to predict the exact impact that
increasing the carbon tax will have on the different levels of society and industry (i.e. federal and regional
governments, local and international companies, and individuals), this is a worthwhile area of research and
exploration, with potentially huge consequences in helping political leaders make more informed choices
in addressing the climate crisis. It is therefore useful to continue gathering data and building trust between
members of the political ecosystem and ML practitioners to learn from each other and to facilitate the
deployment of technological solutions in setting government policies.
On an individual level, there are many reasons why individuals cannot, or will not, act on climate change,
either common misconceptions regarding the fact that individuals cannot make meaningful impact on a
global problem, or cognitive biases that increase an individual‚Äôs psychological distance to climate change.
In the Ô¨Årst case, ML-infused tools to estimate the carbon footprint of individuals and households [Jones and
Kammen, 2011] and to model individual behavior with regards to sustainable lifestyle choices and technolo-
gies [Carr-Cornish et al., 2011] can be very useful if they are sufÔ¨Åciently accurate and deployed on a large
scale. Finally, minimizing psychological distance to the future effects of climate change is a promising way
to reduce cognitive bias ‚Äì in this regard, it is possible to use images generated using Generative Adversarial
Networks (GANs) which represent the impacts of extreme events on locations that have personal value to
the viewer [Schmidt et al., 2019]. A crucial part of developing ML tools for individuals is, once again,
working with multidisciplinary experts in psychology, scientiÔ¨Åc communication, and user design to ensure
that the tools created reach the largest possible audience and maximize their positive impact.</body>
  <conclusion>Technology in general, and ML more speciÔ¨Åcally, carries a great potential for change and disruption. While
neither of these is guaranteed to make the world a better place, this potential can most deÔ¨Ånitely be used
to have a positive impact on the world. In the present article, we have illustrated some inspiring projects
that aim to make the world a better place and by using the powerful techniques and approaches that ML has
brought forward. We believe that as ML researchers and practitioners, we have the responsibility to leverage
our (super)powers to contribute to these efforts. This can be done by connecting with established actors from
industry and policy or experts from other relevant disciplines, by learning from their past experiences, and
by working together to propose innovative solutions to major problems, deployed in places where they will
have a positive impact.
We live in a world with many global and local challenges and issues that are in constant evolution, and
it is easy to be overwhelmed by this Ô¨Çux of information and focus on a small sandbox in which we feel
safe and in control, in order to develop and study the aspects of ML that interest us most. But it is naive
9 to believe that our sandbox is an isolated isle that is not connected to the rest of the world ‚Äì since even in
the case of theoretical work, communication and cross-pollination are unavoidable ‚Äì and each of us is also
a citizen who is concerned collective debates, while many of us could worry about the world in which our
descendants will live. We believe that there are thought processes that should take place in the head of every
ML practitioner regarding the nature of the work they are doing and the potential pitfalls and impacts of
this work in the world around them, some of which we have listed in the Ô¨Årst part of the current paper. And
while we do not claim to have all the answers to all of these tough questions, we hope that we can start a
conversation that will accompany ML research and practice throughout its infancy towards its tumultuous
teenage years in the coming decades, and eventually towards mature adulthood beyond that.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Robert G Abbott. Automated expert modeling for automated student evaluation. In International Confer-
ence on Intelligent Tutoring Systems , pages 1‚Äì10. Springer, 2006.
M Allen, P Antwi-Agyei, F Aragon-Durand, M Babiker, P Bertoldi, M Bind, S Brown, M Buckeridge,
I Camilloni, A Cartwright, et al. Technical summary: Global warming of 1.5c. an ipcc special report
on the impacts of global warming of 1.5c above pre-industrial levels and related global greenhouse gas
emission pathways, in the context of strengthening the global response to the threat of climate change,
sustainable development, and efforts to eradicate poverty, 2019.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias, propub-
lica. https://www :propublica :org/article/machine-bias-risk-assessments-
in-criminal-sentencing , 2016. Accessed: 2019-11-25.
Filippo Arcadu, Fethallah Benmansour, Andreas Maunz, Jeff Willis, Zdenka Haskova, and Marco Prunotto.
Deep learning algorithm predicts diabetic retinopathy progression in individual patients. NPJ digital
medicine , 2(1):1‚Äì9, 2019.
UN General Assembly. Universal declaration of human rights. UN General Assembly , 302(2), 1948.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to
computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural
information processing systems , pages 4349‚Äì4357, 2016.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gen-
der classiÔ¨Åcation. In Conference on fairness, accountability and transparency , pages 77‚Äì91, 2018.
Simone Carr-Cornish, Peta Ashworth, John Gardner, and Stephen J Fraser. Exploring the orientations
which characterise the likely public acceptance of low emission energy technologies. Climatic change ,
107(3-4):549‚Äì565, 2011.
Devendra Singh Chaplot, Eunhee Rhim, and Jihie Kim. Predicting student attrition in moocs using senti-
ment analysis and neural networks. In AIED Workshops , volume 53, pages 54‚Äì57, 2015.
Cristina Conati, Kaska Porayska-Pomsta, and Manolis Mavrikis. Ai in education needs interpretable ma-
chine learning: Lessons from open learner modelling. arXiv preprint arXiv:1807.00154 , 2018.
Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women, reuters business
news. https://www :reuters :com/article/us-amazon-com-jobs-automation-
insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-
against-women-idUSKCN1MK08G , 2018. Accessed: 2019-11-25.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248‚Äì255.
Ieee, 2009.
Clement Duhart, Gershon Dublon, Brian Mayton, and Joseph Paradiso. Deep learning locally trained
wildlife sensing in real acoustic wetland environment. In International Symposium on Signal Processing
and Intelligent Recognition Systems , pages 3‚Äì14. Springer, 2018.
10 Ashish Dutt, Maizatul Akmar Ismail, and Tutut Herawan. A systematic review on educational data mining.
IEEE Access , 5:15991‚Äì16005, 2017.
Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, and Rajesh Ranganath. Opportu-
nities in machine learning for healthcare. arXiv preprint arXiv:1806.00388 , 2018.
Jeffrey Alan Golden. Deep learning algorithms for detection of lymph node metastases from breast cancer:
helping artiÔ¨Åcial intelligence be seen. Jama , 318(22):2184‚Äì2186, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems , pages 2672‚Äì2680, 2014.
Don W Gotterbarn, Amy Bruckman, Catherine Flick, Keith Miller, and Marty J Wolf. Acm code of ethics:
a guide for positive action, 2018.
Brian W Head et al. Wicked problems in public policy. Public policy , 3(2):101, 2008.
Yasmeen Hitti, Eunbee Jang, Ines Moreno, and Carolyne Pelletier. Proposed taxonomy for gender bias in
text; a Ô¨Åltering methodology for the gender generalization subtype. In Proceedings of the First Workshop
on Gender Bias in Natural Language Processing , pages 8‚Äì17, 2019.
Jordan Hoffmann, Louis Maestrati, Yoshihide Sawada, Jian Tang, Jean Michel Sellier, and Yoshua Bengio.
Data-driven approach to encoding and decoding 3-d crystal structures. arXiv preprint arXiv:1909.00949 ,
2019.
IEEE. Ieee standard review ‚Äî ethically aligned design: A vision for prioritizing human wellbeing with
artiÔ¨Åcial intelligence and autonomous systems. In 2017 IEEE Canada International Humanitarian Tech-
nology Conference (IHTC) , pages 197‚Äì201. IEEE, 2017.
Anna Jobin, Marcello Ienca, and Effy Vayena. ArtiÔ¨Åcial intelligence: the global landscape of ethics guide-
lines. arXiv preprint arXiv:1906.11668 , 2019.
Christopher M Jones and Daniel M Kammen. Quantifying carbon footprint reduction opportunities for us
households and communities. Environmental science &amp; technology , 45(9):4088‚Äì4095, 2011.
Lynn H Kaack, George H Chen, and M Granger Morgan. Truck trafÔ¨Åc monitoring with satellite images. In
Proceedings of the Conference on Computing &amp; Sustainable Societies , pages 155‚Äì164. ACM, 2019.
Yann LeCun, L√©on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to
document recognition. Proceedings of the IEEE , 86(11):2278‚Äì2324, 1998.
Kelly Levin, Benjamin Cashore, Steven Bernstein, and Graeme Auld. Overcoming the tragedy of super
wicked problems: constraining our future selves to ameliorate global climate change. Policy sciences , 45
(2):123‚Äì152, 2012.
Nate G McDowell, Nicholas C Coops, Pieter SA Beck, Jeffrey Q Chambers, Chandana Gangodagamage,
Jeffrey A Hicke, Cho-ying Huang, Robert Kennedy, Dan J Krofcheck, Marcy Litvak, et al. Global
satellite monitoring of climate-induced vegetation disturbances. Trends in plant science , 20(2):114‚Äì123,
2015.
Roger Nkambou, Riichiro Mizoguchi, and Jacqueline Bourdeau. Advances in intelligent tutoring systems ,
volume 308. Springer Science &amp; Business Media, 2010.
Silvio Nocera, Cayetano Ruiz-Alarc√≥n-Quintero, and Federico Cavallaro. Assessing carbon emissions from
road transport through trafÔ¨Åc Ô¨Çow estimators. Transportation Research Part C: Emerging Technologies ,
95:125‚Äì148, 2018.
Benjamin D Nye. Intelligent tutoring systems by and for the developing world: A review of trends and
approaches for educational technology in a global context. International Journal of ArtiÔ¨Åcial Intelligence
in Education , 25(2):177‚Äì203, 2015.
11 I Perisic. How artiÔ¨Åcial intelligence is shaking up the job market. https://www :weforum :org/
agenda/2018/09/artificial-intelligence-shaking-up-job-market/ , 2018. Ac-
cessed: 2019-11-25.
Rimma Pivovarov, Adler J Perotte, Edouard Grave, John Angiolillo, Chris H Wiggins, and No√©mie Elhadad.
Learning probabilistic phenotypes from heterogeneous ehr data. Journal of biomedical informatics , 58:
156‚Äì165, 2015.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predic-
tions of any classiÔ¨Åer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining , pages 1135‚Äì1144. ACM, 2016.
David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, An-
drew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luc-
cioni, et al. Tackling climate change with machine learning. arXiv preprint arXiv:1906.05433 , 2019.
RSS. A guide for ethical data science. https://www :actuaries :org :uk/system/files/
field/document/An%20Ethical%20Charter%20for%20Date%20Science%20WEB%
20FINAL :PDF :, 2019. Accessed: 2019-11-25.
Victor Schmidt, Alexandra Luccioni, S Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer
Chayes, and Yoshua Bengio. Visualizing the consequences of climate change using cycle-consistent
adversarial networks. arXiv preprint arXiv:1905.03709 , 2019.
Lars Wilko Sommer, Tobias Schuchert, and J√ºrgen Beyerer. Fast deep vehicle detection in aerial images.
In2017 IEEE Winter Conference on Applications of Computer Vision (WACV) , pages 311‚Äì319. IEEE,
2017.
Patrick Suppes and Mona Morningstar. Computer-assisted instruction. Science , 166(3903):343‚Äì350, 1969.
M Szczepa ¬¥nski. Economic impacts of artiÔ¨Åcial intelligence (ai), european parliamentary research ser-
vice, pe 637.967. http://europarl :europa :eu/RegData/etudes/BRIE/2019/637967/
EPRS_BRI(2019)637967_EN :pdf, 2019. Accessed: 2019-11-25.
UNHCR. The sustainable development goals and addressing statelessness. https://
www :refworld :org/docid/58b6e3364 :html , 2017. Accessed: 2019-11-25.
Jos√© R V√°zquez-Canteli and Zolt√°n Nagy. Reinforcement learning for demand response: A review of
algorithms and modeling techniques. Applied energy , 235:1072‚Äì1089, 2019.
Stefan V oigt, Thomas Kemper, Torsten Riedlinger, Ralph KieÔ¨Ç, Klaas Scholte, and Harald Mehl. Satellite
image analysis for disaster and crisis-management support. IEEE transactions on geoscience and remote
sensing , 45(6):1520‚Äì1528, 2007.
Cyril V oyant, Gilles Notton, Soteris Kalogirou, Marie-Laure Nivet, Christophe Paoli, Fabrice Motte, and
Alexis Fouilloy. Machine learning methods for solar radiation forecasting: A review. Renewable Energy ,
105:569‚Äì582, 2017.
Sun Wei, Zhang Chongchong, and Sun Cuiping. Carbon pricing prediction based on wavelet transform
and k-elm optimized by bat optimization algorithm in china ets: the case of shanghai and hubei carbon
markets. Carbon Management , 9(6):605‚Äì617, 2018.
James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Vi√©gas, and Jimbo Wil-
son. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualiza-
tion and computer graphics , 26(1):56‚Äì65, 2019.
Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez, Kenneth
Jung, Katherine Heller, David Kale, Mohammed Saeed, et al. Do no harm: a roadmap for responsible
machine learning for health care. Nature medicine , 25(9):1337‚Äì1340, 2019.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In
International Conference on Machine Learning , pages 325‚Äì333, 2013.
</biblio>


  <preamble>surveyTermExtraction.pdf</preamble>
  <titre>AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT
SYSTEMS</titre>
  <auteurs>
    <auteur>
        <name>M. Teresa Cabr√© Castellv√≠</name>
        <mail>teresa.cabre@trad.upf.es</mail>
        <affiliation>Institut Universitari de Ling√º√≠stica Aplicada, Universitat Pompeu Fabra</affiliation>
    </auteur>
    <auteur>
        <name>Rosa Estop√† Bagot</name>
        <mail>rosa.estopa@trad.upf.es</mail>
        <affiliation>Institut Universitari de Ling√º√≠stica Aplicada, Universitat Pompeu Fabra</affiliation>
    </auteur>
    <auteur>
        <name>Jordi Vivaldi Palatresi</name>
        <mail>jorge.vivaldi@info.upf.es</mail>
        <affiliation>Institut Universitari de Ling√º√≠stica Aplicada, Universitat Pompeu Fabra</affiliation>
    </auteur>
  </auteurs>
  <abstract>In this paper we account for the main  characteristics and performance of a
number of recently developed term extraction systems. The analysed tools
represent the main strategies followed by  researchers in this area. All systems
are analysed and compared against a set of technically relevant characteristics.
</abstract>
  <introduction>
In the late 80s there was an acute need , from different disciplines and goals, to
automatically extract terminological units from specialised texts. In the 90s large
computerised textual corpora have been constructed resulting in the first programs for terminology extraction
1 (henceforth TE) which have showed
encouraging results.
Throughout the current decade computa tional linguists, applied linguists,
translators, interpreters, scientific journalists and computer engineers have been
interested in automatically isolating terminology from texts. There are many
goals that have led these different profe ssional groups to design software tools so
as to directly extract terminology from texts: building of glossaries, vocabularies
and terminological dictionaries; text indexing; automatic translation; building of

* In Bourigault, D.; Jacquemin, C.; L‚ÄôHomme, M-C. (2001) Recent Advances in Computational
Terminology , 53-88.
1 In order to give a broa der view of TE we use both extractor  and detector  to refer to the same notion.
However, we are aware of the fact that some schol ars attribute different meanings to these words. 2 Aut omatic Term Detection: a Review of C urrent  Systems
knowledge databases; construction of hype rtext systems; construction of expert
systems and corpus analysis.
From the appearance of TERMINO (the first broadly known term detector) in
1990 until today a number of projects to design different types of automatic
terminology detectors have been carried out to assist terminological work.
However, despite the large number of studies in progress, the automatisation of
the terminological extraction phase is still fraught with problems. The main
problems encountered by term extractors are: (1) identification of complex terms, that is, determining where a terminological phrase begins and ends; (2) recognition of complex terms, that is, deciding whether a discursive unit
constitutes a terminological phrase or a free unit; (3) identification of the
terminological nature of a lexical unit, that is, knowing whether in a specialised
text a lexical unit has a terminological na ture or belongs to general language and
(4) appropriateness of a terminological unit to a given vocabulary (this has
scarcely been addressed from the point of view of automatization).
Systems for TE are based on three types of knowledge: (a) linguistic; (b) statistical; (c) hybrid (statistical and linguistic). Hence, there are different
approaches to automatic term detecti on. All systems analyse a corpus of
specialised texts in electronic form and extract lists of word chunks (i.e.
candidate terms) that are to be confirmed by the terminologist. To make the terminologist‚Äôs task easier the candidate term is provided with its context and,
when available, with any other furt her information (freque ncy, relationship
between terms, etc.)
Two relevant aspects regarding the nature of terms are termhood and
unithood2; TE systems may be designed based on only one of these two aspects.
Some practical experiments following each scheme for ranking a set terms
extracted from Japanese texts are presented in (Nakagawa &amp; Mori, 1998). They show that results in precision and recall are very close but the set of terms
extracted are a somewhat different. This is still a research issue.
Alongside term detection we find the task of automatic document indexing (i.e.
information retrieval, IR). This applied field of natural language processing
(NLP) techniques has an interesting common point with automatic term
detection, that is, word chunks that index a given document are often
terminological units. This same goal explains why many extraction systems are
rooted on IR as well as on the analys is of a specific IR system with no
application whatsoever to TE.

2 (Kageura &amp; Umino, 1996) refer to unithood  as the degree of stability of  syntagmatic combinations
(collocations) and termhood  as the degree in that a linguistic unit is related to a domain-specific
concept. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS  3

The difference between these two approaches lies in the fact that a tool for TE
should extract all terminological units from a te xt, whereas IR focuses on the
extraction of only words or word sequences that better describe the contents of
the document regardless of their grammatical features.
The standard approach to IR consists in processing documents so as to extract the so-called indexing terms . These terms are usually isolated words containing
enough semantic load to provide information about its goodness when describing
documents. Queries are processed in  a similar fashion to extract query terms .
With regard to queries the relevance of documents is based exclusively on their
representing terms. This is the reason why their choice is crucial.
Often these indexing terms are single words although it is known that isolated
words are seldom relevant enough to decide the semantic value of a document
with regard to the query. This fact has given rise to the ever-growing appearance,
in the TREC3 assessments, of word and word-sequence indexing systems using
NLP techniques.
Statistically based systems function by de tecting two or more lexical units whose
occurrence is higher than a given level. This is not a random situation, but it is
related to a particular usage of these lexical units. This principle, called Mutual
Information , also applies to other science domains such as telecommunications
and physics. Term detectors based on hybrid knowledge tend to use this idea
prior to a linguistic-based processing.
The problem with this kind of approach  is that there are low-frequency terms
difficult to be managed by extraction systems. Here it is important to note that
these systems use basically numerical information and thus are prone to be
language-independent. The two most frequen tly used measures in the assessment
of these systems are found in IR: recall  and precision . Recall is defined as the
relationship between the sum of retrieved terms and the sum of existing terms in
the document that is being explored. In contrast precision accounts for the
relationship between those extracted terms that are really terms and the aggregate
of candidate terms that are found. These measures can be interpreted as the
capacity of the detection system to  extract all terms from a document ( recall ) and
the capacity to discriminate between those units detected by the system which
are terms and those which are not ( precision ). The fact that recall accounts for all
terms from a document implies that it is a figure much more difficult to estimate
and improve than precision.
In contrast with this traditional approach, other approaches attempt to solve the problem by using linguistic knowledge, which may include two types of
information:

3 TREC ( Text Retrieval Engineering Conference ) refers to a series of conferences supported by NIST
and DARPA (U.S. agencies). Further information can be found at: http://trec.nist.gov/ . 4 Aut omatic Term Detection: a Review of C urrent  Systems
a) Term specific: it consists in the detection of the recurrent patterns from
complex terminological units such as  noun-adjective and noun-preposition-noun.
This calls for the use of regular expressions  and techniques of finite state
automata.
b) Language generic: it consists in the use of more complex systems of NLP that
start with the detection of more basic linguistic structures: noun phrase (NP),
prepositional phrase (PP), etc.
In both approaches each word is associated to a morphological category. In order
to do so different strategies are proposed: from coarse systems that do not make
use of any dictionary to complex syst ems that have an extremely detailed
morphological analysis and a final phase of disambiguation.
Systems that harness structural information resort to techniques of partial
analysis to detect potentially terminologi cal phrasal structures. There are also
systems that benefit from their understanding of what is a non-term so they are at
some point in between those systems already mentioned. Other systems try to
reutilize current terminological databases to find terms, variants or new terms.
Systems based on linguistic knowledge tend to use noise  and silence as a
measure of its efficiency. Noise attempts to assess the rate between discarded
candidates and accepted ones; silence attemp ts to assess those terms contained in
an analysed text that are not detected  by the system. Noise is common problem
of those systems using this approach. Errors in the assignation of morphological category are also shared by these systems.
The type of knowledge used leads to language-specific systems and therefore it
requires a prior linguistic analysis and probably a redesign of many parts of the
system. Knowledge in artificial intelligence has been traditionally obtained from
experts in each domain. This has yielde d several difficulties so that some
scholars have focused on automatization and systematisation in knowledge
acquisition. This strategy seems to show the benefits of a terminological
approach. Thus some researchers (e.g . Condamines, 1995) have proposed the
construction of terminological knowledge da tabases so as to include linguistic
knowledge in traditional databases. Although this is a recent a pproach, there is
no database yet containing all the features that could be used in TE, i.e. there is
hardly any semantic information. Thus closed lists of words containing sparse
semantic information within a given specialised domain have been proposed.
In this paper we attempt to analyse the main systems of terminology extraction in
order to describe its current status and thus  be able to enrich them. This paper is
divided up into two main parts: firstly, the largest part is devoted to describe
various systems of terminology extraction together with a short evaluation in
which weak and strong points have been  outlined. Secondly, the terminology
extraction systems have been classifi ed according to some parameters.</introduction>
  <body>Description of some terminology extraction systems
In the following sections we offer a critical description of number of
semiautomatic terminology extraction systems. In all cases, the following
information is given:
a) The reference data of the system, that  is, the author and the publication where
the tool is first mentioned and the system goal.
b) A brief description of the system.
c) A short evaluation of the most relevant aspects. This evaluation is mainly based on papers, oral presentations in  congresses and working papers, etc.
2.1. ANA
Reference publication:  Enguehard and Pantera (1994)
Main goal: Term extraction
ANA (Automatic Natural Acquisition) ha s been developed in accordance with
the following design principles: non-utilis ation of linguistic knowledge, dealing
with written and oral texts (interview  transcripts) and non-concern about
syntactic errors.
According to the current trend of harnessing statistical techniques in the study of
natural language, scholars use Mutual Information as a measure of lexical
association4. In order to avoid the involvement of linguistic knowledge the
concept of ‚Äúflexible string recognition‚Äù is created, which generates a
mathematical function so as to determine the degree of similarity between words.
Thus, no tool for morphological analysis is needed. For instance, the string
colour of painting  represents other similar strings like: colour of paintings ,
colour of this painting , colour of any painting , etc. The system has neither a
dictionary nor a grammar.
The architecture of ANA is composed of 2 modules: a familiarity module and a discovery module. The first module determines the following 3 groups of words,
which constitute the only required knowledge for term detection:
a. function words (i.e. empty words): a, any , for, in, is, of, to...
b. scheme words (i.e. words establishing semantic relationships) such as box of
nails , where the preposition shows some kind of relationship between box and
nails .
c. bootstrap (i.e. set of terms that cons titutes the kernel of the system and the
starting point for term detection).
The second module consists in a gradua l acquisition process of new terms from
existing ones. Further, links between detected terms are automatically generated

4 Remarkable examples of the use of these tec hniques are the works of Church &amp; Hanks (1989) on
word association and Smadja (1991) on co llocation extraction from large corpora. 6 Aut omatic Term Detection: a Review of C urrent  Systems
to build a sem antic network. This module is based on word co-occurrence that
can have 3 t ypes of i nterpret ations:
‚Ä¢ expression : high-frequency  existing term s (T EXP) in the sam e window . The new
word i s consi dered a new t erm and t hus i s included in the semantic network. For
instance if the system  has diesel and engine as a known t erms and fi nds
sequences like: ... the diesel engine is... or ... this diesel engine has... Then the
sequence diesel engi ne is accepted as a new term  and is included in the semantic
network as a new node wi th links t o diesel and engine (see figure below).
‚Ä¢ candi date: an existing term  appears frequently (TCAND) together wi th anot her
word and a schem e word as in: ... any shade  of wood... or ... this shade  of
colour ... Here shade  becom es a new term  and is placed in a new node of the
semantic network (see fi gure bel ow).
‚Ä¢ expansi on: an existing term  appears freq uently (T EXPA) in the sam e word
sequence, without including an y schem e word: ... use any  soft woods  to... or ...
this soft woods  or... As a result, soft wo od is in corporated into the term  list an d
the sem antic network as a new node wi th a link to woods  (see fi g. 1 bel ow).
The system  keeps on recursively
seeking elem ents with the three
interpretatio ns alread y m entioned
until a new term  is found.
Enguehard and Pant era (1994)
tested it by processi ng a docum ent
in Engl ish of around 25,000 words
and 29 reference term s. The sy stem
managed t o extract 200 t erms with
an error rate of 25%.  shade
Soft
wood woodDiesel
engine
engine diesel
Figure 1 Term candidates inter pretation
Evaluation
Minimising linguistic resources i s an ext remely interesting issue, si nce i t is
difficult to compile th em. Lik ewise flexib le strin g recognition may well ap ply to
actual texts.
A negative asp ect o f the system  is th at those term inological u nits ad ded to the list
of valid terms after each cycle are not  validated. Thus ANA  allows for the
inclusion of non-val id terms that add up t o the term list. However , no dat a about
the efficiency  of this proposal  are report ed.
2.2. CLARIT5
Refer ence publication:  Evans and Zhai  (1996)
Main goal : Docum ent indexi ng

5 Further infor mation can be found at: http://www .clarit.com . AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS  7

Docum ent indexi ng for IR  is an i mportant field of appl ication of NLP
techni ques. This branch holds common points with term detection si nce t he word
sequences t hat help in docum ent indexi ng are norm ally terminological units too.
CLARIT belongs t o the group of sy stems that advocat e an elaborat ed textual
processing to detect com plex terms in order to reach a m ore appropriate
descri ption of docum ents. This is the reason why  we have i ncluded t his system
amongst  terminology detectors.
Evans and Zhai  (1996) propose t he fol lowing ki nd of phrases for indexat ion
purposes:
1. lexical atoms (hot dog, stainless steel , data base , on line, ...)
2. head m odifier pai rs (treated strip , ...)
3. subcom pounds ( stainless steel strip , ...)
4. cross-preposi tion m odification pai rs (quality su rface vs. quality o f surface)
The methodol ogy starts with the morphol ogical anal ysis of words and t he
detection of noun phrases (NPs). The sy stem distinguishes si mplex noun phrases
from  cross-preposi tion si mplex phrases.
What is behi nd this is the introduct ion of statistics to corpus linguistics. Statistics
here focuses on docum ents, that is, there is no pri or training corpus. Linguistic
knowledge facilitates the calculation weedi ng out irrelevant st ructures, im proves
the reliab ility o f statistical d ecisio ns and adjusts th e statistical p arameters.
The whol e process i s showed i n the figure bel ow:
First, th e raw tex t is parsed so as to
extract NPs. Then each NP  is
recursively parsed  with the purpose
of finding t he m ost secure
groupi ngs. In t his phase l exical
atom s are also detected and NPs
are structured. Finally at the
generat ion phase t he rem aining
compounds are obt ained.
Lexical atom s are defined as
sequences of t wo or m ore words
constituting a sin gle semantic unit
such as space sh uttle, part of
speech  and hot dog . Since the
detection of t hese uni ts is fraught
with probl ems two heuri stical rules are proposed:  Figure 2 Whole process in CLARIT NPs
meaningful subcompounds Raw T ext
Lexical atom s
Attested term s
Subcompound
generator Structured
NPs Simplex NP
Parser CLARIT
NP Extractor
a. The wo rds that co nstitute a lex ical ato m estab lish a close relatio nship and tend
to lexicalise as if th ey were a sin gle-wo rd lexical u nit.
b. When act ing as NP , lexical atoms hardl y allow the insertion of words. 8 Aut omatic Term Detection: a Review of C urrent  Systems
The first condition takes place if the frequency of the target pair W1W2 is higher
than any other pair from the NP that is  being processed. In the second condition
the frequencies of grouped and separated o ccurrences are compared and there is
a threshold beyond which the association is weeded out. This threshold is
variable according to the function of  sentence morphological category. In
English texts, the most favoured sequence is that of noun-noun.
NP analysis is also a recursive proce ss. At every new phase the most recent
lexical atoms are used for finding new associations that will be used in the
following phase. The process keeps going until the whole NP is analysed. Let us
consider the example below:
general purpose high performance computer
general purpose [high performance]  computer
[general purpose]  [high performance] computer
[general purpose] [[high performance] computer]
[[general purpose] [[high performance] computer]]
The grouping order shows those sequences with a more reliable association
score. In order to determine the association score a number of rules are taken into
account:
‚Ä¢ Lexical atoms are given score 0 as well as adverb combination with
adjective, past participles and progressive verbs,
‚Ä¢ Syntactically impossible pairs are given score 100 (noun-adjective, noun-
adverb, adjective-ad jective, etc.).
‚Ä¢ As to the remaining pairs, there is a formula that account for the frequency
of each word, the association score of this word with other words from the
NP and of two random parameters.
To increase its reliability the association score is recomputed after every
assignation association. The sy stem has been tested in an actual retrieval task of
document indexing substituting the defau lt NLP module in the CLARIT system.
The corpus and the queries were the st andards used in the TREC conferences.
There have been noticed some improveme nts in recall as well as in precision,
which, in the author‚Äôs opinion, justifies the use of these techniques. Then in the
TREC-5 report a more detailed evaluation of the system is made (Zhai et al.
1996). All in all it is concluded that the use of these techniques is effective, which enforces the similarities between term indexing and terminology extraction.

Evaluation
This seems to be an interesting system and the applicability of some basic ideas to terminology detection appears to be feasible. Actually CLARIT holds
similarities with the Daille‚Äôs (1994) propos al (a linguistically-driven statistics). AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS  9

It should be borne in mind, however, that problems of terminology extraction
and document indexing are similar but no identical so that many decisions
should be re-considered strictly from the point of view of term detection. It is
also noteworthy that this system only extracts NP terminological units and the
data provided about how this system works are related to the application for
which it has been designed.
2.3. Daille-94 (ACABIT)
Reference publication:  Daille (1994)
Main goal: Term extraction
The main idea behind this system is to combine linguistic knowledge with
statistical measures. Here the corpus should contain all the morphological
information. Then a list of candidate terms is created according to text sequences
that provide syntactic patterns of term formation. This information uses
statistical methods to filter out this list. This final process is different from other
systems in that it only uses linguistic resources.
Assuming the fact that all terminologi cal banks are basically composed of
compound nouns, the program focuses on the detection of binary compound
nouns  and disregards other co-occurring categories. This assumption lies in the
fact that there is a large number of this kind of nouns in specialised languages. Further, most of these compounds of 3 or  more constituents can be treated in a
binary form.
Those patterns considered relevant for French are N 1 PREP (DET) N 2 and N
ADJ PREP √† (DET) N 2, together with right and left coordination. Statistical
algorithms are applied to these patterns. The author is aware of  the fact that the
application of statistical measures leads to some noise rate, that is, low-frequency
terms will not be recognised.
The technique used for pattern recognition is that of finite state automata.
Automata are represented by a subset of grammatical tags to which some lemmas, inflected forms and a punctuation mark are added. Thus we can regard
automata as linguistic filters that select  defined patterns and also determine their
occurrence frequency, distance and variation. Each morphosyntactic pattern is associated with a speci fic finite automaton.
The corpus is given a statistical treatment based on a large number of statistical
measures, which are grouped in the following classes: frequency measures,
association criteria, diversity criteria a nd distance measures. The starting point is
considering the two lemmas that constitu te a pair within a pattern as two
variables on which the dependence degree is measured. Data are represented in a
standard contingency table: 10 Aut omatic Term Detection: a Review of C urrent  Systems

 L2 Ln where a = L 1L2 occurrences
L1 A b  b = L 1+Ln (n‚â†2) occurrences
Lm C d  c = L m+L2 (m‚â†1) occurrences
    d = L m+Ln (m‚â†1 and n‚â† 2)
Eighteen measures are applied with th e aim of establishing the degree of
independence of the variables in the contingency table. The analysis of the
results shows that only four of these measures are relevant to the purpose:
frequency, cubed association criterion6 (IM3), likelihood criterion,
Fager/MacGowan criterion.
Evaluation
Unlike in other systems, in ACABIT frequency has turned out to be one of the
most important measures for term detection from a given area. However, the
classification resulting from the application of this frequency shows an important
number of frequent sequences that are not terms and, in contrast, does not
suggest the low-frequency terms.
Daille (1994) believes that the best measur e is the likelihood criterion, since it is
a real statistical test, it proposes a cl assification that acc ounts for frequency, it
behaves adequately with large and medium size corpora and it is not defined in
those cases that are not to be considered . In any case, this measure yields some
noise due to several reasons:
a. Errors in the morphological mark-up.
b. Some combinations that are never of a compounding nature: ko bits (kilobits),
√† titre d‚Äôexemple  (as an example)... ...
c. Combinations of 3 or more elements, related to the problems of composition
and modification: bande lat√©rale  -unique-  (-single- side band), service fixe  -par
satellite-  (-satellite- fixed service) , etc.
2.4. FA S T R7
Reference publication : Jacquemin (1996)
Main goal : Term variation detection
The aim of this tool is to detect terms variants from a set of previously known
terms. These terms may be available from a reference database or a term
acquisition software. What is crucial in this system that it is not needed to start
from scratch every time. Optionally Fastr can also be used for TE.
The first step for applying Fastr is to obtain and analyse a set of existing terms and thus having a set of rules of a given grammar. The FASTR grammatical

6 The formula was experimentally obtained by the autor from the association number described in
Brown et al. (1988) in the aim of fa vouring the most frequent pairs: IM3=log 2 (a3/(a+b)(a-b))
7 Further information can be obtained at http://www.limsi.fr/Indivi du/jacquemi/F ASTR/index.html  AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
11

formalism is an extension of that of PATR-II (Shieber, 1986). A partial parser
based on the unification mechanism is responsible for the application of these
rules. Term variants are obtained through a metarule mechanism that is
dynamically calculated.
For instance, the term serum albumin  corresponds to the Noun-Noun sequence
and is associated with the following rule:
rule 1: N 1 ‚Üí N2 N3
&lt;N 1 lexicalization&gt;= ‚ÄòN 2‚Äô
&lt;N 2 lemma&gt;=serum
&lt;N 3 lemma&gt;=albumin.
The value indicated by the feature ‚Äúlexicalization‚Äù will be use just before partial parsing to selectively activate the target rules. Thus the above rule is linked to the
word serum  and so is activated when this wo rd occurs in the sentence that is
being parsed.
At a different level several metarules generate new rules in order to describe all
possible variations of each term from the re ference list. Each metarule presents a
particular structure and a specific pa ttern type. For instance, the following
metarule can be applied to the previous rule:
Metarule Coor(X 1 √Ü X2 X3) = X1 √Ü X2 C4 X5 X3
which leads to the new rule: N 1 √Ü N2 C4 X5 N3
This latter rule allows new constructions that substitute C 4 for a conjunction and
X5 for an isolated word such as serum  and egg albumin . The candidate term is
not the whole new construction but the coordinated term (i.e., egg albumin ). The
words that have given way to the new rule (egg  and albumin) maintain their
function of constricted equations of the original rule. Further, they are the
anchoring point for the application of the metarule. A metarule can be associated
with specific restricti ons, as for instance: ( &lt;C 4 lemma&gt; ‚â†but) or ( &lt;X 5 cat&gt; ‚â† Dd).
In this way, those sequences with no lexical relationship such as serum and the
albumin are rejected.
The above rule is a coordination rule and it should be noted that there are also
other types of rules that account for different kinds of variations:
1. insertion rules:  medullary carcinoma  √é medullary thyroid carcinoma
2. permutation rules: control center     √é center for disease control
The FASTR metagrammar for English contains 73 metarules altogether: 25
coordination rules, 17 insertion rules and 31 permutation rules. In any case, for
efficiency reasons the new rules are dyna mically generated. Each rule is linked
to a pattern extractor that permits a very quick acquisition of information. As has
been pointed out, the FASTR grammati cal formalism is a PATR-II extension
(Shieber, 1986). This language allows to write grammars using feature structures.
The rules describing terms are composed of a free-context part (N 1 ‚Üí N2 N3) and 12 Aut omatic Term Detection: a Review of C urrent  Systems
a number of restriction equations (e.g. &lt;N 2 lemma&gt;= serum ). First, the system
filters the rules that are to be applied according to the given text and then an
analysis take place.
When Fastr is applied for term acquisition the process is gradual: from a given
set of terms the system detects new ones, which allows the beginning of a new
cycle and the detection of new candi dates. The loop goes on until new terms
cannot be detected. The author presents an experiment carried out on a medicine
corpus of 1,5 million words and a reference list of 70,000 terms from different
specialised domains. After 15 cycles 17,000 terms were detected of which 5,000
were new. The text was processed at a 2,562 word/minute speed.
However, the number of recognised terms decreases when the reference list has fewer items. For instance, if the reference sublist of medicine drops to 6,000
terms, then only 3,800 new terms are recognised.
The author also postulates the existence of a conceptual relation. between the
new terms and the term that has led to their recognition. This relationship is variable in accordance with th e type of rule that is applied i.e., insertion or
coordination rule. Permutation does not a llow any relationship due to the phrasal
nature of the relationship.
All the language dependent data used by Fa str is stored in separated text files.
This feature facilitates the us e of the system in other languages as showed by the
recent application of Fastr to Japa nese, German and Spanish/Catalan.
Recently Jacquemin has developed the de tection of semantic  variation using
resources like WordNet or the Microsoft Word97 thesaurus (Jacquemin, 1999).
Evaluation
The main characteristic of FASTR is its ability for detecting term variants, an
aspect often not considered by other systems. The fact of using already
recognised and accepted terms is very usef ul, although, as the author admits, it
places restrictions on the ac quisition of new terms that are not related to the
source terms.
TE in Fastr implies that terms that are a dded to the list of valid terms after each
cycle are not validated. Thus, a non-valid term may be added to the list so it is
likely that in forecoming cycles more non-valid terms are added. Jacquemin
(1996) believes that this is not an impor tant error source because the system, in
some way, corrects itself since ‚Äúnormally‚Äù non-correct candidates do not give
way to new potential candidate terms.
Actually this technique should not be isolately applied. Rather, it should be
coordinated with other strategies as in (Jacquin &amp; Liscouet 1996) and (Daille
1998).  AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
13

                                                          2.5. HEID
Reference publication:  Heid et al. (1996)
Main goal: Term extraction
Heid et al.  (1996) believe that automatic TE has various applications and
dictionary or glossary construction w ould be the major one. In dictionary
construction from computerized corpora two phases are distinguished: linguistic
pre-analysis and a term identification t ool. Each of these phases requires specific
computer tools.
In the linguistic pre-processing phase the following processes are required8:
a) tokenizing, which identifies word and sentence boundaries.
b) morphosyntactic analysis, which identifies grammatical categories as well as
distributional and morphosyntactic features.
c) POS tagging, which disambiguate morphosyntactic hypotheses.
d) lemmatization, which identifies the lemma candidates.
For term identification the system has a general corpus retrieval interface that
includes a corpus query processor (CQP), a macroprocessor for the CQP query
language and a key word in context (KWIC) program, to extract and sort
concordances and lists of absolute and relative frequency of search items.
TE is linked to a complex query langua ge. The queries will be different
according to the types of candidate term s searched for. Thus, for instance,
queries about single-word terms are made from morphemes or typical
components of compound or derived words (derivatives). In these queries it is
assumed that NP affixed terms from specialised languages use more specific
affixes and/or prefixes than others. A ll the word sequence ex tracted (N-A, N-N,
N-V), are based on POS patterns.
Heid et al.  (1996) have applied these tools to technical texts on automobile
engineering in German, which amounts to 35,000 words. The sample has been
manually analysed before the application of the above procedures. The results
are as follows:
‚Ä¢ With regard to single-word terms, there has been found a 90% of candidate
terms and a 10% of silence. This rate  varies from one scheme to another.
‚Ä¢ With regard to multiword terms, there are no concluding results. The results are less satisfactory and that the same problems as linguistic based are
found: POS patterns do not constrain enough the context and produce too
much noise. Heid et al.  (1996) believe that by using a syntactic parser, as it
is the case in English, noise would diminish.

8 Heid et al.  (1996) note that a broad coverage morphosynt actic parser for German is not attained.
Thus parser results are simulated using POS patterns.   14 Aut omatic Term Detection: a Review of C urrent  Systems
‚Ä¢ Finally, collocation extraction is shown to produce noise but not silence,
since Heid et al.  (1996) consider the frequency criterion.
The Ahmad‚Äôs statistical measure (Ahmad et al , 1992) of relative frequency in
corpora of specialised and general language  is applied to this corpus of 35,000
words. They show that the results produced by linguistic corpus query are
included in the output of statistical methods. However noise in statistical
methods is higher than in linguistic methods.
Evaluation
To tackle this system it should be taken into account the morphosyntactic
features of the German language. Unlike Romance languages, German prefers to form compounds in a synthesising manner. It means that what other languages
express via terminological phrases in German is expressed with a single-word
term (by word is meant any segment found between two gaps). Thus it can be
seen that in German automatic term detection does not depend much on term
delimitation but on the terminological nature of a word. This is the reason why
we need parameters to distinguish a te rm from a word of the general language,
both having the same morphosyntactic structure.
Like most of the reviewed programs, Heid focuses on NP terms although it can also extract collocations combining nouns and verbs. In this case Heid et al.
(1996) note that the results are much worse. We do not have specific data about
the performance and the results of this system.
2.6. LEXTER
Reference publication:  Bourigault (1994)
Main goal: Term extraction
This system has been developed in the need of the EDF ( Electricit√© de France )
society for improving their indexation system. LEXTER aims at locating boundaries among which potentially terminological NPs could be isolated.
LEXTER carries a superficial analysis a nd makes use of the text heuristics in
order to obtain those NPs of maximum length that it regards as candidate terms.
The program is composed of several modules and works as follows:
1. Morphological analysis and disambiguation module . Texts receive information
about the POS and the lemma assigned to every word.
2. Delimitation module . At this stage a local syntactic analysis is carried so as to
split the text into maximal-length NPs. For example: alimentation en eau (water
supply),  pompe d‚Äôextraction  (extraction pump),  alimentation electrique de la
pompe de refoulement  (electric supply of the forcing back supply). Here the
system takes advantage of the negative knowledge about the parts of complex
terms. Thus those patterns of a potential term ‚àí finite verbs, pronouns and
conjunctions ‚àí that will never become part of a term are identified and considered AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
15

as boundaries. Some of these patterns are simple whereas others are complex
(sequences of preposition + determiner).
A French example of the latter would be SUR (prep) + LE (definite article): the
most common analysis is to propose that this sequence establishes a boundary
between NPs as in: on raccorde le c√¢ble d‚Äôalimentation du banc sur le  coffret de
d√©charge batterie. However there is a rate (10%) in which this sequence is part
of the term: action sur le  bouton poussoir de r√©armement or action sur le
systeme d‚Äôalimentation de secours
To solve this and other similar situations, the system uses an endogenous
learning strategy of the patterns sub-categorisation. This strategy consists in
looking at the corpus to find those sequences of (noun) + sur + le  having
different contexts on the right hand side. Then non-productive nouns are weeded
out. Then sequences such as s ur + le  are considered sentence boundaries, except
for those cases wherein sequences are preceded by the productive noun located
in the learning phase. To see how this system works let us suppose that at a first
analysis the sequences below are found.
Le protection contre
Protection contre
il s‚Äôagit de maintenir la teneur en oxyg√®ne de cette eau dans
on proc√®de √† l‚Äôinjection d‚Äôeau dans
on proc√®de √† l‚Äôinjection d‚Äôeau dans
le syst√®me permet l‚Äôaiguillage des automates sur le gel est assur√©e par
les grands froids
les limites fix√©es
les limites fix√©es
les g√©n√©rateurs de vapeur
le pr√©l√®vement effectu√©
Then productive sequences are not regarded as term boundaries whereas non-productive sequences are viewed as extern al boundaries of the candidate term. In
the example above protection contre  and eau dans  do not become boundaries
whereas automates sur  does.
This strategy permits to detect a considerable amount of complex nouns which otherwise would have been lost. Unfortunately it also allows a great deal of
undesirable material (between 10% and 50%).
3. Splitting module . NPs are analysed and their constituents are divided into head
and expansion. For example the term candidate pompe d‚Äôextraction (extraction
pump)  is splitted into : pompe ‚Äìhead‚Äì (pump) + extraction ‚Äì expansion‚Äì
(extraction).
At this point the system may find ambiguous situations such as ‚ÄúNoun Adj 1
Adj 2‚Äù and ‚ÄúNoun 1 Prep Noun 2 Adj‚Äù whose analysis is uncertain. To solve these
cases an endogenous learning process is followed which is similar to that
presented in the delimitation module.
4. Structuring module . The list of term candidates is organised in a
terminological network. This network can be produced only by looking at a list
of candidate terms and recognising the di fferent parts of each candidate term,
like in the following example: 16 Aut omatic Term Detection: a Review of C urrent  Systems
E expansion E expansion
N head
electric forcing back
head N‚Äô electric supply  E expansion
E‚Äô expansion N head N head
supply  extraction
electric supply  of the
forcing back pump forcing back pump  extraction pump  pump
Addi tionally Lexter calculate som e product ivity figures based on l inks t ype
occurrences. These coefficien ts do not become filters, b ut are p assed on to the
terminologist as a piece of data so as to facilitate the evaluation of candidate
terms.
5. Navigation m odule. A consulting interface is built (called terminological
hypert ext) from  the source corpus, t he candi date term network and t he above-
mentioned coef ficients and lists.
Although LEXTER  is excl usively based on linguistic techni ques it produces
highly satisfying resul ts and i s current ly used to exploit different  corpora from
EDF and di fferent  research project s. Besides i t has been proved helpful in: text
indexat ion, hy pertextual consul ting of t echni cal docum entation, knowl edge
acqui sition and const ruction of  terminological databases.
LEXTER is also used as a term inology ext ractor in the terminological
knowledge base designed by the Terminologie et Intelligence Artificielle
(Terminology &amp; Artificial Intelligence) terminology group. SYCLADE (Habert,
1996), a t ool for word cl assification al so m akes use of LEXTER .
Evaluation
LEXTER  was born i n an indust rial envi ronm ent and from  the very beginning it
sought a robust, accurate and dom ain-inde pendent tools. These objectives were
basically attained al though m ark-up and di sambiguation errors weaken t he
capacity of the system . Som e scholars note that th is system  (like those wh ich
make use of sy mbolic techni ques) produce a consi derabl e amount of noise. Thus
of a corpus of 200,000 words t here are obt ained 20,000 candi date terms whi ch,
after the val idation st age, am ount to 10,000. Also, B ourigault stresses t he silence
probl em, whi ch he est imated around 5% of t he total valid terms. Li ke the vast
majority of sy stems, LEXTER  only focuses on NPs si nce verbs are believed to
be term boundary  and so t hey are never part  of candi date terms.
One of the most rem arkabl e achi evements of t his system is the endogenous
learni ng m echani sm that allow to work aut onom ously and so t here is no need for
a com plex and l arge dictionary . In a si milar vein it shoul d be highlighted the
usefulness o f presenting the resu lts h ypertextually, sin ce it facilitates the
terminologist‚Äôs task. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
17

2.7. NAULLEAU
Reference publication:  Naulleau (1998)
Main goal: Noun phrase filtering system
The model designed by Naulleau is a NP extraction system that proposes as term
candidates those sequences that comply with certain user tailored profile. The
whole process can be divided in two main stages: profile acquisition and profile
application.
To define its own profile the user chooses the set of phrases that s/he considers
relevant for his task and discards the ones that s/he does not consider useful at
that time. The data collect ed in such way is gene ralised according to their
morphological, syntactical and semantic characteristics dynamically creating a
set of positive and negative filters. A simple example of positive and negative
filters is the following:
(1) positive filter: metallic/automatic/nuclear/industrial taps
(2) negative filter: important/recent/necessary/unreliable taps
Then, those filters produced in the lear ning stage are applied to new sequences
analysed. As a result, some noun arguments and/or PPs can be eliminated. Thus a
NP can be divided or reduced and the resulting sequences are passed on to an
expert to be evaluated.
In doing so the author acknowledges the sociolinguistic nature of the term. It
implies that there is no linguistic model that can tell whether a NP is a term or
not beyond the scope of a field or even the application. Also, this procedure
introduces the idea of how relevant a phrase is in relation to the interest profile of
the user and assumes that such relevance may be evaluated on linguistic grounds.
This is a fully symbolic approach that uses the AlethIP engine that produces
sentences fully lemmatised, tagged and syntactically parsed. Then nouns and
adjectives are semantically tagged acco rding to both suffix information and
semantic data from AlethIP and using a set of contextual rules for the more
frequent and ambiguous words. The whole strategy is based on the evaluation of
the relevance of simple syntactic depende ncies. Such relevance is only based on
the data provided by the user.
According to the author, the results are encouraging. However it is difficult to
evaluate due to the practical problem posed by such a detailed evaluation. Some
additional experiments are described in (Nalleau, 1999).
Evaluation
This system may be considered the first one to use semantic data as a specific
resource for proposing term candidates. Also, as far as we know, is the first time
since the very beginning in the design of a TE systems that the user and the idea
of relevance to an application are taken into account. 18 Aut omatic Term Detection: a Review of C urrent  Systems
In this way the user may adapt the syst em to its specific needs but also its
intervention may crucially affect the performance of the system. The loss of
specific data makes difficult to evaluate the tool behaviour in an actual context.
2.8. NEURAL
Reference publication:  Frantzi and Ananiadou (1995)
Main goal: Term extraction
Neural is a system for TE of a hybrid nature, that is it uses both linguistic
(morphosyntactic patterns and a list of suffixes specific to the domain) and
statistically knowledge (frequency and mutual information). Frantzi &amp;
Ananiadou (1995) pays special attention to two different problems: detection of
nested terms and detection of low frequency terms using statistical methods.
The test bench is a corpus of 55,000 words in the domain of medicine
(ophthalmology). The structures analysed are Noun-Noun and Adjective-Noun
that are identified using a standard tagger. The list of suffixes includes those
frequently found in terminological units in the field of ophthalmology like -oid, -
oma, - ium. The system is implemented using a Back-Propagation (BP) two
layers neural network. The threshold has b een set to .5 but this may vary. The BP
neural network has been trained with a set of 300 compounds and the tests were
made with another set of 300 words. It obtained a success rate of 70 %.
The author and other scholars from the Manchester Metropolitan University have
been active since 1995 developing specific statistical figures for TE. In this way
it is necessary to mention those tasks related to the adding of context information
(Frantzi, 1997, Maynard &amp; Ananiadou, 1999). Usually the context is discarded
or, alternatively, considered as a bag of words although its relevance is signalled
by many scholars. Here the basic assumption is that terms tend to appear
grouped in real text, so the termhood figure of a candidate would increase if there are other terms (or candidates highly ranked) in the context.
Both Frantzi, 1997 and Maynard &amp; Ananiadou, 1999 propose a similarity figure based on the distance between the candidate and the context words (nouns,
adjectives and verbs). This figure is calculated by Frantzi (1997) using statistical
and syntactic information while Maynard &amp; Ananiadou (1999) include also
semantic information from a specialised thesaurus (UMLS semantic Network).
In Maynard &amp; Ananiadou (1999) this similarity figure may also be used to take
into account some kind of semantic disambiguation for the sense that gets a
better value. A context factor (CF) is added to the figure already used to rank the
candidates (Cvalue) and thus reorderi ng the set of candidates as follows:
SNCvalue(a) = 0.8*Cvalue(a) + 0.2*CF(a). The authors report improvements in
the ranking of term candidates from his eye pathology corpus.
Evaluation AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
19

The original system can be seen as a standard hybrid system. The linguistic
knowledge includes Greek and Latin affixes and morphosyntactic patterns. The
incorporation of this kind of suffixes should be highly productive. However the
chosen patterns may well apply to English but not to Romance Languages.
The incorporation of the context as part of the data available for evaluating the
termhood of a candidate is a very interesting contribution to the behaviour of
terms in real texts. It should also serve to increase the relevance of low frequency
candidates but no specific figure is given.
It is necessary to mention the use of semantic information as a kind of resource that is increasingly used in the TE field.
2.9. NODALIDA-95
Reference publication : Arppe (1995)
Main goal : Term extraction
NODALIDA, a product designed by the Lingsoft firm, is based on an enhanced
version of NPtool that is a program de veloped at the Department of General
Linguistics at the Helsinki Univers ity (Finland). NPtool (V outilanen 1993)
generates lists of NPs occurring in the sentences of a text and provides an
assessment about whether these phrases are candidates terms or not (ok/?). From
these lists all the acceptable sub-chains ar e obtained. Besides, the source list is
multiplied. Let us see an actual example, for the sentence: ‚Äú exact form of the
correct theory of quantum gravity ‚Äú NPtool proposes the following additional
NPs:
form of the correct theory of quantum  gravity  form   correct theory
exact form of the correct theory    exact form   gravity
form of the correct theory     theory  quantum gravity
Simultaneously there are a number of premises that become the first filter like in
the following: ‚ÄúThose NPs preceded by a determiner, adjective or prefixed
sentence ( kind of, some, one, ... ) are weeded out.‚Äù
As for the remaining NPs, their occurren ce frequency is calculated. Further, they
are ordered and grouped according to thei r grammatical head and are presented
to the terminologist together with thei r context. The NPtool module (V outilanen,
1993) is at the heart of the system. It is a NP detector largely based on the
constraint grammar formalism (Karlsson, 1990). Its main features are: (1)
Morphological/syntactical descriptions are based on a large set of hand-coded
linguistic rules, (2) both the grammar and the lexicon allow a corpus analysis
with non-controlled text and (3) disa mbiguation is made according to only
linguistic criteria. As a result, between 3% and 6% of the words remain
ambiguous. 20 Aut omatic Term Detection: a Review of C urrent  Systems
The text goes through a previous process so as to determine sentence boundaries,
idiomatic expressions, compound forms, typographical signs, etc. Then it is
morphologically analysed and a result like this is obtained9:
(‚Äú&lt;*the&gt;‚Äù  (‚Äúthe‚Äù DET CE NTRAL ART SG/PL (@&gt;N)))
(‚Äú&lt;inlet&gt;‚Äù  (‚Äúinlet‚Äù N NOM SG))
(‚Äú&lt;and&gt;‚Äù  (‚Äúand‚Äù CC (@CC)))
(‚Äú&lt;exhaust&gt;‚Äù (‚Äúexhaust‚Äù &lt;SVO&gt;  V SUBJUNCTIVE VFIN (@V))
   (‚Äúexhaust‚Äù &lt;SVO&gt; V IMP VFIN (@V))
   (‚Äúexhaust‚Äù &lt;SVO&gt; V INF)
   (‚Äúexhaust‚Äù &lt;SVO&gt; V PRES -SG3 VFIN (@V))
   (‚Äúexhaust‚Äù N NOM SG))
(‚Äú&lt;manifold&gt;‚Äù (‚Äúmanifold‚Äù N NOM PL))
At this moment disambiguation takes place. For example in the sentence: ‚Äù The
inlet and exhaust manifolds are mounted on opposite sides of the cylinder head ‚Äú
two analyses are obtained:
(1) on/@AH opposite/@N side s/@NH of/@N&lt; the/@&gt;N cylinder/@NH  head/@V
(2) on/@AH opposite/@N side s/@NH of/@N&lt; the/@&gt;N cylinder/@&gt;N  head/@NH
What distinguishes these two analyses is the consideration of whether the final sequence ( cylinder head) is a NP or not. The ongoing process gives only two
possible analyses for each sentence. First, those NPs of a maximal length are
preferred ( NP-friendly ) and, second, those NPs of a minimal length are preferred
(NP-hostile ). Then the system compares both strategies and labels each NP as
ok/? by considering whether the analysis is shared or not by both strategies
Thus the last sentence gets this analysis below:
(3) ok : inlet and exhaust manifolds   ?: opposite sides of the cylinder
ok: exhaust manifolds     ? :  opposite sides of the cylinder head
In order to validate this additional information the terminologist is provided with
a list of candidate terms. The results reported by the NPtool module are pretty
good (precision=95-98% and r ecall=98.5-100%) with a text of about 20 Kwords.
Evaluation
NODALIDA is based on the use of linguistic knowledge through a structural
approach (i.e., detection of phrasal structures and structural disambiguation).
Arppe (1995) presents high-quality results. However, the corpus should be
enlarged, since so far tests have been made  on quite small corpora. It is not clear
how precision and recall figures are calculated, particularly how to determine
which terms are deemed to be correct (i.e., those which have the ok signal or all
of them). Also it should be stressed that NODALIDA has not been tested using the NPtool enhanced version in an actual situation of terminology problems.

9 The meaning of the syntactic function tags are: @&gt;N = pre-modifier; @&lt;N = post-modifier; @CC
and @CS= coordination and subor dination conjunction; @V = Verb; @NH = nominal head. Finally,
‚Äú&gt;‚Äù and ‚Äú&lt;‚Äù indicate the direction of the phrasal head. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
21

Taking into account that the disambiguator is one of the main error sources in
this kind of systems, Arppe (1995) belie ves that a high-degree quality is achieved
despite the fact that there are no data about terminology extraction in real
situations. Besides, to achieve this quality NODALIDA proposes a great deal of
rules, which yields management and control overhead.
The list that is passed on to the terminologist to be validated comprises those
candidates signalled with ok and ?. The way in which potential NPs are obtained
by the system leads us to suspect that  there are many candidate terms in the
validation list that the terminologist has to analyse.
2.10. TERMIGHT
Reference publication:  Dagan and Church (1994)
Main goal: Translation aid
Termight is currently used by A&amp;T Business Translation Systems. It was created
to be a tool for automating some stages of the professional translator
terminological research.
To do so it starts with a tagged and disambiguated text as well as a list of
predetermined syntactic patterns that could be adjusted to every document. Thus,
a list of candidate terms is obtained comprising one or more words. Single-word
candidates are defined as all those words that are not included in a previously
determined list of empty words (i.e. stop list). Multiword terms are referred to
one of the predetermined syntactic patte rns via regular expressions. Dagan and
Church (1994) considered only noun sequences patterns.
Candidate terms are grouped and classifi ed according to their lemma (i.e. the
right hand side noun) and frequency. Thos e candidates sharing the same lemma
are classified alphabetically in accord ance with the inverse order of their
compounding words. Thus it is showed the order of changes of the English
simple NPs.
For each candidate term the corresponding concordances are obtained, which are
alphabetically classified acco rding to their context. Th is information enables the
terminologist to evaluate  whether each candidate is appropriate or not.
Dagan and Church (1994) note that the rate of term list construction is of 150 and 200 terms per hour, which is twice faster than the average. As for the
extraction quality, they state that, un like exclusively statistical methods,
Termight permits to extract low-frequency terms.
Moreover this system has a bilingual module which, via statistical methods,
obtains a word-level alignment from texts. Thus terms found in language A are
referred to their counterparts in language B. This well-ordered list of candidate
terms is again passed on to the terminologist to be evaluated. 22 Aut omatic Term Detection: a Review of C urrent  Systems
The Termight bilingual module does not seem to be developed and tested as the
basic one. Tests have been made on 192 terms from a technical manual in
English and German. The correct translation is found in the first suggested
solution in 40% of the cases, whereas only 7% corresponds to the correct
translation suggested in the second place.  As for the remaining, the correct
translation was in other places of the proposal list.
Evaluation
Termight is a remarkable system in that  there is an accurate classification and
presentation of candidate terms and it does not attempt to become an automatic
system. Rather, it helps the translator.
However, it presents a number of shortcomings: (1) The only syntactic pattern
considered is very simple: noun sequences . This pattern may well be valid for
English but not for Romance languages and (2) no numerical information about the recognition quality is given. The ty pe of pattern considered may suppose
high precision but low recall

2.11. TERMINO
Reference publication:  Plante and Dumas (1989)
Main goal: Facilitation of the term extraction terminographer‚Äôs task.
The TERMINO program is composed of several tools to facilitate TE in French.
It is a help for the terminologist insofar as the identification of those discourse
units that denominate notions or objects. Besides it provides every unit with the
immediate context from which data relevant to the notions denominated by
theses units can be obtained. There are a number of TERMINO versions which improve in some ways previous ones.
This tool is based mainly on linguistic knowledge and it comprises 3 sub-systems: a pre-editor, which separate s texts into words and sentences and
identifies proper nouns, a morphosyntactic parser and a record-drafting facility.
The text does not get any special treatment: it is only required to be codified in ASCII form.
With regard to term delimitation and extraction the more interesting sub-system
is the morphosyntactic parser. It consists of 3 modules: a morphological parser; a
syntactic parser and a synapsy detector.
The morphological parser has two functions: a) automatic categorisation; b)
lemma and tag identification. According to Plante and Dumas, 30% of words in French can be attributed to more than one category. This has led to the tagging of
all the possible categories for each word. As a result, there is an overproduction
of words with different tags. Categorisation and lemmatisation are obtained from
the application of the LCML program, it is not a dictionary but a morphological
parser of lexical forms so it can correc tly lemmatise and tag new lexical forms. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
23

The syntactic parser is responsible for weeding out the vast majority of
ambiguities generated in previous stages . It is managed through the construction
of a syntactic structure for each sentence.
Finally, the synapsy detector (MRSF) selects, among the syntactic units from the parser, those lexical noun units that are likely to be terms. S. David (David and
Plante, 1991) created MRSF especially for TERMINO. MRSF is based on
principles of noun group construction. Da vid‚Äôs understanding of synapsy is that
of a polylexical unit of a syntactic nature that is the head of the NP. Thus,
synapsies are only NPs groups: some of them will become terms and some of
them will not. Further, some of them will only be ‚Äútopics‚Äù that will enable the
terminologist to know different concepts or grasp an overview of the text topics.
The MRSF module comprises 5 sub-modules: (1) head hunter module; (2)
expansion recogniser module; (3) categor isation module; (4) synapsy generator
module and (5) representation and evaluation module.
TERMINO has a set of software tools, which is much larger and comprises different modules that allow to manipulate terminological data. These tools help
the terminologist decide whether a synapsy is a term or not, elaborate
terminological filing forms and create terminological databases.
TERMINO recognises between 70% and 74% of the complex terms. The fact that 30% of terms are not recognised by TERMINO can be explained by
coordination (it is a signal of segment breaking), acronyms and common nouns
in capital letters. Moreover, there is 28% of noise, of which 47% is due to a
wrong mark-up and a 53% is due to sy napsies belonging to general language.
Evaluation
TERMINO is one of the first candidate term extractors that worked and it is a
linguistically-based extractor, composed of different independent modules. This
system is based on the concept of synapsy . The synapsy detector is based on the
establishment of a number of heuristic rules that may well be increased provided
the corpus is delimited.
There is a need to improve this system taking into account that it is still too noisy
(28%), which could be improved, for example, with a different treatment of
capital letters and acronyms.
2.12. TERMS
Reference publication:  Justeson and Katz (1995)
Main goal: Term extraction
Justeson and Katz (1995) hold the following views about terms:
a) Terminological noun phrases (TNP) are different from non-terminological noun phrases (nTNP) in that the modifiers of the first ones are much shorter than
those of the second ones. 24 Aut omatic Term Detection: a Review of C urrent  Systems
b) An entity introduced by a nTNP can be later referred to only by the head of the
NP and often by other NP (synonyms, hyponyms, hyperonyms). By contrast, an
entity introduced by a TNP is normally rep eated identically in a given document,
as a single omission of a modifier could yield a change of the referred entity.
c) In technical texts lexical NPs ar e almost exclusively terminological.
d) Multiword technical terms are near ly always composed of nouns and
adjectives (97%) and some prepositions (3%) between two NPs.
e) The average length of a TNP is of 1.91 words.
The proposed filter finds strings with a frequency equal or higher than two. These strings follow with this regul ar expression: ((A|N)+ | ((A|N)*(N
P)?)(A|N)*N. Those candidate terms of a length of 2 (2 patterns: AN and NA)
and 3 (5 patterns: AAN, ANN, NAN, NNN and NPN) are by far the most
commonly encountered.
The purpose of this algorithm is to combine good coverage of the usual
terminology from technical texts with high quality in the extraction phase. The
algorithm prefers quality to coverage, since if it only made use of the
grammatical constraints then the system would propose many irrelevant NPs.
The vast majority of relevant NPs overcome the frequency constraint.
Selection of grammatical patterns also affects quality. If prepositions are admitted within the pattern many candi dates are introduced, although few will be
valid. As a result, quality decreases whereas quantity increases and, accordingly,
Justeson and Katz (1995) prefer not to take prepositions into consideration.
The implementation of grammatical patterns also affects the quality/coverage
trade-off. There are two ways in which a given linguistic unit is attributed to a
grammatical category: disambiguation and filtering. The first one is rejected
because disambiguators are not totally reliable yet.
Filtering consist in parsing and lemmatisi ng each word of the text. Then those
sequences following the pattern are consid ered. If a word is not identified as a
noun, adjective or preposition, it is dis carded. Thus each word maintains its
nominal, adjectival and prepositional values and in this order. The chain is
weeded out if more than one word can be identified as a preposition or if it does
not follow the pattern (e.g. if the pattern ends with a noun and there is more than
one preposition then the word following the preposition is not a noun).
Filtering has a coverage at least as good as what can be attained by a standard
tagger. However, quality is not that good (e.g. fixed  is only identified as an
adjective ‚Äì bug fixed ‚Äì, but it can also become a verb: fixed disk drive ). In
contrast, filtering is much faster than parsing.
In any case, Justeson and Katz (1995) suggest to control the patterns, the list of
grammatical words and the frequency to ad just the performance of the system to
each type of text. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
25

This system has been applied to different domains (metallurgy, spatial
engineering and nuclear energy) and it is used at IBM Translation Center. The
TERMS results are presented on the basis of 3 technical texts (statistical
classification of patterns, lexical semantics and chromatography). Coverage has
only been estimated for one of the text and it is of 71%. Quality has been
estimated between 77% and 96% of the instances.
Evaluation
Although Justeson and Katz (1995) present a detailed study on the performance of terminological units (wherein there are some overstatements), the proposed
filter does not seem to take advantage of these previous analyses of terms.
Further, it should be noted that this type of filtering based on quite simple
patterns would not be so efficient if they were applied to languages other than
English such as Romance languages. Also, this kind of patterns produces a lot of
noise.
3. Contrastive Analysis
Here we will contrast the systems‚Äô ma in features, according to six relevant
aspects when designing a new detection system of terminological units:
linguistic resources, strategies of term de limitation, strategies of term filtering,
classification of recognised terms and obtained results. For some of these criteria
we have created a table containing the mo st significant data so as to make the
system comparison easier.
3.1. Linguistic resources
It has been observed that the vast majority of the reviewed systems make use of
some sort of linguistic information, at least a list of empty words taken as
boundaries. The standard process includes a morphological analysis followed by
some kind of disambiguation system. The systems altering this procedure are the
following:
a. ANA: does not use any linguistic resource, just a list of auxiliary words
b. TERMS: use its own disambiguation system: POS filtering
c. Naulleau: introduces semantic information
Additionally, for the systems that use an incremental strategy, like ANA and
Fastr, it is necessary a set of in itial terms to bootstrap the process.
3.2. Strategies of term delimitation
All systems of terminology extraction have to determine at some point the beginning and the end of the candidate term, that is, delimit the potential
terminological unit. The reviewed programs have different strategies to delimit 26 Aut omatic Term Detection: a Review of C urrent  Systems
terms: word-boundary elements, structural patterns, syntactic parser, text
distribution, typographical elements, term lists, structure disambiguation. Below
we show a summary of the differen t options adopted by each system:

Table 1: Strategies of term telimitation
System term delimitation structure disamb.
 Name /Author boundaries Patterns Parser Other learning Other
1 ANA    X  -
2 CLARIT   X   statistical
3 Daille  X    -
4 FASTR  X X X  -
5 Heid  X    -
6 LEXTER X    X
7 Naulleau    X  -
8 NEURAL  X    -
9 NODALIDA-95    X  -
10 Termight  X    -
11 TERMINO   X   -
12 TERMS  X    -
3.3. Strategies of term filtering
Term filtering is a key stage of any term detection system. This means that the
list of candidates is reduced as much as possible. The following table shows the
strategies found in all the reviewed systems:

Table 2: Strategies of term filtering
System Term Filtering
 Name /Author Freq.10 Linguisti
c statistical +
linguistic linguistic +
statistical reference
terms user
defined
1 ANA     X
2 CLARIT   X X
3 Daille    X
4 FASTR     X
5 Heid  X
6 LEXTER  X
7 Naulleau      X
8 NEURAL    X
9 NODALIDA-95  X
10 Termight X X
11 TERMINO  X
12 TERMS X X

10 The technique of term filtering through frequency terms has been c onsidered something in between
those methods based on linguistic  knowledge and those methods ba sed on extralinguistic knowledge. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
27

3.4. Classification of recognised terms
Some of the analysed systems classify recognised terms by grouping them
according to some criteria. Thus, the related terms stay close to each other. Even
FASTR attempts to infer an ontology from the recognised terms. Those systems
which show some classification of recognised terms are the following:
a. ANA: it builds a semantic netw ork from the detected terms.
b. FASTR: it builds a graph to relate recognised terms. Also it proposes the
construction of partial ontologies for some terms.
c. LEXTER: it builds a terminological network splitting terms into head and
expansion.
3.5. Results
The table below summarises for each system the type of corpus used for the tests
and the results attained:

Table 3: Results
System Test corpora Terms %
 Name /Author Domain Language Size.[Kw.] precision recall
1 ANA Aviation engineering
Acoustics French
English 120
25
? ?
?
2 CLARIT11 News English 240 Mb - 81.6
3 Daille Telecommunications French 800 ? ?
4 FASTR Medicine (abstracts) French 1.560 86.7 74.9
5 Heid Engineering German 35 ? ?
6 LEXTER Engineering French 3.250 95 ?
7 Naulleau Technical French ? ? ?
8 NEURAL Medicine  English 55 ? 70
9 NODALIDA-95 Cosmology
Technical text English 20 95-98 98.5-100
10 Termight Computer science English ? ? ?
11 TERMINO Medicine French ? 72 70-74
12 TERMS Statistics
Semantics
Chromatography English 2.3
6.3
14.9 77
86
96</body>
  <conclusion>We can reach some conclusions after havi ng analysed and evaluated some of the
main systems of TE designed in the last decade:

11 The system has been intensively te sted with regard to the indexing frequency, but not in relation to
the quality of the extracted terms. 28 Aut omatic Term Detection: a Review of C urrent  Systems
a) The efficiency of the extraction presen ts a high degree of variation from one
to another. Broadly speaking, there is neither clear nor measurable explanation of
the final results. Besides, we have to b ear in mind that these systems are tested
with small and highly specialised corpora. This lack of data makes it difficult to
evaluate and compare them. However, it does not prevent pinpointing those
solutions, which are considered va lid to solve specific problems.
b) None of the systems is entirely satisfactory due to two main reasons. First, all
systems produce too much silence, especially statistically-based systems.
Second, all of them generate a great deal of noise, especially linguistically-based
systems.
c) Taking into account the noise generated, all systems propose large lists of
candidate terms, which at the end of the process have to be manually accepted or
rejected.
d) Most of the TE systems are related to only one language: French or English.
Usually the language specific data is embedded in the tool. This makes difficult to use the system in a language other than the original.
e) As has been already pointed out, traini ng corpora tend to be small (from 2.3 to
12 Kwords) and highly specialised with regard to the topic as well as the
specialisation degree. This allows for a quite precise patterns and lexicosemantic,
formal and morphosyntactic heuristics albeit this only applies to highly
specialised corpora.
f) All systems focus entirely on NPs and none of them deals with verbal phrases.
This is because there is a high rate of terminological NPs in specialised texts.
This rate can vary according to the topi c and the specialisation degree. Despite
what has just been noted, it is noteworthy that all specialised languages have their own verbs (or specific combinations of a verbal nature), no matter how low the ratio is in comparison with nouns.
g) As a result, none of the systems refers to the distinction between nominal
collocations and nominal terminological units of a syntactic nature. Nor do they
refer to phraseology.
h) Many of the systems make use of a number of morphosyntactic patterns to
identify complex terms. However they account for most of the terminological
units they are still too few and also not very constraining. Thus, for English are
AN and NN, for French NA and N prep N.  Some terms present structures other
than these ones and they are never det ected. Those systems based only on these
types of linguistic techniques generate too much noise.
i) It is generally agreed that frequency is a good criterion to indicate that a
candidate term is actually a terminological unit. However, frequency is not on its
own a sufficient criterion, as it yields a great deal of noise.
j) Only a few recent systems use semantic  information to recognise and delimit
terminological units although its use takes place at different levels. AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
29

k) None of the systems uses extensively the combinatory features of terms from
specialised languages in relation to a given domain. It is needed more studies
about the type of constraints that terminological units present with regard to
conceptual field and text type.
l) Only one of the analysed systems take profit of the possibilities given by the
alignment of specialised text.
i) Most of the authors consider the POS disambiguation as one of the most
important error sources. However, they do not provide exact figures about its
incidence degree.
To improve these systems of terminology extraction and lessen the noise and
silence that are generated, two type of studies should be encouraged. First, it is
required more linguistic oriented studies on the semantic relationships among
terms, the semantic relationships among constituents of a terminological unit,
semantico-lexical representation, constraints of terminological units within a
given specialised domain and in a given text type, all the grammatical categories
that are likely to become terms in specialised domai ns, the influence of the
syntactic function of terminological phrases on texts, the relationships between
terms and their arrangement in texts.
Second, we should focus on software systems that: combine in a more active manner statistical and linguistic methods; improve statistical measures; combine
more than one strategy; are easily applicable to more than one language; improve
interfaces to facilitate the m achine-user interaction. Also it should be very useful,
as suggested in Kageura et al.  (1998), the development of a common test bench
for aiding the evaluation/comparison of extracting methods.
In sum, should we progress in the field of automatic terminology extraction,
statistical and linguistic methods have to actively be combined. It means that
they are not either-or approaches but co mplementary ones. The final goal is to
reduce the amount of silence and noise so that the process of terminological
extraction becomes as automatic and preci se as possible. In the future, we
believe that any current terminology extractor, apart from accounting for the
morphological, syntactic and structural aspects of terminological units, has to
necessarily include semantic aspects if th e efficiency of the system is to be
improved with regard to the existing ones.</conclusion>
  <discussion>N/A</discussion>
  <biblio>
Arppe, A. 1995. ‚ÄúTerm extraction from unrestricted text‚Äù. Lingsoft Web Site:
http://www.lingsoft.com
Ahmad, K., Davies, A., Fulford, H. and Rogers, M. 1992. ‚ÄúWhat is a term? The
semiautomatic extraction of terms from text‚Äù. Translation Studies ‚Äì an
interdiscipline. Amsterdam: John Benjamins.
Bourigault, D. 1994. LEXTER, un Logiciel d'EXtraction de TERminologie.
Application √† l'acquisition des connaissances √† partir de textes. PhD Thesis.
Paris: √âcole des Hautes √ât udes en Sciences Sociales.
Bourigault, D., Gonzalez- Mullier, I. and Gros, C. 1996. ‚ÄúLEXTER, a Natural
Language Processing Tool for Terminology Extraction‚Äù. Proceedings of the
7th EURALEX International Congress. G√∂teborg.
Brown, P. F., Cocke, F., Pietra, S., Felihek. F., Merces, R. and Rossin, P. (1988)
A statistical approach to language translation. Procedings of 12th International
Conference of Computational Linguistic (Coling-88).  Budapest, Hungary.
Cabr√©, M.T. 1999. Terminology. Theory, methods and applications.  Amsterdam:
John Benjamins.
Church, K. 1989. ‚ÄúWord association norms, mutual information and
lexicography‚Äù. Proceedings of the 27th annual meeting of the ACL. Vancouver,
76-83.
Condamines, A. 1995. ‚ÄúTerminology: new needs, new perspectives‚Äù.
Terminology, 2, 2: 219-238.
Dagan, I. and Church, K. 1994. ‚ÄúTermight: Identifying and translating technical
terminology‚Äù. Proceedings of the Fourth Conference on Applied Natural
Language Processing, 34-40.
Daille, B. 1994. Approche mixte pour l'extraction de terminologie: statistique
lexicale et filtres linguistiques . PhD dissertation. Paris: Universit√© Paris VII.
Daille, B. and Jacquemin, C 1998. ‚ÄúLexi cal database and information access: a
fruitfull association?‚Äù. First International Conference on LREC.  Granada.  AUT OMA TIC TERM DETECTION: A REVIEW  OF CURRENT  SYSTEMS
31

David, S. and Plante, P. 1991. ‚ÄúLe progi ciel TERMINO: de la n√©cessit√© d'une
analyse morphosyntaxique pour le d√©pou illement terminologique des textes‚Äù.
Proceedings of the Montreal Coll oquium Les industries de la langue:
perspectives des ann√©es 1990,  1: 71-88.
Enguehard, C. and Pantera, L. 1994. ‚ÄúAutomatic Natural Acquisition of a
Terminology‚Äù. Journal of Quantitative Linguistics,  2, 1: 27-32.
Estop√†, R. 1999. Extracci√≥ de terminologia: elements per a la construcci√≥ d‚Äôun
SEACUSE (Sistema d‚Äôextracci√≥ autom√†tica de candidats a unitats de
significaci√≥ especialitzada).  PhD thesis, Barcelona: Universitat Pompeu Fabra.
Estop√†, R. and Vivaldi, J. 1998. ‚ÄúSyst√®mes de d√©tection automatique de
(candidats √†) termes: vers  une proposition int√©gratrice‚Äù. Actes des 7√®mes
Journ√©es ERLA-GLAT, Brest , 385-410
Evans, D.A. and Zhai, C. 1996. ‚ÄúNoun-phrase Analysis in Unrestricted Text for
information retrieval‚Äù. Proceedings of ACL, Sant a Cruz, University of
California,  17-24.
Frantzi, K. and Ananiadou, S. 1995. Statistical measures for terminological
extraction. Working paper of the Department of Computing of Manchester
Metropolitan University.
Frantzi, K. T. 1997. ‚ÄúIncorporating context information for extraction of terms‚Äù.
Proceedings  of ACL/EACL, Madrid, 501-503.
Habert, B., Naulleau, E.  and Nazarenko, A. 1996.  ‚ÄúSymbolic word clustering for
medium-size corpora‚Äù. Proceedings of Coling‚Äô96 : 490-495.
Heid, U., Jauss, S., Kr√ºger, K. and Hohmann, A. 1996. ‚ÄúTerm extraction with
standard tools for corpus exploration. Experience from German‚Äù. In: TKE ‚Äò96:
Terminology and Knowledge Engineering,, 139-150. Berlin: Indeks Verlag.
Jacquemin, C. 1994. ‚ÄúRecycling Terms into a Partial Parser‚Äù. Proceedings of
ANLP‚Äô94 , 113-118.
Jacquemin, C. 1999. ‚ÄúSyntagmatic and paradigmatic representations of term
variation‚Äù. Proceedings of  ACL'99, University of Maryland, 341-348.
Jacquin, C. and Liscouet, M. 1996. ‚ÄúTerminology extraction from texts corpora:
application to document keeping via Internet‚Äù. In: TKE ‚Äò96: Terminology and
Knowledge Engineering, 74-83.  Berlin: Indeks Verlag.
Justeson, J. and Katz, S. 1995. ‚ÄúTechnical terminology: some linguistic
properties and an algorithm for identification in text‚Äù. Natural Language
Engineering,  1, 1: 9-27.
Kageura, K. and Umino, B. 1996. ‚ÄúMet hods of Automatic Term Recognition‚Äù.
Papers of the  National Center for Science Information Systems , 1-22. 32 Aut omatic Term Detection: a Review of C urrent  Systems
Kageura, K., Yoshioka, M., Koyama, T. and Nozue, T. 1998. ‚ÄúTowards a
common testbed for corpus-based computational terminology‚Äù. Proceedings of
Computerm ‚Äò98, Montreal, 81-85.
Karlsson, F. 1990. ‚ÄúConstraint grammar as a framework for parsing running
text‚Äù. Proceedings of the  13th International conference on computational
linguistic , 3: 168-173.
Lauriston, A. 1994. ‚ÄúAutomatic recogniti on of complex terms: Problems and the
TERMINO solution‚Äù. Terminology , 1, 1: 147-170.
Maynard, D. and Ananiadou, S. 1999. ‚ÄúIdentifying contextual information for
multi-word term extraction‚Äù. In: TKE ‚Äò99: Terminology and Knowledge
Engineering, 212-221. Vienna: TermNet.
Nakagawa, H. and Mori , T. 1998. ‚ÄúN ested collocation and Compound Noun for
Term Extraction‚Äù. Proceedings of Computerm ‚Äô98,  Montreal, 64-70.
Naulleau, E 1998. Apprentissage et filtrage syntaxico-s√©mantique de syntagmes
nominaux pertinents pour la recherche documentaire.  PhD thesis. Paris:
Universit√© Paris 13.
Naulleau, E. 1999. ‚ÄúProfile-guided terminology extraction‚Äù. In: TKE‚Äò99:
Terminology and Knowledge Engineering . 222-240. Vienna: TermNet.
Plante, P. and Dumas, L. 1998. ‚ÄúLe D√©poulliment terminologique assist√© par
ordinateur‚Äù. Terminogramme , 46, 24-28.
Shieber, S.N. 1986. ‚ÄúAn Introduction to Unification-Based Approaches to
grammar ‚Äù. CSLI Lecture Notes of  University Press, 4.
Smadja, F. 1991.  Extracting collocations from text. An application : language
generation . Columbia: Columbia University. Department of Computer
Science. [Unpublished doctoral dissertation ]
Voutilainen, A. 1993. ‚ÄúNPtool, a de tector of English noun phrases‚Äù. Proceedings
of the  Workshop on Very Large Corpora.
Zhai, C., Tong, X., Milic-Frayling, N. and Evans, D.A. 1996. ‚ÄúEvaluation of
syntactic phrase indexing CLARIT. NLP track report‚Äù. Proceedings of the
TREC-5.  TREC Web Site:  http://trec.nist.gov/pubs /trec5/t5_proceedings.htm</biblio>


  <preamble>IPM1481.pdf</preamble>
  <titre>A hybrid approach to managing job offers and candidates
</titre>
  <auteurs>
    <auteur>
      <name>R√©my Kessler</name>
      <mail>remy.kessler@univ-avignon.fr</mail>
      <affiliation>LIA/Universit√© d‚ÄôAvignon et des Pays de Vaucluse, 339 chemin des Meinajari√®s, 84911 Avignon, France</affiliation>
    </auteur>
    <auteur>
      <name>Nicolas B√©chet</name>
      <mail>nicolas.bechet@inria.fr</mail>
      <affiliation>INRIA Domaine de Voluceau, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
    </auteur>
    <auteur>
      <name>Mathieu Roche</name>
      <mail>mathieu.roche@lirmm.fr</mail>
      <affiliation>LIRMM, CNRS Universit√© Montpellier 2, 161 rue Ada, 34392 Montpellier, France</affiliation>
    </auteur>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>uan-manuel.tor-
res@univ-avignon.fr</mail>
      <affiliation>√âcole Polytechnique de Montr√©al, CP 6079, succ. Centre-ville, Montr√©al (Qu√©bec) Canada H3C 3A7</affiliation>
        <auteur>
      <name>Marc El-B√®ze</name>
      <mail>marc.elbeze@univ-avignon.fr</mail>
      <affiliation>LIA/Universit√© d‚ÄôAvignon et des Pays de Vaucluse, 339 chemin des Meinajari√®s, 84911 Avignon, France</affiliation>
    </auteur>
    </auteur>
  </auteurs>
  <abstract>The evolution of the job market has resulted in traditional methods of recruitment becom-
ing insufÔ¨Åcient. As it is now necessary to handle volumes of information (mostly in the
form of free text) that are impossible to process manually, an analysis and assisted catego-rization are essential to address this issue. In this paper, we present a combination of the
E-Gen and
CORTEX systems. E-Gen aims to perform analysis and categorization of job offers
together with the responses given by the candidates. E-Gen system strategy is based onvectorial and probabilistic models to solve the problem of proÔ¨Åling applications according
to a speciÔ¨Åc job offer.
CORTEX is a statistical automatic summarization system. In this work,
E-Gen uses Cortex as a powerful Ô¨Ålter to eliminate irrelevant information contained incandidate answers. Our main objective is to develop a system to assist a recruitmentconsultant and the results obtained by the proposed combination surpass those of E-Gen
in standalone mode on this task.
/C2112012 Elsevier Ltd. All rights reserved.
</abstract>
  <introduction>
The evolution of the job market has resulted in that traditional methods of recruitment becoming insufÔ¨Åcient. The
Internet has introduced a new way of managing human resources. Theoretically, shifting job search and recruitment activ-
ities to the Internet improves the quality of job matching by reducing search costs, increasing contact opportunities and
rationalizing the screening process of job applicants ( Marchal, Mellet, &amp; Rieucau, 2007 ). Over the last few years, there has
been a signiÔ¨Åcant expansion of online recruitment (e.g. August 2003: 177,000 job offers, May 2008: 500,000 job offers).1
The Internet has become essential in this process because it allows a better Ô¨Çow of information, either through job searchsites or by e-mail exchanges. Nowadays, job seekers can send their curriculum vitae (CV) directly to companies (by e-mail
or uploaded to dedicated servers on the Web). The job search task is becoming easier and less time consuming. The Internet
makes every user a potential job seeker. Employees may be constantly in search of new career opportunities and job candi-
dates may provide more interaction than can be managed efÔ¨Åciently by companies ( Bourse, LeclFre, Morin, &amp; Trichet, 2004 ).
As intellectual capital has become one of the most strategic assets of successful organizations in the last decade, the capa-
bility of managing people‚Äôs expertise, skills and experience represents a key factor in facing up to the increasing competitive-
ness of the global market ( Colucci et al., 2003 ). Even though a browser has become a universal and easy tool for users, they frequently have to enter data into Web forms from paper sources and the need to ‚Äò‚Äòcopy and paste‚Äô‚Äô data between different
applications is symptomatic of the issues of data integration. In this context, electronic recruitment tends to automate match-
ing between the published information about the candidates and job offers. The Laboratoire Informatique d‚ÄôAvignon (LIA),2the
Laboratoire d‚ÄôInformatique, de Robotique et de Micro√©lectronique de Montpellier (LIRMM),3and Aktor Interactive4are developing
the E-Gen system to resolve this issue. E-Gen is a Natural Language Processing (NLP) and Information Retrieval (IR) system
composed of three main modules:
1. The Ô¨Årst one extracts the information from a corpus of e-mails of job offers from Aktor‚Äôs database.
2. The second module analyses the candidate‚Äôs answers (i.e. splitting e-mails into cover letter (CL) and curriculum vitae).
3. The third module analyses and computes a relevant ranking of the candidate‚Äôs answers.
Our Ô¨Årst work ( Kessler, Torres-Moreno, &amp; El-B√®ze, 2007 ) presented the Ô¨Årst module: the identiÔ¨Åcation of different parts
of a job offer and the extraction of relevant information (type of contract, salary, localization, etc.). The second module
analyses the content of a candidate‚Äôs e-mail, using a combination of rules and machine learning methods (Support Vector
Machines, SVM) and was presented in Kessler, Torres-Moreno, and El-B√®ze (2008b) . Furthermore, it separates the distinct
parts of CV and CL with a precision of 0.98 and a recall of 0.96. Reading a large number of candidate answers for a job is a
very time consuming task for a recruiting consultant. In order to facilitate this task, we propose a system capable of pro-
viding an initial evaluation of candidate answers according to various criteria. We do not seek the best or even a good can-
didate as no scoring is involved, but simply a candidate who has a close application to those already selected. Our previous
work ( Kessler, B√©chet, Roche, El-B√®ze, &amp; Torres-Moreno, 2009 ) presented an approach based on a process of relevance feed-
back, permitting a reinforcement learning ( Sutton &amp; Barto, 1998 ). In this paper, we present an original combination of the
E-Gen and CORTEX systems. Each document contains a number of additional information, present in many applications and
which is partially removed by classical pre-processing. Each application added by the process of relevance feedback adds
relevant information but also multiplies additional information. CORTEX allows us to Ô¨Ålter these sentences and keep only the
most relevant sentences at the evaluation step. Some related studies are brieÔ¨Çy discussed in Section 2. Section 3shows a
general system overview. In Section 4, we describe the E-Gen pre-processing task, the strategy used to rank the candidate
answers with relevance feedback and the coupling of E-Gen with the CORTEX summarization system. In Section 5, we present
statistics about the textual corpus, experimental protocol, an example of CL summary generated by CORTEX , and several
results.
</introduction>
  <body>
Many approaches have been proposed in the literature to reduce the costly and tedious task of managing human re-
sources. Candidate answers to a job offer come as ad hoc documents, and require semantic approaches to analyse them.
The BONOM system is based on an indexing method ( Morin, LeclFre, &amp; Trichet, 2004; Cazalens &amp; Lamarre, 2001 ). This
method consists in using distributional attributes of documents to locate each part for the Ô¨Ånal indexation of the
document.
A semantic-based method to select candidate answers and to discuss the economic impacts on the German government
was proposed by Tolksdorf, Mocho, Heese, Oldakowski, and Christian (2006) . In the same way ( Gorenak &amp; Mlaker KaF, 2010 ),
perform a comparison between Slovenian, German, and British online job advertisements (ads). More recently ( Marchal et
al., 2007 ), present a comparison between French and English job search sites and newspapers as well as the various short-
coming of current matching systems. They propose a comparative analysis of job offers posted on the Internet with those
posted in newspapers and they observe that search engine toolkits have a considerable impact on ad content which is gen-
erally more standardized and quantiÔ¨Åed than before.
Mocho, Paslaru, and Simperl (2006) discuss the relevance of a common ontology (HR ontology) to work efÔ¨Åciently with
this kind of document. Using the same model ( Dorn &amp; Naz, 2007 ), outline a HR-XML based prototype dedicated to the job
search task. The prototype selects and favors relevant information (paycheck, topic, abilities, etc.) from many job-service
websites, such as Jobs.net ,aftercollege.com ,Directjobs.com , etc. Bourse et al. (2004) describe an efÔ¨Åcient model
and a management tool used for the selection of candidate-answers. They propose a prototype job portal which uses seman-
tically annotated job offers and applicants to obtain a more accurate job search with query approximation.
The limitations of current systems for automatic selection of candidate answers are presented in Rafter, Bradley, and Smyt
(2000) . They propose a system based on collaborative Ô¨Ålters ( ACF) to automatically select proÔ¨Åles of candidate answers on
the JobFinder website. Enrica and Iezzi (2006) present a model for ranking skills in the Ô¨Åeld of information technology in Italy
with multidimensional scaling and cluster analysis. In the same way, Colucci et al. (2003) present a semantic based approach
to the issue of skills detection in an ontology supported framework. Based on Description Logics formalization and reasoning,
they propose a skill matching approach with contradiction matches and partial matches between skill proÔ¨Åles. Loth et al.
2http://www.lia.univ-avignon.fr .
3http://www.lirmm.fr .
4A French recruitment agency specialized in recruiting on the internet, ( http://www.aktor.fr ). Author's personal copy 1126 R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
(2010) combine, through the SIRE project (Semantics-Internet-Recruitment-Employment) a linguistic approach and machine
learning methods to perform an extraction of key terms of job ads in order to improve the categorization of each job offer.
The study of the most relevant document ‚Äì the CV ‚Äì to use it automatically has been a major subject of research. Ben
Abdessalem Karaa (2009) presents a system for analyzing and structuring CVs with an extension of General Architecture
of Text Engineering (GATE5). They obtain good results in precision/recall for each part of the document (personal information,
experience, skill, and so forth) on a small corpus of CVs in French. Yahiaoui, Boufaƒ± ¬®da, and Pri√© (2006) provide a semantic ap-
proach to generating some annotations of CVs and job offers with the help of a specialized ontology to match graduates and the
level of a job offer. They present interesting results on a sample of data. Clech and Zighed (2003) propose a data mining ap-
proach. Their aim is to build automats which recognize CV topologies and candidate/job offer proÔ¨Åles. A Ô¨Årst step differentiates
the CVof employed executives from other CV. They use a speciÔ¨Åc term extraction to obtain a categorization with the C4.5 deci-
sion tree algorithm ( Quilan, 1993 ). This method focuses on the speciÔ¨Åcity of selected terms or concepts, such as education level
or relevant abilities, to build a classiÔ¨Åer. The results of this method are still poor (an accuracy between 0.5‚Äì0.6 of correctly cat-egorized CV). Roche and Kodratoff (2006) and Roche and Prince (2008) have made a terminology study of corpus composed of
CVs (of the Vediorbis company ( http://www.vediorbis.com )). Their approach extracts collocations from a CV corpus based on
syntactic patterns such as Noun-Noun, Adjective-Noun, etc. Then, these collocations are ranked according to relevance to build
a specialized ontology.
There are few studies on the treatment of the cover letter. Audras and Ganascia (2006) use cover letters to detect the
usual errors in the Ô¨Åeld of acquisition of written French as a foreign language. The approach proposed is the detection of
syntactic patterns particular to a group of learners, and which are absent or little used among native speakers. The study
focuses in part on cover letter writing. Among the innovative solutions on the market, Twitter6has launched the job search
sitehttp://www.twitterjobsearch.com based on the concept of short messages (less than 140 characters) and ZaPoint7with an
original solution, SkillsMapper, which transforms each CV into graphic format with various curves (training, education, etc.). In
this paper, we present an approach to the application ranking by using a combination of similarity measures, relevance feedback
and summaries of a CV and CL. Our approach is distinguished from other work by a purely statistical approach as well as rein-
forcement learning through the process of relevance feedback.
3. System overview
Nowadays technology proposes new approaches to the online employment market. E-Gen is a system which meets this
challenge as fast and judiciously as possible. We chose emails as the input format, which is the most frequent mode of com-
munication in this Ô¨Åeld. An e-mail inbox receives messages sometimes with an attached Ô¨Åle containing the job offer. When a
job offer is published online, a particular segmentation is required by the job search sites. Firstly, the job offer language is
identiÔ¨Åed by using n-grams. Then, E-Gen parses the e-mail, splits the job offer into thematic segments, and retrieves relevant
information (contract, salary, starting date, location, etc.) to generate an XML document for the job offer. Subsequently, a
Ô¨Åltering and lemmatisation process is applied to the text, and is represented in a vector space model (VSM). A categorization
of text segments (preamble, skills or proÔ¨Åle, mission) is obtained by using a SVM classiÔ¨Åer ( Fan, Chen, &amp; Lin, 2005 ). This pre-
liminary classiÔ¨Åcation is then transmitted to a ‚Äò‚Äòcorrective‚Äô‚Äô post-process which improves the quality of the solution (Module
1, described in Kessler et al., 2007 ). Preliminary experiments showed that segment categorization without segment position
in job posting is not enough and may be a source of errors. In order to avoid this kind of error, we have decided to consider
each job posting as produced by a succession of states in a Markov machine and we have applied a post-processing, based on
the Viterbi algorithm ( Viterbi, 1967 ). During the publication of a job offer, Aktor generates a temporary e-mail address for
applying to the job. Each e-mail is redirected to human resources software (Gestmax8) to be read by a recruiting consultant.
At this step, E-Gen analyses the candidate‚Äôs answers to identify each part of the application and extracts the text from the e-mail
and attached Ô¨Åles (by using wvWare9and pdftotext10).
After a pre-processing task, we use a combination of rules and machine learning methods to separate each distinct part
(CVor CL). We use a vector representation of each document with a label (CVor CL). With a learning set of 2.000 documents
of each type, the system gets very good performance (F-score between 0.95 and 0.98). This process (Module 2 represented by
the lowest box in Fig. 1 ) is more fully described in Kessler et al. (2008b) . Once the CL and CV have been identiÔ¨Åed, the CORTEX
system is applied to each document (Cover Letter and CV) and a summary is generated by concatenating high-scoring sen-
tences. Afterwards, E-Gen performs an automated proÔ¨Åling of this application by using measures of similarity and a small
number of applications that have been previously validated as relevant by a recruitment consultant (Module 3). The whole
chain is summarized in Fig. 1 .
5http://gate.ac.uk/ .
6http://twitter.com .
7http://www.zapoint.com .
8http://www.gestmax.fr .
9http://wvware.sourceforge.net .
10http://www.bluem.net/downloads/pdftotext_en . Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135 1127
4. Coupling E-Gen proÔ¨Åling module and the CORTEX system
4.1. E-Gen proÔ¨Åling module4.1.1. Linguistic pre-processing
Firstly, we remove information such as e-mail adresses, the names of candidates, addresses, names of cities in order to
ensure that the applications become anonymous. Then, classic pre-processing is applied to textual information (job offer,
CV, and CL). French accents are deleted and capital letters are converted to lower case. This pre-processing task is performed
to obtain a representation well suited for the Vector Space Model (VSM). In order to avoid the introduction of noise into the
models, the following items are also deleted: verbs and functional words (to be, to have, to need, etc.), common expressions
with a stop word
11list (for example, that is, each of, etc.), numbers (in numeric and/or textual format), symbols such as ‚Äò‚Äò$‚Äô‚Äô, ‚Äò‚Äò#‚Äô‚Äô,
‚Äò‚Äò‚ÅÑ‚Äô‚Äô. Finally, lemmatisation12is performed to signiÔ¨Åcantly reduce the size of the lexicon. All these processes allow us to repre-
sent the collection of documents through the bag-of-words paradigm (a matrix of frequencies of terms (columns) for each can-
didate answer (rows)). To improve Ô¨Åltering, we tried parsing applications with different signiÔ¨Åcant terms (like ‚Äò‚ÄòPersonal
Information‚Äô‚Äô, ‚Äò‚ÄòEducation‚Äô‚Äô, ‚Äô‚ÄôWork Experience‚Äô‚Äô, etc.) and extract only paragraphs with the relevant information, but initial tests
showed a decline in results due to the great variability of signiÔ¨Åant terms and order of paragraphs.
4.1.2. Proximity between applications and job offer using similarity measures
After the step of linguistic pre-processing, each document is transformed into a vector with weights characterizing the
frequency of terms Tf. Some tests with Tf-idf (Salton &amp; Mcgill, 1986 ) were made but they offered no improvement. We have
established a strategy using measures of similarity, to rank all applications in relation to a job offer. We combined different
similarity measures between the candidate‚Äôs answers (CV and CL) and the associated job offer. We decided to use several
similarity measures as deÔ¨Åned in Bernstein, Kaufmann, Kiefer, and Bnrki (2005) : Cosine (Eq. (1)), which calculates the angle
between job offer and each candidate answer, Minkowski distances (Eq. (2))(p= 1 for Manhattan, p= 2 for Euclidean). The
last measure used is Okabis (Eq. (3))(Bellot &amp; El-B√®ze, 2001 ). Based on the formula of Okapi ( Robertson, Walker, Jones, Han-
cock-Beaulieu, &amp; Gatford, 1994 ), this measure is often used in Information Retrieval. To combine these measures, we use an
Algorithm Decision (AD) ( Boudin &amp; Torres Moreno, 2007 ), which weights the values obtained by each measure of similarity.
Several other similarity measures (Overlap, Enertex, Needleman-Wunsch, Jaro-Winkler, Jensen-Shannon divergence) have
been tested but they are not retained in this study, because the results obtained were disapointing. All measures used
and their combinations are described in Kessler, B√©chet, Roche, El-B√®ze, and Torres-Moreno (2008a) .
candidatures
rankingInternet
Job offer
processingJob offer
publication
CVSplitting
candidate‚Äôs
e-mailsProfilingModule 1
Module 2Module 3DescriptionTitle
Mission
ProfileCandidatecompanies
LIA
CORTEX
SystemCL
Relevance
Feedback
Fig. 1. System overview.
11http://sites.univ-provence.fr/veronis/donnees/index.html .
12Lemmatisation Ô¨Ånds the root of verbs and transforms plural and/or feminine words into masculine singular form. So we conÔ¨Çate terms developer,
development, developing, to develop into develop. Author's personal copy 1128 R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
cosine √∞j;d√û¬ºPn
i¬º1ji/C1diÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉPn
i¬º1j2
i/C1Pn
i¬º1d2
iq √∞1√û
Minkowski √∞j;d√û¬º1
1√æPn
i¬º1jji/C0dijp/C0/C1 1
p√∞2√û
Okabis √∞j;d√û¬ºP
i2d\jPn
i¬º1ji/C1di
Pn
i¬º1ji/C1di√æÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É
jdjp
Md√∞3√û
where jis a job offer, dis a candidate answer, ia term, jiand dioccurrence of irespectively in jand d, and Mdtheir average
size.
4.1.3. Relevance Feedback
We previously changed the system to incorporate a process of Relevance Feedback ( Sparck Jones, 1970 ). Relevance Feed-
back is a standard method used particulary for manual query reformulation. For example, the user carefully checks the an-
swer set resulting from an initial query, and then reformulates the query. Rocchio‚Äôs algorithm ( Rocchio, 1971 ) and variations
have found wide usage in Information Retrieval and related areas such as Text Categorisation ( Joachims, 1997 ). Relevance
Feedback has been proposed in Smyth and Bradley (2003) to help the user to Ô¨Ånd a job with server logs from the jobFinder
site.13In our system, Relevance Feedback takes into account the recruiting consultant‚Äôs choice during a Ô¨Årst evaluation of a few
CVs. Our goal is not a system capable of Ô¨Ånding the best candidate, but a system capable of reproducing the judgement of therecruitment consultant. It is critical for recruiters not to miss a promising candidate that they may have unfortunately rejected.
The goal of this Relevance Feedback approach is to help them to avoid this kind of error. We assume that successful candidates
have similar proÔ¨Åles or, at least, that they have much in common. This approach uses documents returned in response to a Ô¨Årst
request to improve the search results ( Salton &amp; Buckley, 1990 ). In this case, we randomly take a few candidate answers (1‚Äì6 in
our experiments) from all relevant candidate answers. These selected candidate answers are added to the job offer. So, we use
manual Relevance Feedback to reÔ¨Çect user judgements in the resulting ranking. We increase the vector representation with the
terms from the candidates considered relevant by a recruitment consultant. The system will recompute the similarity between
the candidate‚Äôs answer that we evaluate and the job offer enriched with relevant candidates. This allows Sim
0to be recalculed
for each measure of similarity between the application evaluated and the job offer expanded by relevant applications of the
relevance feedback process:
Sim0
measure √∞j;d√û¬ºSim measure √∞j;dkp1k/C1/C1/C1k pn√û √∞4√û
where jis a job offer, dis a candidate‚Äôs response, piis arelevant candidate‚Äôs response, nare numbers of retained applications
for Relevance Feedback and kis the concatenation operator.
The results, presented in Kessler et al. (2009) and hereafter called ISMIS Result showed an improvement in the quality of
the ranking obtained for each application added to the process of relevance feedback. However, we suspected that a lot of
unnecessary information was still kept in the evaluation and we wanted to use a Ô¨Ålter to take into account the content of
sentences. Each document contains additional information (hobbies, greeting and complimentary close, etc.) and standard
pre-processing only partially removes it. The idea was to use a system of automatic summarization, coupled to E-Gen, as
a powerful Ô¨Ålter capable of removing non-essential information contained in CV and Cover Letters.
4.2. The CORTEX summarization system
Automatic summarization is useful to cope with ever increasing volumes of information. An abstract is, by far, the most
concrete and recognized kind of text condensation. However, the CV is already a kind of summary, with a very important
structure. We suspect that the Ô¨Åltering system of automatic summarization may not be useful in this case. Since the CL is
in free text, we used CORTEX (Torres-Moreno, St-Onge, Gagnon, El-B√®ze, &amp; Bellot, 2009, 2001 ), an efÔ¨Åcient state-of-art summa-
rization system, in order to retain the more informative segments of the CL.
Each document of the application is transmitted to the CORTEX system which provides a summary based on the requested
size. CORTEX is a document extract summarization system using an optimal decision algorithm that combines several metrics.
These metrics result from processing statistical and informational algorithms on the document vector space representation.
Fig. 2 presents an overview of the system.
The idea is to represent the text in an appropriate vectorial space and apply numeric processings to it. In order to reduce
complexity, a pre-processing of the document is performed: words are Ô¨Åltered, lemmatized, and stemmed. Based on the
terms that remain in the text after Ô¨Åltering, a frequency matrix cis built in the following way: Each element cl
iof this matrix
represents the number of occurrences of the word iin the sentence l.
13JobFinder ( http://www.jobÔ¨Ånder.com ). Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135 1129
c¬ºc1
1c12 ...c1
i ...c1
NL
c21c22 ...c2
i ...c2
NL
..................
cl
1cl
2 ...cl
i...cl
NL
..................
cNS
1cNS
2 ...cNS
i...cNS
NL2
66666666666643
7777777777775;
cl
i2f0;1;2;...g√∞ 5√û
Another matrix n, called a binary virtual orpresence matrix , is deÔ¨Åned as:
nl
i¬º1i f cl
i‚Äì0
0 elsewhere()
√∞6√û
Each line of these matrices represents a sentence of the text. Matrices candcTare the frequency matrix of the sentences
and frequency matrix of the titles respectively.
The CORTEX system can use up to C= 11 metrics ( Torres-Moreno, Velazquez-Morales, &amp; Meunier, 2002 ) to evaluate the sen-
tence‚Äôs relevance.
The system scores each sentence with a decision algorithm which relies on the normalized metrics. Two averages are cal-
culated, a positive ks&gt; 0.5, and a negative ks&lt; 0.5 tendency (the case ks= 0.5 is ignored). The following algorithm combines
the vote of each metric:
Psa¬ºPC
v¬º1kv
s/C13/C13/C13/C13/C00:5/C0/C1
;kv
s/C13/C13/C13/C13&gt;0:5
Ps
b¬ºPC
v¬º10:5/C0kv
s/C13/C13/C13/C13/C0/C1
;kv
s/C13/C13/C13/C13&lt;0:5
Cis the number of metrics and vis the index of the metrics. The value given to each sentence sis calculated with:
ifPsa&gt;Ps
b/C18/C19
then Scorecortex
s ¬º0:5√æPsa=C: retain s
else Scorecortex
s ¬º0:5/C0Psb=C: not retain s
Fig. 2. CORTEX overview. Author's personal copy 1130 R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
The sentences are then ranked according to the obtained values. Depending on the desired compression rate, the sorted
sentences will be used to produce the summary. The CORTEX system is applied to each document (Cover Letter) and a sum-
mary is generated by concatenating high-scoring sentences. We generated several abstracts with a variable compression rate
(5%, 10%, 20%, ..., 50%, 75% of the size of the documents, in sentences) in order to test the impact of our powerful Ô¨Ålter on the
E-Gen system. The entire process chain is illustrated in Fig. 1 . The best compression rates are generally with 30% ( Torres-
Moreno et al., 2009 ). The results are presented in Section 5.3.
5. Experiments
We selected a data subset from Aktor‚Äôs database composed of 1917 candidates. This subset is called the Mission Corpus .I t
has a size of 10 MB of raw texts and contains 1,375,000 words. The Mission Corpus is composed of a set of 12 job offers cov-
ering various themes (jobs in accountancy, business, computer science, etc.) and their candidates. Each Job Offer is associated
with at least six candidates identiÔ¨Åed as relevant . As described in Kessler et al. (2008a) , each document is segmented to keep
the relevant parts (we remove the description of the company (D) for the job offer). Each candidate answer is tagged as rel-
evant orirrelevant .Arelevant value corresponds to a potential candidate for a speciÔ¨Åc job chosen by the recruiting consul-
tant. An irrelevant value is associated with an unsuitable candidate for the job (this is a decision made by the manager of a
human resources company). Our study was conducted on French job offers because the French market represents Aktor‚Äôs
main activity. Table 1 shows a few statistics about the Mission Corpus .
5.1. Example of CL summaries
Fig. 3 presents14an example of an original Cover Letter and Fig. 4 . Itscorresponding summary15generated by the CORTEX sys-
tem with a 30% compression rate (in number of sentences).
All the documents of Mission Corpus were previously made anonymous. We observe that the original CL contains a
number of useless information for ranking, such as addresses, phone numbers or form of address at the beginning or
end of the letter. The last part of the CL is generally as ‚Äò‚ÄòYours faithfully‚Äô‚Äô, ‚Äò‚ÄòYours sincerely‚Äô‚Äô, ‚Äò‚ÄòBest regards‚Äô‚Äô, all of which
represent irrelevant information. We further observe in Fig. 4 that the summary obtained with CORTEX removes all this
information.
5.2. Experimental protocol
We measured the similarity between a job offer and its candidate‚Äôs responses. These measures (Section 4.1.2 ) rank the
candidate‚Äôs answers by computing a similarity between a job offer and the associated candidate answers. We use the
ROC curves to evaluate the quality of the ranking obtained. ROC curves ( Ferri, Flach, &amp; Hernandez-Orallo, 2002 ) come from
the Ô¨Åeld of signal processing. They are used in medicine to evaluate the validity of diagnostic tests. In our case, ROC curves
show the rate of irrelevant candidate answers on the X-axis and the rate of relevant candidate answers on the Y-axis. The
14Pierre ASPRE
26 years old
19 Verdun street 92870 Vannes06-06-06-06-06.Subject: collaboration offerVannes, November 27th, 2008Dear Sir,The Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing proÔ¨Åtability. With his legal knowledge in t ax
and social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices and
knowing how to deal with the late settlement of invoices.Therefore I offer my skills. They allow me to:‚Äì Manage with rigueur the accounts of a company.‚Äì Ensure legal compliance activities (payroll, tax billing etc.).‚Äì Provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis.I suggest we meet to discuss all the terms of our future cooperation.I look forward to hearing from you.
Best regards.
Pierre ASPRE.
15Pierre ASPRE
Subject: collaboration offerThe Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing proÔ¨Åtability. With his legal knowledge in t ax
and social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices andknowing how to deal with late settlement of invoices.‚Äì ensure legal compliance activities (payroll, tax billing etc.).
‚Äì provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis. Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135 1131
Area Under the Curve (AUC) can be interpreted as the effectiveness of a measurement of interest. In the case of candidate
answers ranking, a perfect ROC curve corresponds to obtaining all relevant candidate answers at the beginning of the list
and all irrelevant ones at the end. This situation corresponds to AUC = 1. The diagonal line corresponds to the performance
of a random system, progress of the rate of relevant candidates being accompanied by an equivalent degradation in the rate
of irrelevant candidates. This situation corresponds to AUC = 0.5, as explained in Fawcett (2006) . An effective measurementFig. 3. Example of full Cover Letter.Table 1
Mission corpus statistics.
Number Job title Number of candidate answers Number of
Relevant Irrelevant
34861 Sales engineer 40 14 26
31702 Accountant, department suppliers 55 23 3233633 Sales engineer 65 18 4734865 Accountant assistant 67 10 57
34783 Accountant assistant 108 9 99
33746 3 chefs 116 60 5633553 Trade commissioner 117 17 10033725 Urban sales consultant 118 43 7531022 Recruitment assistant 221 28 19331274 Accountant assistant junior 224 26 19834119 Sales assistant 257 10 24731767 Accountant assistant junior 437 51 386
Total 1917 323 1594
Fig. 4. Summary of Cover Letter (see Fig. 3 ) at a 30% compression rate. Author's personal copy 1132 R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
of interest to order candidate‚Äôs answers consists in obtaining the highest AUC value. This is strictly equivalent to minimizing
the sum of the ranks of the relevant candidate‚Äôs answers. ROC curves are resistant to imbalance (for example, an imbalance
in the number of positive and negative examples) ( Roche &amp; Kodratoff, 2006 ). For each job offer, we evaluated the quality of
the ranking obtained by this method. Candidate answers considered are only those composed of CV and CL.
5.3. Results
In this section, we present the results obtained by combining the CORTEX system with the E-Gen ranking application. CORTEX
was used as an additional Ô¨Ålter which generates a summary of each document before E-Gen evaluation. We keep the struc-
ture of data for job offers as described in Kessler et al. (2008a) . A job offer is composed of a Description (D), a Title (T), a
Mission (M), and a ProÔ¨Åle (P). For these experiments, we use two combinations of a job offer content, keeping only Title, Mis-
sion, ProÔ¨Åle (TMP) and all information of a job offer (DTMP). Results are presented in Tables 2 and 3 . Each column presents a
part of the application with different sizes of summaries for each line (75%, 50%, ..., 5%). Full text is a result obtained with
100% of the document and was published previously in Kessler et al. (2008a, 2009) .
Table 2 presents results obtained for each part of the application separately. We observe that AUC of CVs remains below
the baseline whatever the percentage of compression. We notice however a gradual decrease in AUC scores depending on the
percentage of compression. We explain this by the fact that a CV is already a summary of the most important information
about the candidates and thereby attempting to summarize degrades Ô¨Ånal results. We apply the same process with cover
letters. Performance is still low overall for CLs in comparison with CVs, however, there is a slight increase in AUC scores with
a compression rate of 30%. We explain these results by particular information contained in a cover letter such as the form of
address at the beginning or end of the letter (see Fig. 4 ) which are noise for the ranking system of E-Gen. Results with TMP
segmentation (i.e. conserving only Title, Mission, and ProÔ¨Åle of job offer) are of better quality.
Table 3 presents the results obtained by combining both parts of the application. Full text values are computed with the
whole documents of the application. The Ô¨Årst two columns show the results obtained by combining the summary of the CV
and the CL. We observe again a deterioration in the results when trying to summarize the CV. Even if results are lower, it
should be noted, however, that the best score is again obtained at 30%. The last two columns present the results with a sum-
marized CL and the full CV. We observe an overall improvement of the AUC score and the best results with a compression
rate of 30% of the Cover Letter.
Next step is to combine summaries of the cover letter, which suppresses noise and enriches the offer with the Relevance
Feedback process. Table 4 presents the results obtained with different sizes of Relevance Feedback (RF1 corresponds to one
application added to the job offer, RF2 two applications added to the job offer, etc.). Each application added with the rele-
vance feedback process consists in a full CV and a summary of the cover letter with a compression rate of 30%. A random
distribution of applications produces an AUC approximately at 0.5 like explained in Fawcett (2006) . We compare ISMIS Result
with those obtained using a summary of the cover letter. Each test is carried out 100 times with a random distribution of
relevant applications for Relevance Feedback. Then we compute an average of AUC scores obtained (the curve shows theTable 2
Results of CL or CV according to the compression rate of Cortex and part of job offer (with or without Description part).
CORTEX compression rate (%) CV + DTMP CV + TMP CL + DTMP CL + TMP
100 (full text) 0.622 0.648 0.567 0.560
75 0.565 0.575 0.563 0.55650 0.558 0.569 0.553 0.56040 0.552 0.565 0.561 0.56530 0.549 0.560 0.569 0.571
20 0.520 0.558 0.564 0.566
10 0.559 0.559 0.543 0.5545 0.550 0.542 0.521 0.523
Table 3Results for CV and cover letter according to the compression rate.
CORTEX compression rate (%) CV and CL summaries Full CV and CL summary
DTMP TMP DTMP TMP
100 (full text) 0.634 0.642 0.634 0.642
75 0.521 0.581 0.639 0.64150 0.556 0.551 0.643 0.64940 0.544 0.568 0.643 0.651
30 0.570 0.587 0.646 0.653
20 0.569 0.533 0.641 0.65210 0.564 0.534 0.631 0.6455 0.546 0.547 0.638 0.649 Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
average for each size). In fact, we compute the Residual Ranking ( Billerbeck &amp; Zobel, 2006 ): Documents that are used for Rel-
evance Feedback are removed from the collection before ranking with the reformulated query. We assume that the Rele-
vance Feedback process would behave as a reinforcement learning ( Sutton &amp; Barto, 1998 ) but it is impossible to
experiment RF nwith n&gt; 6 with this corpus because the number of relevant candidates is too small for some job offers
(see Table 1 ). We observe a slight improvement in results for almost any size of Relevance Feedback. We are conscious that
the performance gain is low, however, it conÔ¨Årms previous results on the Cover Letter. Fig. 5 shows this improvement. This
Ô¨Ågure conÔ¨Årms that the addition of just one relevant candidate (RF1) enables the AUC value to be enhanced (i.e. an improve-
ment of 0.5‚Äì1.2%). This Relevance Feedback (i.e. RF1) is not very time-consuming for the expert.
Fig. 6 shows detailed results of one test. For clarity reasons, we present only 3 of the 12 jobs of our dataset in order to
compare results with and without CORTEX (for each job, RFCare AUC scores with CORTEX and RFwithout CORTEX ).
For standard system, we observe a positive progress from 1% to 10% for 10 jobs between RF0 and RF1 (e.g. Ô¨Åve jobs have
an improvement between 5% and 10%). Note that between RF0 and RF6, 6 jobs have a signiÔ¨Åcant positive progress between
10% and 12%. The combination of the E-Gen and CORTEX systems improve standard system results for Ô¨Åve jobs from 1% to 5%
between RF0 and RF1. Between RF0 and RF6, the Cortex version improves E-Gen‚Äôs results for eight jobs from 1% to 5%.
The study of the results shows that job offer 31702 contains some relevant applications with a bad labeling (CV are la-
beled CL and CL are only a hyperlink to a CV). The reduction of information on the main document of the application leads
the system version using summaries to degrade the AUC scores. Job offer 34861 shows a good improvement with each size of
relevance feedback (RF0:0.65, RF1:0.70, RF6:0.73) and with CORTEX (RF0:0.68, RF1:0.72, RF6:0.79). The detailed study of re-
sults shows that job offer 33746 contains some empty applications labeled relevant. This leads the system with and without
CORTEX to degrade Ô¨Ånal results. In the same way, an application added without CL explains the identical score in RF2 between
RF and RFC for job offer 31274.</body>
  <conclusion>Job offer processing is a difÔ¨Åcult and highly subjective task. The retrieval of relevant information concerning job descrip-
tions and skills is not a trivial task ( Loth et al., 2010 ) and results on this type of document have been quite low ( Clech &amp;Table 4
Comparison of AUC score for each size of Relevance Feedback with CORTEX summarization system.
Size of Relevance Feedback ISMIS result Full CV and CL summary 30% compression rate
Random distribution 0.500 0.500
RF0 0.642 0.653RF1 0.654 0.658
RF2 0.657 0.659
RF3 0.659 0.661RF4 0.659 0.659RF5 0.660 0.662RF6 0.661 0.663
Fig. 5. Results of Relevance Feedback with and without summaries of CL. Author's personal copy 1134 R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135
Zighed, 2003 ). The information we use in this kind of process is not well formated in natural language, but follows a conven-
tional structure. This paper deals with the CORTEX summarizer and the E-Gen system for processing job offers. E-Gen assists an
employer in the recruitment task. This paper focuses on candidate answers to job offers. We rank the candidate answers by
using different similarity measures and different document representations in a vector space model. We use a process of rel-
evance feedback to perform reinforcement learning, whereby each new application added to the process assists in the deci-
sion-making. We choose to evaluate the quality of our approaches by computing Area Under the Curve .CORTEX is a
summarization system using an optimal decision algorithm that combines several metrics. We present the results obtained
by combining both systems. AUC obtained with summarized cover letter at 30% of compression size and a full CV shows a
slight improvement in the results. As future work, we plan to apply other techniques, such as Ô¨Ånding discriminant features of
irrelevant applications using the Rocchio algorithm ( Rocchio, 1971 ), weighting the different parts of an application, etc. in
order to improve results. We also plan to use a categorization of jobs to take into consideration similar jobs, such as ‚Äô‚Äôdevel-
oper‚Äô‚Äô and ‚Äò‚Äòprogrammer‚Äô‚Äô. Finally we propose to measure the CV quality by building an evaluation on an Internet portal. Our
aim with this evaluation is to present a job-seeker with a list of the most suitable job ads according to his proÔ¨Åle.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Audras, I., &amp; Ganascia, J.-G. (2006). Apprentissage du frantais langue TtrangFre et TALN: Analyses de corpus Tcrits a l‚Äôaide d‚Äôoutils d‚Äôextraction a utomatique
du langage. In J.-M. Viprey (Ed.), 8Fmes JournTes d‚ÄôAnalyse de DonnTes Textuelles (pp. 67‚Äì78). Univ. de Franche ComtT, Besanton 2006.
Bellot, P., &amp; El-B√®ze, M. (2001). ClassiÔ¨Åcation et segmentation de textes par arbres de dTcision. In TSI(Vol. 20, pp. 107‚Äì134). HermFs.
Ben Abdessalem Karaa, W. (2009). Web-based recruiting: A framework for cvs handling. In Second international conference on web and information
technologies ‚Äò‚ÄòICWIT‚Äô09‚Äô‚Äô, kerkennah Island, Sfax, Tunisia, June 12‚Äì14 (pp. 395‚Äì406).
Bernstein, A., Kaufmann, E., Kiefer, C., &amp; Bnrki, C. (2005). Simpack: A generic java library for similarity measures in ontologies. Tech. rep., Unive rsity of Zurich
Department of Informatics.
Billerbeck, B., &amp; Zobel, J. (2006). EfÔ¨Åcient query expansion with auxiliary data structures. Information Systems, 31 (7), 573‚Äì584.
Boudin, F., &amp; Torres Moreno, J. M. (2007). Neo-cortex: A performant user-oriented multi-document summarization system. In CICLing (pp. 551‚Äì562).
Bourse, M., LeclFre, M., Morin, E., &amp; Trichet, F. (2004). Human resource management and semantic web technologies. In ICTTA 2004 Damascus Syria (pp. 641‚Äì
642).
Cazalens, S., &amp; Lamarre, P. (2001). An organization of internet agents based on a hierarchy of information domains. In Proceedings MAAMAW‚Äô2001, Annecy,
France (pp. 573‚Äì584).
Clech, J., &amp; Zighed, D. A. (2003). Data mining et analyse des cv: une exp√©rience et des perspectives. In EGC‚Äô03 Revue des Sciences et Technologies de
l‚ÄôInformation (Vol. 17, pp. 83‚Äì92). Lyon.
Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F. M., Mongiello, M., &amp; Mottola, M. (2003). A formal approach to ontology-based semantic match of ski lls
descriptions. Journal of Universal Computer Science, Special issue on Skills Management, 9 , 1437‚Äì1454.
Dorn, J., &amp; Naz, T. (2007). Meta-search in human resource management. In Proceedings of 4th international conference on knowledge systems ICKS‚Äô07
Bangkok,Thailand (pp. 105‚Äì110).
Enrica, A., &amp; Iezzi, D. F. (2006). Recruitment via web and information technology: A model for ranking the competences in job market. In JADT‚Äô2006,
Besanton, France (pp. 79‚Äì88).
Fan, R.-E., Chen, P.-H., &amp; Lin, C.-J. (2005). Working set selection using the second order information for training SVM. Journal of Machine Learning Research ,
1889‚Äì1918.
Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27 , 861‚Äì874.
Ferri, C., Flach, P., &amp; Hernandez-Orallo, J. (2002). Learning decision trees using the area under the ROC curve. In Proceedings of ICML 2002: Sydney, NSW,
Australia (pp. 139‚Äì146).
Gorenak, I., &amp; Mlaker KaF, S. S. O. (2010). Cross-cultural comparison of online job advertisements. JLST, Journal of Logistics and Sustainable Transport, 2 , 37‚Äì52 Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124‚Äì1135 1135
Joachims, T. (1997). A probabilistic analysis of the rocchio algorithm with tÔ¨Ådf for text categorization. In ICML 1997, Nashville, Tennessee, USA (pp. 143‚Äì151).
San Francisco, CA, USA.
Kessler, R., B√©chet, N., Roche, M., El-B√®ze, M., &amp; Torres-Moreno, J. M. (2008a). Automatic proÔ¨Åling system for ranking candidates answers in human
resources. In OTM ‚Äô08 in Monterrey, Mexico (pp. 625‚Äì634).
Kessler, R., B√©chet, N., Roche, M., El-B√®ze, M., &amp; Torres-Moreno, J. M. (2009). Job offer management: How improve the ranking of candidates . Prague: ISMIS.
431‚Äì441.
Kessler, R., Torres-Moreno, J. M., &amp; El-B√®ze, M. (2007). E-Gen: Automatic job offer processing system for human ressources. In MICAI, Aguscalientes, Mexique
(pp. 985‚Äì995).
Kessler, R., Torres-Moreno, J. M., &amp; El-B√®ze, M. (2008b). E-Gen: ProÔ¨Ålage automatique de candidatures. In TALN 2008, Avignon, France (pp. 370‚Äì379).
Loth, R., Battistelli, D., Chaumartin, F., De Mazancourt, H., Minel, J. L., &amp; Vinckx, A. (2010). Linguistic information extraction for job ads (SIRE p roject). In
RIAO‚Äô2010 9th conference 28‚Äì30 April, Paris, France (pp. 300‚Äì303).
Marchal, E., Mellet, K., &amp; Rieucau, G. (2007). Job board toolkits: Internet matchmaking and changes in job advertisements. Human Relations, 60 (7),
1091‚Äì1113.
Mocho, M., Paslaru, E., &amp; Simperl, B. (2006). Practical guidelines for building semantic e-recruitment applications. In I-Know‚Äô06 special track on advanced
semantic technologies, Graz, Austria, September 2006 .
Morin, E., LeclFre, M., &amp; Trichet, F. (2004). The semantic web in e-recruitment. In The Ô¨Årst European symposium of semantic Web (ESWS‚Äô2004) (pp. 67‚Äì78).
Quilan, J. (1993). C4.5: Programs for machine learning . San Mateo, CA, San Francisco, CA, USA: Morgan Kaufmann.
Rafter, R., Bradley, K., &amp; Smyt, B. (2000). Automated collaborative Ô¨Åltering applications for online recruitment services. In International conference on adaptive
hypermedia and adaptive web-based systems, Trento, Italy (pp. 363‚Äì368).
Robertson, S., Walker, S., Jones, S., Hancock-Beaulieu, M. M., &amp; Gatford, M. (1994). Okapi at trec-3. NIST Special Publication 500-225: TREC-3, pp. 1 09‚Äì126.
Rocchio, J. (1971). Relevance feedback in information retrieval. In The smart system: Experiments in automatic document processing (pp. 313‚Äì323). Prentice-
Hall.
Roche, M., &amp; Kodratoff, Y., 2006. Pruning terminology extracted from a specialized corpus for CVontology acquisition. In OTM‚Äô06, Montpellier, France (pp.
1107‚Äì1116).
Roche, M., &amp; Prince, V. (2008). Evaluation et dTtermination de la pertinence pour des syntagmes candidats a la collocation. In JADT (pp. 1009‚Äì1020).
Salton, G., &amp; Buckley, C. (1990). Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science , 288‚Äì297.
Salton, G., &amp; Mcgill, M. J. (1986). Introduction to modern information retrieval . New York, NY, USA: McGraw-Hill Inc.
Smyth, B., &amp; Bradley, K. (2003). Personalized information ordering: A case-study in online recruitment. Journal of Knowledge-Based Systems , 269‚Äì275.
Sparck Jones, K. (1970). Some thoughts on classiÔ¨Åcation for retrieval. Journal of Documentation , 89‚Äì101.
Sutton, R. S., &amp; Barto, A. G. (1998). Reinforcement learning: An introduction (adaptive computation and machine learning) . The MIT Press.
Tolksdorf, R., Mocho, M., Heese, R., Oldakowski, R., &amp; Christian, B. (2006). Semantic-Web-Technologien im Arbeitsvermittlungsprozess.
Wirtschaftsinformatik , 17‚Äì26.
Torres-Moreno, J. M., Vel√°zquez-Morales, P., &amp; Meunier, M. (2001). CORTEX, un algorithme pour la condensation automatique de textes. In ARCo (Vol. 2, pp.
365‚Äì371).
Torres-Moreno, J. M., St-Onge, P.-L., Gagnon, M., El-B√®ze, M., &amp; Bellot, P. (2009). Automatic summarization system coupled with a question-answeri ng
system (qaas). In CoRR abs/0905.2990.
Torres-Moreno, J. M., Velazquez-Morales, P., &amp; Meunier, J. (2002). Condens√©s de textes par des m√©thodes num√©riques. JADT, St Malo, France, 2 , 723‚Äì734.
Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13 ,
260‚Äì269.
Yahiaoui, L., Boufaƒ± ¬®da, Z., &amp; Pri√©, Y. (2006). Semantic annotation of documents applied to e-recruitment. In SWAP 2006 ‚Äì Semantic web applications and
perspectives . ISSN: 1613-0073.</biblio>
</article>