<article>
  <preamble>acl2012.pdf</preamble>
  <titre>Finding Salient Dates for Building Thematic Timelines</titre>
  <auteurs>
    <auteur>
      <name>Rémy Kessler</name>
      <mail>kessler@limsi.fr</mail>
      <affiliation>LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>Xavier Tannier</name>
      <mail>xtannier@limsi.fr</mail>
      <affiliation>Univ. Paris-Sud,
LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>Caroline Hagege</name>
      <mail>hagege@xrce.xerox.com</mail>
      <affiliation>Xerox Research Center Europe,
Meylan, France</affiliation>
    </auteur>
    <auteur>
      <name>Véronique Moriceau</name>
      <mail>moriceau@limsi.fr</mail>
      <affiliation>Univ. Paris-Sud, LIMSI-CNRS
Orsay, France</affiliation>
    </auteur>
    <auteur>
      <name>André Bittar</name>
      <mail>bittar@xrce.xerox.com</mail>
      <affiliation>Xerox Research Center Europe, Meylan, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>We present an approach for detecting salient (important) dates in texts in order to automatically build event timelines from a search query (e.g. the name of an event or person, etc.). This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). In order to extract salient dates that warrant inclusion in an event timeline, we first recognize and normalize temporal expressions in texts and then use a machine-learning approach to extract salient dates that relate to a particular topic. We focused only on extracting the dates and not the events to which they are related.</abstract>
  <introduction>Our aim here was to build thematic timelines for a general domain topic defined by a user query. This task, which involves the extraction of important events, is related to the tasks of Retrospective Event Detection (Yang et al., 1998), or New Event Detection, as defined for example in Topic Detection and Tracking (TDT) campaigns (Allan, 2002). The majority of systems designed to tackle this task make use of textual information in a bag-ofwords manner. They use little temporal information, generally only using document metadata, such as the document creation time (DCT). The few systems that do make use of temporal information (such as the now discontinued Google timeline), only extract absolute, full dates (that feature a day, month and year). In our corpus, described in Section 3.1, we found that only 7% of extracted temporal expressions are absolute dates. We distinguish our work from that of previous researchers in that we have focused primarily on extracted temporal information as opposed to other textual content. We show that using linguistic temporal processing helps extract important events in texts. Our system extracts a maximum of temporal information and uses only this information to detect salient dates for the construction of event timelines. Other types of content are used for initial thematic document retrieval. Output is a list of dates, ranked from most important to least important with respect to the given topic. Each date is presented with a set of relevant sentences. We can see this work as a new, easily evaluable task of “date extraction”, which is an important component of timeline summarization. In what follows, we first review some of the related work in Section 2. Section 3 presents the resources used and gives an overview of the system. The system used for temporal analysis is described in Section 4, and the strategy used for indexing and finding salient dates, as well as the results obtained, are given in Section 51.</introduction>
  <body>The ISO-TimeML language (Pustejovsky et al., 2010) is a specification language for manual annotation of temporal information in texts, but, to the best of our knowledge, it has not yet actually been used in information retrieval systems. Neverthe1This work has been partially funded by French National Research Agency (ANR) under project Chronolines (ANR-10CORD-010). We would like to thank the French News Agency (AFP) for providing us with the corpus. less, (Alonso et al., 2007; Alonso, 2008; Kanhabua, 2009) and (Mestl et al., 2009), among others, have highlighted that the analysis of temporal information is often an essential component in text understanding and is useful in a wide range of information retrieval applications. (Harabagiu and Bejan, 2005; Saquete et al., 2009) highlight the importance of processing temporal expressions in Question Answering systems. For example, in the TREC-10 QA evaluation campaign, more than 10% of questions required an element of temporal processing in order to be correctly processed (Li et al., 2005a). In multidocument summarization, temporal processing enables a system to detect redundant excerpts from various texts on the same topic and to present results in a relevant chronological order (Barzilay and Elhadad, 2002). Temporal processing is also useful for aiding medical decision-making. (Kim and Choi, 2011) present work on the extraction of temporal information in clinical narrative texts. Similarly, (Jung et al., 2011) present an end-to-end system that processes clinical records, detects events and constructs timelines of patients’ medical histories. The various editions of the TDT task have given rise to the development of different systems that detect novelty in news streams (Allan, 2002; Kumaran and Allen, 2004; Fung et al., 2005). Most of these systems are based on statistical bag-of-words models that use similarity measures to determine proximity between documents (Li et al., 2005b; Brants et al., 2003). (Smith, 2002) used spatio-temporal information from texts to detect events from a digital library. His method used place/time collocations and ranked events according to statistical measures. Some efforts have been made for automatically building textual and graphical timelines. For example, (Allan et al., 2001) present a system that uses measures of pertinence and novelty to construct timelines that consist of one sentence per date. (Chieu and Lee, 2004) propose a similar system that extracts events relevant to a query from a collection of documents. Important events are those reported in a large number of news articles and each event is constructed according to one single query and represented by a set of sentences. (Swan and Allen, 2000) present an approach to generating graphical timelines that involves extracting clusters of noun phrases and named entities. More recently, (Yan et al., 2011b; Yan et al., 2011a) used a summarizationbased approach to automatically generate timelines, taking into account the evolutionary characteristics of news. 3 Resources and System Overview 3.1 AFP Corpus For this work, we used a corpus of newswire texts provided by the AFP French news agency. The English AFP corpus is composed of 1.3 million texts that span the 2004-2011 period (511 documents/day in average and 426 millions words). Each document is an XML file containing a title, a date of creation (DCT), set of keywords, and textual content split into paragraphs. 3.2 AFP Chronologies AFP “chronologies” (textual event timelines) are a specific type of articles written by AFP journalists in order to contextualize current events. These chronologies may concern any topic discussed in the media, and consist in a list of dates (typically between 10 and 20) associated with a text describing the related event(s). figure 1 shows an example of such a chronology. Further examples are given in figure 2. We selected 91 chronologies satisfying the following constraints: • All dates in the chronologies are between 2004 and 2011 to be sure that the related events are described in the corpus. For example, “Chronology of climax to Vietnam War” was excluded because its corresponding dates do not appear in the content of the articles. • All dates in the chronology are anterior to the chronology’s creation date. For example, the chronology “Space in 2005: A calendar”, published in January 2005 and listing scheduled events, was not selected (because almost no rocket launches finally happened on the expected day). • The temporal granularity of the chronology is the day. For example, “A timeline of how the London transport attacks unfolded”, relating the events hour by hour, is not in our focus. in <NewsML Version="1.2"> <NewsItem xml:lang="en"> dates <HeadLine>Key Thaicrisis</HeadLine> land’s political <DateId>20100513T100519Z</DateId> <NameLabel>Thailand-politics</NameLabel> <DataContent> <p>The following is a timeline of events since the protests began, soon after Thailand’s Supreme Court confiscated 1.4 billion dollars of Thaksin’s wealth for abuse of power.</p> <p>March 14: Tens of thousands of Red Shirts demonstrate in the capital calling for Abhisit’s government to step down, [...]</p> <p>March 28: The government and the Reds enter into talks but hit a stalemate after two days [...]</p> <p>April 3: Tens of thousands of protesters move from Bangkok’s historic district into the city’s commercial heart [...]</p> <p>April 7: Abhisit declares state of emergency in capital after Red Shirts storm parliament.</p> <p>April 8: Authorities announce arrest warrants for protest leaders.</p> . . . </DataContent> </NewsItem> </NewsML> figure 1: Example of an AFP manual chronology. learning and evaluation purposes, For all chronologies were converted to a single XML format. Each document was manually associated with a user search query made up of the keywords required to retrieve the chronology. 3.3 System Overview figure 3 shows the general architecture of the system. first, pre-processing of the AFP corpus tags and normalizes temporal expressions in each of the articles (step (cid:172) in the figure). Next, the corpus is indexed by the Lucene search engine2 (step (cid:173)). Given a query, a number of documents are retrieved by Lucene ((cid:174)). These documents can be filtered ((cid:175)), and dates are extracted from the remaining documents. These dates are then ranked in order to show the most important ones to the user ((cid:176)), to2http://lucene.apache.org Chronology of 18 months of trouble in Ivory Coast Chechen rebels’ history of hostage-takings Iraqi political wrangling since March 7 election Athletics: Timeline of men’s 800m world record Major accidents in Chinese mines Space in 2005: A calendar Developments in Iranian nuclear standoff Chronology of climax to Vietnam War Timeline of ex-IMF chief’s sex attack case A timeline of how the London transport attacks unfolded figure 2: Examples of AFP chronologies. figure 3: System overview. gether with the sentences that contain them. 4 Temporal and Linguistic Processing In this section, we describe the linguistic and temporal information extracted during the pre-processing phase and how the extraction is carried out. We rely on the powerful linguistic analyzer XIP (A¨ıtMokhtar et al., 2002), that we adapted for our purposes. 4.1 XIP The linguistic analyzer we use performs a deep syntactic analysis of running text. It takes as input XML files and analyzes the textual content enclosed in the various XML tags in different ways that are specified in an XML guide (a file providing instructions to the parser, see (Roux, 2004) for details). XIP performs complete linguistic processing ranging from tokenization to deep grammatical dependency analysis. It also performs named entity recog nition (NER) of the most usual named entity categories and recognizes temporal expressions. Linguistic units manipulated by the parser are either terminal categories or chunks. Each of these units is associated with an attribute-value matrix that contains the unit’s relevant morphological, syntactic and semantic information. Linguistic constituents are linked by oriented and labelled n-ary relations denoting syntactic or semantic properties of the input text. A Java API is provided with the parser so that all linguistic structures and relations can be easily manipulated by Java code. In the following subsections, we give details of the linguistic information that is used for the detection of salient dates. 4.2 Named Entity Recognition Named Entity (NE) Recognition is one of the outputs provided by XIP. NEs are represented as unary relations in the parser output. We used the existing NE recognition module of the English grammar which tags the following NE types: location names, person names and organization names. Ambiguous NE types (ambiguity between type location or organization for country names for instance) are also considered. 4.3 Temporal Analysis A previous module for temporal analysis was developed and integrated into the English grammar (Hag`ege and Tannier, 2008), and evaluated during TempEval campaign (Verhagen et al., 2007). This module was adapted for tagging salient dates. Our goal with temporal analysis is to be able to tag and normalize3 a selected subset of temporal expressions (TEs) which we consider to be relevant for our task. This subset of expressions is described in the following sections. 4.3.1 Absolute Dates Absolute dates are dates that can be normalized without external or contextual knowledge. This is the case, for instance, of “On January 5th 2003”. In these expressions, all information needed for normalization is contained in the linguistic expression. 3We call normalization the operation of turning a temporal expression into a formated, fully specified representation. This includes finding the absolute value of relative dates. However, absolute dates are relatively infrequent in our corpus (7%), so in order to broaden the coverage for the detection of salient dates, we decided to consider relative dates, which are far more frequent. 4.3.2 DCT-relative Dates DCT-relative temporal expressions are those which are relative to the creation date of the document. This class represents 40% of dates extracted from the AFP corpus. Unlike the absolute dates, the linguistic expression does not provide all the information needed for normalization. External information is required, in particular, the date which corresponds to the moment of utterance. In news articles, this is the DCT. Two sub-classes of relative TEs can be distinguished. The first sub-class only requires knowledge of the DCT value to perform the normalization. This is the case of expressions like next Friday, which correspond to the calendar date of the first Friday following the DCT. The second sub-class requires further contextual knowledge for normalization. For example, on Friday will correspond either to last Friday or to next Friday depending on the context where this expression appears (e.g. He is expected to come on Friday corresponds to next Friday while He arrived on Friday corresponds to last Friday). In such cases, the tense of the verb that governs the TE is essential for normalization. This information is provided by the linguistic analysis carried out by XIP. 4.3.3 Underspecified Dates Considering the kind of corpus we deal with (news), we decided to consider TEs whose granularity is at least equal to a day. As a result, TEs were normalized to a numerical YYYYMMDD format (where YYYY corresponds to the year, MM to the month and DD to the day). In case of TEs with a granularity superior to the day or month, DD and MM fields remain unspecified accordingly. However, these underspecified dates are not used in our experiments. 4.4 Modality and Reported Speech An important issue that can affect the calculation of salient dates is the modality associated with timestamped events in text. For instance, the status of a salient date candidate in a sentence like “The meet ing takes place on Friday” has to be distinguished from the one in “The meeting should take place on Friday” or “The meeting will take place on Friday, Mr. Hong said”. The time-stamped event meeting takes place is factual in the first example and can be taken as granted. In the second and third examples, however, the event does not necessarily occur. This is expressed by the modality introduced by the modal auxiliary should (second example), or by the use of the future tense or reported speech (third example). We annotate TEs with information regarding the factuality of the event they modify. More specifically, we consider the following features: Events that are mentioned in the future: If a time-stamped event is in the future tense, we add a specific attribute MODALITY with value FUTURE to the corresponding TE annotation. Events used with a modal verb: If a timestamped event is introduced by a modal verb such as should or would, then attribute MODALITY to the corresponding TE annotation has the value MODAL. Reported speech verbs: Reported speech verbs (or verbs of speaking) introduce indirect or reported speech. We dealt with time-stamped events governed by a reported speech verb, or otherwise appearing in reported speech. Once again, XIP’s linguistic analysis provided the necessary information, including the marking of reported speech verbs and clause segmentation of complex sentences. If a relevant TE modifies a reported speech verb, the annotation of this TE contains a specific attribute, DECLARATION=”YES”. If the relevant TE modifies a verb that appears in a clause introduced by a reported speech verb then the annotation contains the attribute REPORTED=”YES”. Note that the different annotations can be combined (e.g. modality and reported speech can occur for a same time-stamped event). For example, the TE Friday in “The meeting should take place on Friday, Mr. Hong said” is annotated with both modality and reported speech attributes. 4.5 Corpus-dependent Special Cases While we developed the linguistic and temporal annotators, we took into account some specificities of our corpus. We decided that the TEs today and <EC TYPE="LOCORG">Iraq</EC>, <DCT value="20050105"/> <EC TYPE="TIMEX" value="unknown">The year 2004</EC> was the deadliest <EC TYPE="TIMEX" value="unknown">in a decade</EC> for journalists around the world, mainly because of the number of reporters killed in the <EN TYPE="ORG">Reporters media Sans Borders) <EC TYPE="DATE" SUBTYPE="REL" REF="ST" DECLARATION="YES" value ="20050105">Wednesday</EC>. figure 4: Example of XIP output for a sample article. rights group Frontieres</EN> (Reporters Without said Implementation and Example now were not relevant for the detection of salient dates. In the AFP news corpus, these expressions are mostly generic expressions synomymous with nowadays and do not really time-stamp an event with respect to the DCT. Another specificity of the corpus is the fact that if the DCT corresponds to a Monday, and if an event in a past tense is described with the associated TE on Monday or Monday, it means that this event occurs on the DCT day itself, and not on the Monday before. We adapted the TE normalizer to these special cases. 4.6 As said previously, a NER module is integrated into the XIP parser, which we used “as is”. The TE tagger and normalizer was adapted from (Hag`ege and Tannier, 2008). We used the Java API provided with the parser to perform the annotation and normalization of TEs. The output for the linguistic and temporal annotation consists in XML files where only selected information is kept (structural information distinguishing headlines from news content, DCT), and enriched with the linguistic annotations described before (NEs and TEs with relevant attributes corresponding to the normalization and typing). Information concerning modality, future tense and reported speech, appears as attributes on the TE tag. figure 4 shows an example of an analyzed excerpt of a news article. In this news excerpt, only one TE (Wednesday) is normalized as both The year 2004 and in a decade are not considered to be relevant. The first one being a generic TE and the second one being of granularity superior to a year. The annotation of the relevant TE has the attribute indicating that it time-stamps an event realized by a reported speech verb. The nor malized value of the TE corresponds to the 5th of January 2005, which is a Wednesday. NEs are also annotated. In the entire AFP corpus, 11.5 millions temporal expressions were detected, among which 845,000 absolute dates (7%) and 4.6 millions normalized relative dates (40%). Although we have not yet evaluated our tagging of relative dates, the system on which our current date normalization is based achieved good results in the TempEval (Verhagen et al., 2007) campaign. 5 Experiments and Results In Section 5.1, we propose two baseline approaches in order to give a good idea of the difficulty of the task (Section 5.4 also discusses this point). In Section 5.2, we present our experiments using simple filtering and statistics on dates calculated by Lucene. finally, Section 5.3 gives details of our experiments with a learning approach. In our experiments, we used three different values to rank dates: • occ(d) is the number of textual units (documents or sentences) containing the date d. • Lucene provides ranked documents together with their relevance score. luc(d) is the sum of Lucene scores for textual units containing the date d. • An adaptation of classical tf.idf for dates: tf.idf (d) = f (d).log N df (d) where f (d) is the number of occurrences of date d in the sentence (generally, f (d) = 1), N is the number of indexed sentences and df (d) is the number of sentences containing date d. In all experiments (including baselines), timelines have been built by considering only dates between the first and the last dates of the corresponding manual chronology. Processing runs were evaluated on manually-written chronologies (see Section 3.2) according to Mean Average Precision (MAP), which is a widely accepted metric for ranked lists. MAP gives a higher weight to higher ranked elements than lower ranked elements. Significance of evaluation results are indicated by the p-value results of Student’s t-test (t(90) = 1.9867). BLocc abs 0.2627 Baselines “only DCTs” DCT BLtf.idf Model DCT BLluc BLocc DCT 0.5521 0.5036 MAP Score 0.5523 Baselines “only absolute dates” BLtf.idf Model BLluc abs abs 0.2782 0.2778 MAP Score Baselines “absolute dates or alternatively DCTs” mix BLtf.idf Model BLluc mix MAP Score 0.4110 0.4135 Table 1: MAP results for baseline runs. BLocc mix 0.4005 5.1 Baseline Runs BLDCT . Indexing and search were done at document level (i.e. each AFP article, with its title and keywords, is a document). Given a query, the top 10,000 documents were retrieved. In these runs, only the DCT for each document was considered. Dates were ranked by one of the three values described above (occ, luc or tf.idf) leading to runs BLocc DCT and BLtf idf DCT . DCT , BLluc abs, BLluc abs and BLtf idf abs . BLabs. Indexing and search were done at sentence level (document title and keywords are added to sentence text). Given a query, the top 10,000 sentences were retrieved. Only absolute dates in these sentences were considered. We thus obtained runs BLocc Note that in this baseline, as well as in all the subsequent runs, the information unit was the sentence because a date was associated to a small part of the text. The rest of the document generally contained text that was not related to the specific date. BLmix. Same as BLabs, except that sentences containing no absolute dates were considered and associated to the DCT. Table 1 shows results for these baseline runs. Using only DCTs with Lucene scores or tf.idf(d) already yielded interesting results, with MAP around 0.55. 5.2 Salient Date Extraction with XIP Results and Simple filtering In these experiments, we considered a Lucene index to be built as follows: each document was taken to 0.6982 0.6962 SDtf.idf MAP Score MAP Score Model Model Salient date runs with all dates SDluc Salient dates runs with filtering SDtf.idf 0.6996 0.6975 SDluc R R 0.6993 ∗∗ SDtf.idf 0.6967 SDluc F F 0.7005 ∗ SDtf.idf 0.6978 SDluc M M 0.7091 ∗∗ 0.7066 ∗∗ SDtf.idf SDluc D D 0.7112 ∗∗ 0.7086 ∗∗ SDtf.idf SDluc F M D F M D RF M D 0.7146 ∗∗ RF M D 0.7127 ∗∗ SDtf.idf SDluc Table 2: MAP results for salient date extraction with XIP and simple filtering. Significance of filtering improvement is indicated by Student t-test against no filtering (∗: p &lt; 0.05 (significant); ∗∗: p &lt; 0.01 (highly significant)). Improvement of using tf.idf (d) rather than occ(d) is also highly significant. be a sentence containing a normalized date. This sentence was indexed with the title and keywords of the AFP article containing it. Given a query, the top 10,000 documents were retrieved. Combinations between the following filtering operations were possible, by removing all dates associated with a reported speech verb (R), a modal verb (M) and/or a future verb (F ). All these filtering operations were intended to remove references to events that were not certain, thereby minimizing noise in results. These processing runs are named SD runs, with indices representing the filtering operations. For example, a run obtained by filtering modal and future verbs is called SDM,F . In all combinations, dates were ranked by the sum of Lucene scores for these sentences (luc) or by tf.idf 4. Table 2 presents the results for this series of experiments. MAP values are much higher than for baselines. Using tf.idf (d) is only very slightly better than luc. filtering operations bring some improvement but the benefits of these different techniques have to be further investigated. 5.3 Machine-Learning Runs We used our set of manually-written chronologies as a training corpus to perform machine learning experiments. We used IcsiBoost5, an implementa4We do not present runs where dates are ranked by the number of times they appear in retrieved sentences (occ), as we did for baselines, since results are systematically lower. 5http://code.google.com/p/icsiboost/ tion of adaptative boosting (AdaBoost (Freund and Schapire, 1997)). In our approach, we consider two classes: salient dates are dates that have an entry in the manual chronologies, while non-salient dates are all other dates. This choice does, however, represent an important bias. The choices of journalists are indeed very subjective, and chronologies must not exceed a certain length, which means that relevant dates can be thrown away. These issues will be discussed in Section 5.4. The classifier instances were not all sentences retrieved by the search engine. Using all sentences would not yield a useful feature set. We rather aggregated all sentences corresponding to the same date before learning the classifier. Therefore, each instance corresponded to a single date, and features were figures concerning the set of sentences containing this date. Features used in this series of runs are as follows: 1. Features representing the fact that the more a date is mentioned, the more important it is likely to be: 1) Sum of the Lucene scores for all sentences containing the date 2) Number of sentences containing the date 3) Ratio between the total weights of the date and weights of all returned dates 4) Ratio between the frequency of the date and frequency of all returned dates; 2. Features representing the fact that an important event is still written about, a long time after it occurs: 1) Distance between the date and the most recent mention of this date 2) Distance between the date and the DCT; 3. Other features: 1) Lucene’s best ranking of the date 2) Number of times where the date is absolute in the text 3) Number of times where the date is relative (but normalized) in the text 4) Total number of keywords of the query in the title, sentence and named entities of retrieved documents 5) Number of times where the date modifies a reported speech verb or is extracted from reported speech. We did not aim to classify dates, but rather to rank them. Instead, we used the predicted probability P (d) returned by the classifier, and mixed it with the Lucene score of sentences, or with date tf.idf : MAP Score Model Machine-Learning Runs 0.7033 0.7905 0.7918 M Lluc base M Lluc M Ltf.idf ∗∗ ∗∗ Table 3: MAP results for salient date extraction with machine-learning. M Lluc base used Lucene scores and only the first set of features described above. M Lluc and M Ltf.idf used the three sets of features. They are both highly significant under t-test (p ≈ 6.10−4). score(d) = P (d) × val(d) where val(d) is either luc(d) or tf.idf (d). Because the task is very subjective and (above all) because of the low quantity of learning data, we prefered not to opt for a “learning to rank” approach. We evaluated this approach with a classic 4-fold cross-validation. Our 91 chronologies were randomly divided into 4 sub-samples, each of them being used once as test data. The final scores, presented in Table 3, are the average of these 4 processes. As shown in this table, the learning approach improves MAP results by about 0.05 point.</body>
  <conclusion>This article presents a task of “date extraction” and shows the importance of taking temporal information into consideration and how with relatively simple temporal processing, we were able to indirectly point to important events using the temporal information associated with these events. Of course, as our final goal consists in the detection of important events, we need to take into account the textual content. In future work, we envisage providing, together with the detection of salient dates, a semantic analysis that will help determine the importance of events. Another interesting direction in which we soon aim to work is to consider all textual excerpts that are associated with salient dates, and use clustering techniques to determine if textual excerpts correspond to the same event or not. finally, as our news corpus is available both for English and French (comparable corpus, not necessarily translations), we aim to investigate cross-lingual extraction of salient dates and salient events.</conclusion>
  <discussion>Chronologies hand-written by journalists are a very useful resources for evaluation of our system, as they are completely dissociated from our research and are an exact representation of the output we aim to obtain. However, assembling such a chronology is a very subjective task, and no clear method for evaluation agreement between two journalists seems immediately apparent. Only experts can build such chronologies, and calculating this agreement would require at least two experts from each domain, which are hard to come by. One may then consider our system as a useful tool for building a chronology more objectively. To illustrate this point, we chose four specific topics6 and showed one of our runs on each topic to an AFP expert for these subjects. We asked him to assess the first 30 dates of these runs. 6Namely, “Arab revolt timeline for Morocco”, “Kyrgyzstan unrest timeline”, “Lebanon’s new government: a timeline”, “Libya timeline”. Topic Morocco Kyrgyzstan Libya Lebanon APC 0.5847 0.6125 0.7856 0.4673 APE 0.5718 0.9989 1 0.7652 Table 4: Average precision results for manual evaluation on 4 topics, against the original chronologies (APC), and the expert assessment (APE). Table 4 presents results for this evaluation, comparing average precision values obtained 1) against the original, manual chronologies (APC), and 2) against the expert assessment (APE). These values show that, for 3 runs out of 4, many dates returned by the system are considered as valid by the expert, even if not presented in the original chronology. Even if this experiment is not strong enough to lead to a formal conclusion (post-hoc evaluation with only 4 topics and a single assessor), this tends to show that our system produces usable outputs and that our system can be of help to journalists by providing them with chronologies that are as useful and objective as possible.</discussion>
  <biblio>Salah A¨ıt-Mokhtar, Jean-Pierre Chanod, and Claude Roux. 2002. Robustness beyond Shallowness: Incremental Deep Parsing. Natural Language Engineering, 8:121–144James Allan, Rahul Gupta, and Vikas Khandelwal. 2001Temporal summaries of new topics. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’01, pages 10–18James Allan, editor. 2002. Topic Detection and TrackingSpringerOmar Alonso, Ricardo Baeza-Yates, and Michael Gertz2007In SIGCHI 2007 Workshop on Exploratory Search and HCI WorkshopOmar Rogelio Alonso. 2008. Temporal information retrieval. Ph.D. thesis, University of California at Davis, Davis, CA, USA. Adviser-Gertz, MichaelInferRegina Barzilay and Noemie Elhadadring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Artificial Intelligence Research, 17:35–55Thorsten Brants, Francine Chen, and Ayman Farahat2003. A system for new event detection. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 330–337, New York, NY, USA. ACMHai Leong Chieu and Yoong Keok Lee. 2004. Query based event extraction along a timeline. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’04, pages 425–432Yoav Freund and Robert E. Schapire. 1997. A DecisionTheoretic Generalization of On-Line Learning and an Application to Boosting. Journal of Computer and System Sciences, 55(1):119–139Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu, and Hongjun Lu. 2005. Parameter free bursty events detection in text streams. In VLDB ’05: Proceedings of the 31st international conference on Very large data bases, pages 181–192Caroline Hag`ege and Xavier Tannier. 2008. XTM: A Robust Temporal Text Processor. In Computational Linguistics and Intelligent Text Processing, proceedings of 9th International Conference CICLing 2008, pages 231–240, Haifa, Israel, February. Springer Berlin / HeidelbergSanda Harabagiu and Cosmin Adrian Bejan2005Question Answering Based on Temporal Inference. In Proceedings of the Workshop on Inference for Textual Question Answering, Pittsburg, Pennsylvania, USA, JulyHyuckchul Jung, James Allen, Nate Blaylock, Will de Beaumont, Lucian Galescu, and Mary Swift. 2011Building timelines from narrative clinical records: initial results based-on deep natural language understanding. In Proceedings of BioNLP 2011 Workshop, BioNLP ’11, pages 146–154, Stroudsburg, PA, USAAssociation for Computational LinguisticsNattiya Kanhabua. 2009. Exploiting temporal inforIn Promation in retrieval of archived documentsceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 1923, 2009, page 848Youngho Kim and Jinwook Choi2011. Recognizing temporal information in korean clinical narratives through text normalization. Healthc Inform Res, 17(3):150–5Giridhar Kumaran and James Allen. 2004. Text classification and named entities for new event detectionIn SIGIR ’04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 297–304ACMWei Li, Wenjie Li, Qin Lu, and Kam-Fai Wong. 2005aA Preliminary Work on Classifying Time Granularities of Temporal Questions. In Proceedings of Second international joint conference in NLP (IJCNLP 2005), Jeju Island, Korea, octZhiwei Li, Bin Wang, Mingjing Li, and Wei-Ying Ma2005b. A Probabilistic Model for Restrospective In Proceedings of the 28th News Event DetectionAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval, Salvador, Brazil. ACM Press, New York City, NY, USAThomas Mestl, Olga Cerrato, Jon Ølnes, Per Myrseth, and Inger-Mette Gustavsen. 2009. Time Challenges Challenging Times for Future Information Search. DLib Magazine, 15(5/6)James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent Romary. 2010Iso-timeml: An international standard for semantic annotation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA)Claude Roux. 2004. Annoter les documents XML avec In 11`eme Confrence un outil d’analyse syntaxiqueannuelle de Traitement Automatique des Langues Naturelles, Fs, Maroc, April. ATALA Estela Saquete, Jose L. Vicedo, Patricio Mart´ınez-Barco, Rafael Mu˜noz, and Hector Llorens. 2009. Enhancing QA Systems with Complex Temporal Question Processing Capabilities. Journal of Articifial Intelligence Research, 35:775–811David A. Smith. 2002. Detecting events with date and place information in unstructured text. In JCDL ’02: Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries, pages 191–196, New York, NY, USA. ACMRussell Swan and James Allen. 2000. Automatic generation of overview timelines. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’00, pages 49–56, New York, NY, USA. ACMMarc Verhagen, Robert Gaizauskas, Franck Schilder, Mark Hepple, Graham Katz, and James Pustejovsky2007. SemEval-2007 15: TempEval Temporal Relation Identification. In Proceedings of SemEval workshop at ACL 2007, Prague, Czech Republic, June. Association for Computational Linguistics, Morristown, NJ, USARui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011a. Timeline generation through evolutionary trans-temporal summaIn Proceedings of the 2011 Conference on rizationEmpirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, Edinburgh, UK, pages 433–443Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: a balanced optimization In Proceeding framework via iterative substitutionof the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011, pages 745–754Y. Yang, T. Pierce, and J. G. Carbonell. 1998. A study on retrospective and on-line event detection. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia, August. ACM Press, New York City, NY, USA</biblio>

  <preamble>b0e5c43edf116ce2909ae009cc27a1546f09.pdf</preamble>
  <titre>Inclusive yet Selective: Supervised Distributional Hypernymy Detection</titre>
  <auteurs>
    <auteur>
      <name>Stephen Roller</name>
      <mail>roller@cs.utexas.edu</mail>
      <affiliation>Department of Computer Science
The University of Texas at Austin</affiliation>
    </auteur>
    <auteur>
      <name>Katrin Erk</name>
      <mail>katrin.erk@mail.utexas.edu</mail>
      <affiliation>Department of Linguistics
The University of Texas at Austin</affiliation>
    </auteur>
    <auteur>
      <name>Gemma Boleda</name>
      <mail>gemma.boleda@upf.edu</mail>
      <affiliation>Department of Linguistics
The University of Texas at Austin</affiliation>
    </auteur>
  </auteurs>
  <abstract>We test the Distributional Inclusion Hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. We find that this hypothesis only holds when it is applied to relevant dimensions. We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion.</abstract>
  <introduction>One of the main criticisms of distributional models has been that they fail to distinguish between semantic relations: Typical nearest neighbors of dog are words like cat, animal, puppy, tail, or owner, all obviously related to dog, but through very different types of semantic relations. On these grounds, Murphy (2002) argues that distributional models cannot be a valid model of conceptual representation. Distinguishing semantic relations are also crucial for drawing inferences from distributional data, as different semantic relations lead to different inference rules (Lenci, 2008). This is of practical import for tasks such as Recognizing Textual Entailment or RTE (Geffet and Dagan, 2004). For these reasons, research has in recent years started to attempt the detection of specific semantic relationships, and current results suggest that distributional models can, in fact, distinguish between semantic relations, given the right similarity measures (Weeds et al., 2004; Kotlerman et al., 2010; Lenci and Benotto, 2012; Herbelot and Ganesalingam, 2013; Santus, 2013). Because of its relevance for RTE and other tasks, much of this work has focused on hypernymy. Hypernymy is the semantic relation between a superordinate term in a taxonomy (e.g. animal) and a subordinate term (e.g. dog). Distributional approaches to date for detecting hypernymy, and the related but broader relation of lexical entailment, have been unsupervised (except for Baroni et al. (2012)) and have mostly been based on the Distributional Inclusion Hypothesis (Zhitomirsky-Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009), which states that more specific terms appear in a subset of the distributional contexts in which more general terms appear. So, animal can occur in all the contexts in which dog can occur, plus some contexts in which dog cannot – for instance, rights can be a typical cooccurrence for animal (e.g. “animal rights”), but not so much for dog (e.g. #“dog rights”). This paper takes a closer look at the Distributional Inclusion Hypothesis for hypernymy detection. We show that the current best unsupervised approach is brittle in that their performance depends on the space they are applied to. This raises the question of whether the Distributional Inclusion Hypothesis is correct, and if so, under what circumstances it holds. We use a simple supervised approach to relation detection that has good performance (accuracy .84 on BLESS, .85 on the lexical entailment dataset of Baroni et al. (2012)) and works well across different spaces.1 Furthermore, we show that it can be interpreted as selecting dimensions for which the Distributional Inclusion Hypothesis does hold. So, our answer is to propose the Selective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesis holds, but only for relevant dimensions. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1Code and data are available at http://stephenroller.com/research/coling14.</introduction>
  <body>Distributional models. Distributional models represent a word through the contexts in which it has been observed, usually in the form of a vector representation (Turney and Pantel, 2010). A target word is represented as a vector in a high-dimensional space in which the dimensions are context items (for example, other words) and the coordinates of the vector indicate the target’s degree of association with each context item. In this paper, we also use dimensionality reduced spaces in which dimensions do not stand for individual context items anymore. Pattern-based approaches to inducing semantic relations. Early work on automatically inducing semantic relations between words, starting with Hearst (1992), uses textual patterns. For example, “[NP1] and other [NP2]” implies that NP2 is a hypernym of NP1. Pattern-based approaches have been applied to meronymy (Berland and Charniak, 1999; Girju et al., 2003; Girju et al., 2006), synonymy (Lin et al., 2003), co-hyponymy (Snow et al., 2005), hypernymy (Cimiano et al., 2005), and several relations between verbs (Chklovski and Pantel, 2004). Pantel and Pennachiotti (2006) generalize the idea to a wide variety of relations. Turney (2006) uses vectors of patterns to determine similarity of semantic relations. A task related to semantic relation induction is the extension of an existing taxonomy (Buitelaar et al., 2005). Snow et al. (2006) do this by using hypernymy and co-hyponymy detectors. Lexical entailment, hypernymy, and the Distributional Inclusion Hypothesis. Weeds et al. (2004) introduce the notion of distributional generality, where v is distributionally more general than u if u appears in a subset of the contexts in which v is found, and speculate that hypernyms (v) should be more distributionally general than hyponyms (u). Zhitomirsky-Geffet and Dagan (2005; 2009) introduce the term Distributional Inclusion Hypothesis for the idea that distributional generality encodes hypernymy or the more loosely defined relation of lexical entailment. Weeds and Weir (2003) measure distributional generality using a notion of precision (eq. 1). Here and in all equations below, u is the narrower term, and v the more general one. Abusing notation, we write u for both a word and its associated vector hu1, . . . , uni. Kotlerman et al. (2010) predict lexical entailment with the balAPinc measure, a modification of the Average Precision (AP) measure (eq. 2). The general notion is that scores should increase with the number of dimensions of v that u shares, and also give more weight to the highly ranked dimensions (i.e. largest magnitude) of the narrower term u. This is captured in APinc by computing precision P (r) at every rank r among u’s dimensions – where precision is the fraction of dimensions shared with v –, and weighting by the rank of the same dimension in the broader term, rel0(v, r, u). The final measure, balAPinc, smooths using the LIN similarity measure (Lin, 1998). (We only sketch this measure here due to its complexity; details are given in Kotlerman et al. (2010).) ( Pn 1 if x > 0; 1(x) = Pn 0 otherwise i=1 ui · 1(vi) P|1(u)| W eedsP rec(u, v) = i=1 ui r=1 P (r) · rel0(v, r, u)) p |1(u)| APinc(u, v) · LIN(u, v) balAPinc(u, v) = APinc(u, v) = (1) (2) The ClarkeDE measure (Clarke, 2009) computes degree of entailment as the degree to which the narrower term u has lower values than v across all dimensions (eq. 3). Lenci and Benotto (2012) introduce the invCL measure, which uses ClarkeDE to measure both distributional inclusion of u in v and distributional non-inclusion of v in u (eq. 4). While all other measures interpret the Distributional Inclusion Hypothesis as the degree to which a ⊆ relation holds, Lenci and Benotto test the degree to which proper inclusion ( holds. They consider not only the degree to which the contexts of the narrower terms are included in the contexts of the wider term, but also determine the degree to which the wider term has contexts that the narrower term does not have. 1027 Pn Pn i=1 min(ui, vi) CL(u, v) = p i=1 ui CL(u, v) · (1 − CL(v, u)) (3) invCL(u, v) = (4) Like Lenci and Benotto, we focus on the stricter hypernymy relation, rather than lexical entailment. We believe that the different relations that make up lexical entailment have different distributional indications and that, for that reason, it will be easier to detect the relations separately than together. Baroni et al. (2012) proposes a supervised approach to hypernymy detection that represents two words as the concatenation of their vectors. They also mention in passing another supervised approach that represents two words as the component-wise difference of their vectors. These are broadly the two approaches that we test, though we introduce significant modifications. 3 Data 3.1 Distributional Vector Spaces We use three standard types of distributional spaces. U+W2: This space is based on a concatenation of the Gigaword, BNC, English Wackypedia and ukWaC corpora (Baroni et al., 2009). The corpora are POS-tagged and lemmatized. We keep only content words (nouns, proper nouns, adjectives and verbs) with a corpus frequency of 500 or larger. The resulting U+ corpus has roughly 133K word types and 2.8B word tokens. We created a vector space by counting co-occurrences of these word types within a window of two words on the left and the right, using the top 20k most frequent content words as dimensions. The space was transformed using Positive Pointwise Mutual Information (PPMI). U+Sent: The U+Sent space is constructed the same way as U+W2, but uses full sentence contexts instead of 2-word windows. TypeDM: This space is extracted from the TypeDM tensors (Baroni and Lenci, 2011). TypeDM contains a list of weighted tuples, hhw1, l, w2i, σi, where w1 and w2 are content words, l is a corpus-derived syntagmatic relationship between the words, and σ is a weight estimating saliency of the relationship. We construct vectors for every unique w1 using the set of hl, w2i pairs as dimensions and corresponding σ values as dimension weights. We select TypeDM for its excellent performance in previous comparisons of distributional hypernymy measures (Lenci and Benotto, 2012). Reduced Spaces: In some experiments, we use dimensionality reduced spaces. We reduce all three spaces to 300 dimensions using Singular Value Decomposition. We use a subscript to denote reduced spaces, e.g. U+W2300. When necessary, we use the term original dimensions to refer to the vector dimensions from the original, non-reduced spaces (e.g. U+W2); the term latent dimensions refers to the dimensions in the reduced spaces (e.g. U+W2300). 3.2 Evaluation Data Sets BLESS: The BLESS data set (Baroni and Lenci, 2011) covers 200 concepts, or concrete and unambiguous terms (divided into 17 different general concept classes, including vehicle and ground mammal), and their relationships to other nouns, called relata. Example concepts include van and horse. Each concept is related to several relata through different semantic relations. Following Lenci and Benotto (2012), we focus on the four semantic relations where both concepts and relata are nouns, for a total 14K data points: Hypernymy, denoting a superset relationship (e.g. animal-dog); Co-hyponymy, denoting words that share a common hypernym (e.g. dog-cat); Meronymy, denoting a part-whole relationship (e.g. tail-dog); and Random, denoting no relationship between the words (e.g. dog-computer). 1028 figure 1: Distributions of relata invCL scores for the U+W2, U+Sent, and TypeDM spaces for each of the semantic relations, after per-concept z-normalization. ENTAILMENT: (Baroni et al., 2012): The ENTAILMENT data set consists of 2,770 word pairs, balanced between positive (house-building) and negative (leader-rider) examples of hypernymy, with 1376 unique hyponyms and 1016 unique hypernyms. The positive examples were generated by selecting direct hypernym relationships from WordNet, the negative examples by randomly permuting the hypernyms of the positive examples, and then manually checking correctness. 4 Distributional Inclusion across Spaces We test several unsupervised distributional approaches to hypernymy detection from the literature, focusing on the underlying vector space representation as the main parameter that we vary. We use the three spaces described in Section 3. We test four hypernymy detection approaches, all of them similarity measures based on the Distributional Inclusion Hypothesis: WeedsPrec, balAPinc, ClarkeDE, and invCL. Our baseline is the standard cosine measure. We evaluate on the BLESS dataset. To evaluate on BLESS, we follow the evaluation scheme laid out in Baroni and Lenci (2011). Given a space and similarity measure, we compute similarity for each concept and relatum. For each concept, we select its nearest neighbors (according to the given similarity measure) in each of the four relations (COHYP, HYPER, MERO, RANDOM), and transform the corresponding four similarities to z-scores. Across all concepts, this yields four sets of z-normalized similarity scores, one for each relation. These four sets describe the relative similarity of concepts to their nearest neighbors in different relations. Tukey’s Honestly Significant Difference test is used for testing whether scores differ significantly between relations (threshold: p &lt; 0.05). figure 1 shows the distributions of z-scores for invCL for the four relations, with one graph for each of the three spaces we consider. For this illustration, we focus on invCL because it shows the overall best performance at identifying hypernymy. The rightmost plot in figure 1 replicates the analysis of Lenci and Benotto (2012), who used the TypeDM space. It confirms their finding that invCL gives significantly higher values to hypernyms than co-hyponyms – at least on this space. However, in the U+W2 and U+Sent spaces (leftmost and middle plot), invCL clearly loses any ability to rank hypernyms the highest; indeed, in both spaces, co-hyponymy and meronymy both have significantly higher z-scores than hypernymy. Concerning the other measures, we found that they patterned with invCL. On TypeDM, ClarkeDE and WeedsPrec had significantly higher nearest-neighbor values for hypernyms than co-hyponyms.2 On U+W2 and U+Sent, all measures ranked co-hyponyms significantly higher than hypernyms. With the baseline measure, cosine, the similarity ratings for the CO-HYP relation are always the highest, no matter the space, followed by HYPER, MERO, RANDOM in this order. Following Kotlerman et al. (2010) and Lenci and Benotto (2012), we also report the performance of the measures using Mean Average Precision (MAP). Average Precision (AP) is a measure often used in 2balAPinc could not be evaluated on TypeDM due to computational issues. U+W2, invCL l l l l l l l l l l l l ll l l l l l l l l l 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 Co−hyp Hyper Mero Random 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 U+Sent, invCL l ll l l l l l ll l l l l Co−hyp Hyper Mero Random 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 TypeDM, invCL l l l l l l l l l ll ll l l l l ll l l l l l l ll l l l l l l Co−hyp Hyper Mero Random 1029 Measure CO-HYP HYPER MERO RANDOM U+W2 .20 .19 .18 U+Sent .18 .15 .13 TypeDM .19 .35 .36 .68 .66 .60 .66 .66 .59 .78 .45 .38 .27 .28 .31 .28 .29 .34 .20 .25 .27 cosine ClarkeDE invCL cosine ClarkeDE invCL cosine ClarkeDE invCL .27 .28 .28 .28 .28 .29 .29 .32 .33 Table 1: Mean Average Precision for the unsupervised measures on three spaces. the Information Retrieval community with a maximal AP score of 1 when all relevant documents (relata with the right relationship, in our case) are ranked at the top. We compute AP on a per-concept basis and report the mean over all 200 AP values. An advantage of MAP is that, while the BLESS analysis method focuses on nearest neighbors, MAP evaluates the ranking of all relata. A disadvantage of MAP is that it does not test the degree to which a similarity measure separates different semantic relations, like Tukey does, so it may overstate the discriminative power of a particular measure. However, it provides a more intuitive accuracy-like number compared to the BLESS evaluation. Table 1 shows the Mean Average Precision values for cosine, ClarkeDE, and invCL on all three spaces. We also computed WeedsPrec and balAPinc results, obtaining the same picture; we focus on ClarkeDE and invCL because ClarkeDE is a component of invCL, and invCL is the current best measure. The results corresponding to Lenci and Benotto’s are shown in the lowest part of Table 1, where we report numbers for TypeDM. Like Lenci and Benotto, we find that unsupervised measures other than invCL rank cohyponyms the highest, and obtain relatively low results for hypernyms. For invCL in TypeDM, Lenci and Benotto obtain 0.38 MAP for co-hyponyms and a slightly higher 0.40 for hypernyms, though they do not report significance testing results. We obtain 0.38 for co-hyponyms and 0.36 for hypernyms, and the difference is not significant.3 Even though our results are slightly different from those in Lenci and Benotto (2012), both our results and theirs point to at most a weak preference of invCL for hypernyms over co-hyponyms. Moreover, in the U+W2 and U+Sent spaces we see that all three measures are very poor at identifying hypernyms, and the co-hyponymy relation stubbornly persists as most relevant to all three measures, by a large margin. Our results thus constitute a puzzle for the Distributional Inclusion Hypothesis. It seems that there must be some merit to the hypothesis: On one particular space, namely TypeDM, the nearest neighbors in the hypernymy relation had higher similarity scores than any other relation by a significant margin. This was true for all the hypernymy detectors we studied. But even on TypeDM, the MAP evaluation showed at most a weak hypernymy signal, and when spaces other than TypeDM were used, the effect vanished altogether. So how strong an indication for hypernymy can we expect from distributional inclusion measures in general? We will return to this question below, where our answer will be: The Distributional Inclusion Hypothesis seems to hold after all, but it needs to be applied to the right kind of dimensions – and a supervised approach can help in picking the right dimensions. As the unsupervised approaches struggle to detect hypernymy and do not seem robust to changes in standard space parameters, we think it is time to consider supervised approaches. In the next section, we explore two simple supervised approaches that show good performance and are robust to changes in the underlying space. 3Wilcoxon signed-rank test. 1030 5 Supervised Hypernymy Detection We use two simple, supervised models for predicting BLESS and ENTAILMENT relations. The first (Concat) is a model previously proposed by Baroni et al. (2012). The second (Diff) takes up an idea from a footnote in Baroni et al. (2012), but while that footnote stated that the approach in question did not work, we find that, with a few modifications, it obtains the best performance – and can be interpreted as a supervised version of the Distributional Inclusion Hypothesis. Note that while we used unreduced spaces in the previous section, we now use reduced spaces throughout (these are the spaces with the 300 subscript), in order not to have more features than data points. 5.1 Models, Features, and Method Concat: We use a standard Support Vector Machine (SVM) classifier with a concatenation of vectors as input features. SVMs are binary classifiers which learn the maximum margin hyperplane separating the two classes. SVMs employ kernel functions to find the hyperplanes in higher dimensional spaces which are nonlinear in the original space. As feature vectors for the classifier, we follow Baroni et al. (2012) and use the concatenation of the latent dimension vectors representing words. For the ENTAILMENT dataset, we use the concatenation of the hyponym latent vector and the hypernym latent vector for each word pair as training features, and the entails/doesn’t entail annotations as binary targets. For BLESS, we use the concatenation of the concept latent vector and the relatum latent vector as training features, and the four relationship classes as targets. We choose the four-way task rather than a “hypernymy vs. other” classification because BLESS contains many more co-hyponymy and random than hypernymy pairs, which would give a very high baseline in the two-way task. Additionally, the other relations in BLESS, in particular meronymy, may be interesting in their own right. Since SVMs are binary classifiers, we use SciKit-Learn’s default setting to train 6 pairwise-relation one-vs-one classifiers which vote on the final answer. We use a polynomial kernel with a degree of 3 and a penalty term of C = 1.0, and all other hyperparameters are chosen using the SciKit-Learn default values (Pedregosa et al., 2011). No hyperparameters are tuned in any experiment. Diff: Our second classifier is a Logistic Regression (aka MaxEnt) model trained on difference vectors. Logistic Regression is a statistical model for binary classification. It learns a linear hyperplane separating the classes and estimates a probability for classes using a logistic function. We selected Logistic Regression over other possible linear classifiers for its natural ability to give likelihood estimates, which we believe will be useful in future work in an application of hypernymy classification to RTE. As feature vectors, we use a Mikolov-inspired method of representing word pairs as the difference vectors between the two words.4 Baroni et al. (2012) suggested the use of difference vectors as input to a classifier, but reported them as unsuccessful. We found difference vectors to be excellent features, with three important modifications: a linear classifier is better than a nonlinear one; vectors must be normalized to have a magnitude of 1 before taking the difference; and squared difference vectors must also be included as features. So, we represent each word pair with latent vectors (u, v) as a two part vector hf; gi, where fi = uikuk − vikvk , gi = f 2 i . These differences features5 are analogous to a supervised distributional inclusion measure. The difference between two words on a particular dimension captures the degree of distributional inclusion on that dimension. The primary distinction between the difference features and the unsupervised measures is that the supervised classifier learns to weight the importance of different dimensions. The f features encode directional aspects of distributional inclusion: that the hyponym contexts should be included in 4After recent work using subtraction to represent analogy in certain neural-network spaces (Mikolov et al., 2013). 5We also tried variations, such as not normalizing vectors and removing the difference squared vector, but found this setting the best. We also tried the Diff features with an SVM and other nonlinear classifiers, but they performed worse. 1031 Data set Baseline Classifier U+W2300 U+Sent300 TypeDM300 BLESS .46 ENTAILMENT .50 Concat Diff Concat Diff .85 .76 .73 .82 .85 .84 .80 .82 .81 .78 .65 Table 2: Average accuracy of Concat and Diff on BLESS and ENTAILMENT using different spaces for feature generation. those of the hypernym (the weight learned is positive), and the hypernym contexts should not be included in those of the hyponym (the weight learned is negative). So like invCL, this model uses a “proper subset” interpretation of the Distributional Inclusion Hypothesis, but only considers selected dimensions (i.e. those that the model assigns nonzero weights). The difference-squared features (g), on the other hand, typically identify dimensions that are not indicative of hypernymy, by learning negative weights on them (more about this in Section 6). Thus, rather than helping identify hypernyms, they help separate random relations from the rest. We use a L1 regularizer with a strength of C = 1.0. All other hyperparameters are chosen using the SciKit-Learn defaults. Since Diff is also a binary classifier, we use SciKit-Learn’s default setting of training 4 one-vs-all classifiers for BLESS, with the most confident classifier choosing the final answer. Method: For evaluation on BLESS, we hold out one concept and train on the remaining 199 concepts. We also exclude from the training set any pair containing a relatum which appears in the test set. This way, no word that appears in the test set has been seen in training. We report the average accuracy across all concepts. We use the most frequent relation type (random) as our baseline. For the ENTAILMENT data set, we hold out one hyponym and train on all remaining hyponyms. Again, we exclude from training any pair containing a hypernym which appears in the test set. We report average accuracy across all hyponyms. The data set is balanced, so the baseline is 0.5. 5.2 Results Table 2 shows the performance of the two classifiers, Concat and Diff, on both the BLESS and ENTAILMENT datasets, using three underlying spaces. We use the reduced versions of the three spaces, indicated by the subscript 300. Note that the Concat classifier could not converge using features from TypeDM300, so we omit the result. With both methods, we obtain a high accuracy on the two datasets, with results around .8 against baselines around .5. Our best result is .84 on BLESS and .85 on ENTAILMENT. Moreover, both approaches are in general robust to changes in space parameters (with TypeDM/Concat an outlier). Still, the U+W2300 space seems to be the best for this task: Its scores are significantly6 higher than the rest, except for TypeDM on ENTAILMENT, which achieves the same score as U+W2300. Diff achieves significantly higher results than Concat. When provided more information, Concat outperforms Diff. For instance, if cross-validation is done over all pairs in BLESS in the U+W2300 space, Concat achieves .98 accuracy, while Diff obtains .90. However, in this setting the same words appear in the training and test sets (albeit in different pairs). We take this to mean that Concat is memorizing, rather than learning the hypernymy relation. This emphasizes the need for our stricter evaluation that prevents repetition between training and test sets. Clearly, both classifiers do fairly well at predicting hypernymy relations between words, regardless of space. Naturally, one should ask what are the classifiers capturing that the unsupervised measures are missing? We propose that the supervised classifiers perform essentially the same operation as the unsupervised measures, but are learning to determine the relevance of dimensions. In particular, Diff is learning weights on vector difference features. This is equivalent to doing selective distributional inclusion. In the next section, we test this Selective Distributional Inclusion Hypothesis. 6Wilcoxon signed-rank test, p &lt; .001. 1032 figure 2: Distributions of relata scores across concepts using the cosine, ClarkeDE, and invCL measures (after per-concept z-normalization). Here we use the selected dimensions of the U+W2proj space. 6 Selective Distributional Inclusion In order to test how well our supervised model is capturing the notion of selective distributional inclusion, we test each of the unsupervised measures on a smaller space, limited only to the dimensions preferred by the classifier. We emphasize that we do not aim to show that our supervised method outperforms unsupervised methods, but rather that the unsupervised methods benefit greatly from feature selection. Additionally, we analyze which dimensions are selected by the classifier to facilitate understanding of why these dimensions are important. 6.1 Experiment We train the Diff classifier using the dimensionality-reduced U+W2300 space with the same method we use in Section 5. We take the classifier’s learned hyperplane separating hypernyms from other relations, and project the hyperplane back into the original U+W2 space.7 We select the 500 dimensions in the original space that are most relevant according to the classifier weights, and test the unsupervised measures on this new space, which we denote as U+W2proj.8 The 500 most relevant dimensions are selected as follows: We select the 250 most negatively weighted original dimensions using the difference features f. These are the features that have smaller values for hyponyms (e.g. dog) than for hypernyms (e.g. animal), so they characterize hypernymy. We further select the 250 most positively weighted original dimensions using the squared-differences features g. These are the ones where a large difference does not indicate hypernymy. figure 2 shows the boxplots for the BLESS analysis: the distributions of nearest-neighbor similarity scores for the four different semantic relations, for the measures cosine, ClarkeDE, and invCL. We see that invCL now easily discriminates hypernymy from the other relations in the backprojected space. (The difference of HYPER and CO-HYP is significant.) This is even though the space is based on U+W2, where invCL failed to rate hypernyms higher than co-hypernyms in Section 4. Unsurprisingly, cosine, which does not measure distributional inclusion, still prefers CO-HYP. Table 3 shows the MAP scores for three of the measures in the new U+W2proj space. (The results for balAPinc and WeedsPrec are slightly worse than ClarkeDE.) All measures except for cosine assign higher scores to hypernyms than they did in the original space (compare to U+W2 part of Table 1). But it is only invCL that ranks hypernyms significantly higher than co-hyponyms.9 7Ideally we would train on the original space to inspect the relevant dimensions. However, there are more dimensions than examples, so we train on the SVD space and backproject. 8Note that U+W2proj varies slightly from concept to concept, since the hyperplane is learned on a per-concept basis. It is important that we use the linear Diff classifier for this reverse-projection procedure, as the separating hyperplane must be linear in order to complete the projection. In particular, the hyperplane in the Concat classifier cannot be easily backprojected, since it exists in a higher dimensional space than the projection matrix. Furthermore, it is important that we use a classifier trained using the difference features because of its analogy to the Distributional Inclusion Hypothesis. 9Wilcoxon signed-rank test, p &lt; .001. To check that the measures are being improved by the dimension selection and not l l U+W2 proj, cosine 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 l l l l l l l l l Co−hyp Hyper Mero Random l l l l l l l l U+W2 proj, ClarkeDE l l l ll l l l l l l l l l l l l 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 l l l l l l l l l l l l l l l l Co−hyp Hyper Mero Random 1.5 1.0 0.5 z 0.0 −0.5 −1.0 −1.5 U+W2 proj, invCL l l l l l l l l l l l l l l l l l ll l l l l l ll l l l l l l l l l l l l ll l Co−hyp Hyper Mero Random 1033 Measure CO-HYP HYPER MERO RANDOM U+W2proj .20 .39 .58 .69 .55 .42 .24 .24 .24 .28 .29 .29 cosine ClarkeDE invCL Table 3: Mean Average Precision for the unsupervised measures after selecting the top dimensions from a supervised model. For this experiment, we train on all of BLESS except for one concept and then evaluate the unsupervised models on the held-out concept – that is a setting that could, in principle, be used as a hypernymy detector. If we instead train the supervised model on all of BLESS to determine an upper bound of how well dimension selection can do on this dataset, MAP for invCL rises to .67. Overall, these experiments provide strong evidence for the Selective Distributional Inclusion Hypothesis: The Distributional Inclusion Hypothesis holds, but only for relevant dimensions. In addition, hypernymy detectors need to test for “proper inclusion” of distributional contexts in order to really find hypernyms. Analysis of Selected Dimensions. We examine the 500 dimensions selected by the above procedure, in order to see what the classifier is learning. As this is for analysis only, the dimensions were selected by training on all data. Recall that the difference-squared g features can be interpreted as dimensions that the classifier deems not indicative of hypernymy. 200 out of the 250 most relevant dimensions by g are Computer Science related terms like software, configure, or Linux. Since ukWaC, the largest corpus we use, is web-based, it makes sense that it has many CS-related terms, which are noise when it comes to hypernymy detection for BLESS concepts. Also, we find that while the supervised approach needs the negative information from the g features (for Diff in the U+W2300 space, omitting g features yields a drop from .84 to .8), the unsupervised measures cannot use it. Dropping g features improves invCL results from .58 to .61. The g-based dimensions are explicitly those for which distributional inclusion should not hold, so they constitute noise to the unsupervised approaches. The f features can be interpreted as dimensions that characterize hypernyms. An inspection reveals two clear patterns. first, the features are topically relevant for the BLESS dataset. The 17 concept classes in the dataset belong to three broader groups: animals, plants, and artifacts. An annotation of the 250 dimensions by one of the authors showed that 58 dimensions are typical of animals (parasite, extinct), 14 typical of vegetables (ﬂora, nutrient), 80 typical of artifacts (repair, mechanical), 49 are general terms (find, worthy), and 49 have no clear interpretation (thee, enigmatic). Second, the features are general terms. For instance, for animals we find terms like animal, insect, creature, fauna, species, evolutionary, pathogen, nature, ecology. We also find many hypernyms, including many concept class names. Clearly, the selected features are domain dependent; most are directly related to the concepts and concept classes of BLESS. We expect that our method should work well for other data sets, given its high accuracy and the strict training procedure. However, these features are unlikely to be global indicators of hypernymy. This emphasizes the need, in future work, to find a way to automatically determine relevance on a per-word basis.</body>
  <conclusion>In this paper, we have tested the Distributional Inclusion Hypothesis, the basis for distributional approaches to hypernymy. We have found that the hypothesis only works if inclusion is selectively applied to a set of relevant dimensions. just by restricting to a smaller space, we evaluated the similarity measures on a variation of the U+W2 space which uses 500 randomly selected dimensions from the original space. The results are approximately unchanged from those on the original U+W2 space. 1034 We have tested two simple supervised approaches to distributional hypernymy detection and have found that they show good performance, and are robust to changes in the underlying space. Our best classifier achieves .84 accuracy on BLESS and .85 on the ENTAILMENT dataset of Baroni et al. (2012). It uses features that encode dimension-wise difference between vectors. This classifier can be interpreted as selecting the dimensions necessary for the Distributional Inclusion Hypothesis to work, thus as an effective way to implement selective distributional inclusion. The next natural step is to use the supervised features to guide development of an unsupervised measure for hypernymy detection: Now that we have examples, we hope to propose a method which selects relevant features automatically. We also would like to explore detection of other relationships, such as meronymy. finally, we would like to perform an extrinsic evaluation of our hypernymy detection approach in an actual RTE system.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1–10, Edinburgh, UK, July. Association for Computational LinguisticsMarco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23–32, Avignon, France, April. Association for Computational LinguisticsMatthew Berland and Eugene Charniak. 1999. finding parts in very large corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 57–64, College Park, Maryland, USA, June. Association for Computational LinguisticsPaul Buitelaar, Philipp Cimiano, and Bernardo Magnini. 2005. Ontology Learning from Text: Methods, Evaluation and Applications. Frontiers in Artificial Intelligence and Applications Series. IOS Press, AmsterdamTimothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relationsIn Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 33–40Philipp Cimiano, Aleksander Pivk, Lars Schmidt-Thieme, and Steffen Staab. 2005. Learning taxonomic relations from heterogeneous sources of evidence. Ontology Learning from Text: Methods, evaluation and applicationsDaoud Clarke. 2009. Context-theoretic semantics for natural language: an overviewIn Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119, Athens, Greece, MarchAssociation for Computational LinguisticsMaayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics, page 247. Association for Computational LinguisticsRoxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 1–8. Association for Computational Linguistics10http://www.tacc.utexas.edu 1035 Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th Conference on Computational Linguistics, pages 539–545, Stroudsburg, PA, USA. Association for Computational LinguisticsAur´elie Herbelot and Mohan Ganesalingam. 2013. Measuring semantic content in distributional vectorsIn Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 440–445, Sofia, Bulgaria, August. Association for Computational LinguisticsLili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16:359–389, 10Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In *SEM 2012: The first Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 75–79, Montr´eal, Canada, 7-8 June. Association for Computational LinguisticsAlessandro Lenci. 2008. Distributional approaches in linguistic and cognitive research. Italian Journal of Linguistics, 20(1):1–31Identifying synonyms among distributionally Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003similar words. In Proceedings of the 18th international Joint Conference on Artificial intelligence, pages 1492– 1493Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, volume 98, pages 296–304Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representationsIn Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June. Association for Computational LinguisticsGregory L. Murphy. 2002. The Big Book of Concepts. MIT Press, Boston, MAPatrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational LinguisticsFabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertran Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, MMatthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830Enrico Santus. 2013. SLQS: An entropy measure. Master’s thesis, University of PisaRion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1297–1304, Cambridge, MA. MIT PressRion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidenceIn Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 801–808, Stroudsburg, PA, USA. Association for Computational LinguisticsPeter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81–88 1036 Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarityIn Proceedings of the 20th International Conference on Computational Linguistics, pages 1015–1021, Geneva, Switzerland, Aug 23–Aug 27. Association for Computational Linguistics, COLINGMaayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailmentIn Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 107–114, Ann Arbor, Michigan, June. Association for Computational LinguisticsMaayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computational linguistics, 35(3):435–461</biblio>


  <preamble>BLESS.pdf</preamble>
  <titre>How we BLESSed distributional semantic evaluation</titre>
  <auteurs>
    <auteur>
      <name>Marco Baroni</name>
      <mail>marco.baroni@unitn.it</mail>
      <affiliation>University of Trento
Trento, Italy</affiliation>
    </auteur>
    <auteur>
      <name>Alessandro Lenci</name>
      <mail>alessandro.lenci@ling.unipi.it</mail>
      <affiliation>University of Pisa
Pisa, Italy</affiliation>
    </auteur>
  </auteurs>
  <abstract>We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models.</abstract>
  <introduction>In NLP, it is customary to distinguish between intrinsic evaluations, testing a system in itself, and extrinsic evaluations, measuring its performance in some task or application (Sparck Jones and Galliers, 1996). For instance, the intrinsic evaluation of a dependency parser will measure its accuracy in identifying specific syntactic relations, while its extrinsic evaluation will focus on the impact of the parser on tasks such as question answering or machine translation. Current approaches to the evaluation of Distributional Semantic Models (DSMs, also known as semantic spaces, vector-space models, etc.; see Turney and Pantel (2010) for a survey) are taskoriented. Model performance is evaluated in “semantic tasks”, such as detecting synonyms, recognizing analogies, modeling verb selectional preferences, ranking paraphrases, etc. Measuring the performance of DSMs on such tasks represents an in direct test of their ability to capture lexical meaning. The task-oriented benchmarks adopted in distributional semantics have not specifically been designed to evaluate DSMs. For instance, the widely used TOEFL synonym detection task was designed to test the learners’ proficiency in English as a second language, and not to investigate the structure of their semantic representations (cf. Section 2). To gain a real insight into the abilities of DSMs to address lexical semantics, existing benchmarks must be complemented with a more intrinsically oriented approach, to perform direct tests on the specific aspects of lexical knowledge captured by the models. In order to achieve this goal, three conditions must be met: (i) to single out the particular aspects of meaning that we want to focus on in the evaluation of DSMs; (ii) to design a data set that is able to explicitly and reliably encode the target semantic information; (iii) to specify the evaluation criteria of the system performance on the data set, in order to get an estimate of the intrinsic ability of DSMs to cope with the selected semantic aspects. In this paper, we address these three conditions by presenting BLESS (Baroni and Lenci Evaluation of Semantic Spaces), a new data set specifically geared towards the intrinsic evaluation of DSMs, downloadable from: http://clic.cimec.unitn.it/distsem.</introduction>
  <body>Distributional semantics benchmarks There are several benchmarks that have been widely adopted for the evaluation of DSMs, all of them capturing interesting challenges a DSM should meet. We brieﬂy review here some commonly used and representative benchmarks, and discuss why we felt 2 the need to add BLESS to the set. We notice at the outset of this discussion that we want to carve out a space for BLESS, and not to detract from the importance and usefulness of other data sets. We further remark that we focus on data sets that, like BLESS, are monolingual English and, while task-oriented, not aimed at a specific application setting (such as machine translation or ontology population). Probably the most commonly used benchmark in distributional semantics is the TOEFL synonym detection task introduced to computational linguistics by Landauer and Dumais (1997). It consists of 80 multiple-choice questions, each made of a target word (a noun, verb, adjective or adverb) and 4 response words, 1 of them a synonym of the target. For example, given the target levied, the matched words are imposed, believed, requested, correlated, the first one being the correct choice. The task for a system is then to pick the true synonym among the responses. The TOEFL task focuses on a single semantic relation, namely synonymy. Synonymy is actually not a common semantic relation and one of the hardest to define, to the point that many lexical semanticists have concluded that true synonymy does not exist (Cruse, 1986). Just looking at a few examples of synonym pairs from the TOEFL set will illustrate the problem: discrepancy/difference, prolific/productive, percentage/proportion, to market/to sell, color/hue. Moreover, the criteria adopted to choose the distractors (probably motivated by the language proficiency testing purposes of TOEFL) are not known. By looking at the set, it is hard to discern a coherent pattern. In certain cases, the distractors are semantically close to the target word (volume, sample and profit for percentage), whereas in other cases they are not (home, trail, and song for annals). It it thus not clear whether we are asking the models to distinguish a semantically related word (the synonym) from random elements, or a more tightly related word (the synonym, again) from other related words. The TOEFL task, finally, is based on a discrete choice (either you get the right word, or you don’t), with the result that evaluation is “quantized”, leading to large accuracy gains for small actual differences (one model that guesses one more synonym right than another gets 1.25% more points in percentage accuracy). The WordSim 353 data set (finkelstein et al., 2002) is a widely used example of semantic similarity rating set (see also Rubenstein and Goodenough (1965) and Miller and Charles (1991)). Subjects were asked to rate a set of 353 word pairs on a “similarity” scale and average ratings for each pair were computed. Models are then evaluated in terms of correlation of their similarity scores with average ratings across pairs. From the point of view of assessing the performance of a DSM, the WordSim (and related) similarity ratings are a mixed bag, in two senses. first, the data set contains a variety of different semantic relations. In a recent semantic annotation of the WordSim performed by Agirre et al. (2009) we find that, among the 174 pairs with above-median score (and thus presumably related), there is 1 identical pair, 17 synonym pairs, 28 hyper-/hyponym pairs, 30 coordinate pairs, 6 holo-/meronym pairs and 92 (more than half) pairs that are “topically related, but none of the above”. Second, the scores are a mixture of intuitions about which of these relations are more semantically tight and intuitions about more or less connected pairs within each of the relations. For example, among the top-rated scores we find synonyms such as journey/voyage and coordinate concepts (king/queen). If we look at the relations characterizing pairs around the median rating, we find both less “perfect” synonyms (monk/brother, that are synonymous only under an unusual sense of brother) and less close coordinates (skin/eye), as well as pairs instantiating other, less taxonomically tight relations, such as many syntagmatically connected items (family/planning, disaster/area, bread/butter). Apparently, a single scale is merging intuitions about semantic similarity of specific pairs and semantic similarity of different relations. A perhaps more principled way to evaluate DSMs that has recently gained some popularity is the concept categorization task, where a DSM has to cluster a set of nouns expressing basic-level concepts into gold standard categories. A particularly carefully constructed example is the Almuhareb-Poesio (AP) set of 402 concepts introduced in Almuhareb (2006). Concept categorization sets also include the Battig (Baroni et al., 2010) and ESSLLI 2008 (Baroni et al., 2008) lists. The AP concepts must be clustered into 21 classes, each represented by between 13 and 21 nouns. Examples include the ve 3 hicle class (helicopter, motorcycle. . . ), the motivation class (ethics, incitement, . . . ), and the social unit class (platoon, branch). The concepts are balanced in terms of frequency and ambiguity, so that, e.g., the tree class contains a common concept such as pine but also the casuarina tree, as well as the samba tree, that is not only an ambiguous term, but one where the non-arboreal sense dominates. Concept categorization data sets, while interesting to simulate one of the basic aspects of human cognition, are limited to one kind of semantic relation (discovering coordinates). More importantly, the quality of the results will depend not only on the underlying DSMs, but also on the clustering algorithm being used (and on how this interacts with the overall structure of the DSM), thus making it hard to interpret the performance of DSMs. The forced “hard” category choice is also problematic, and exaggerates performance differences between models especially in the presence of ambiguous terms (a model that puts samba in the occasion class with dance and ball might be penalized as much as a model that puts it in the monetary currency class). A more general issue with all benchmarks is that tasks are based on comparing a single quality score for each considered model (accuracy for TOEFL, correlation for WordSim, a clustering quality measure for AP, etc.). This gives little insight into how and why the models differ. Moreover, there is no well-established statistical procedure to assess significance of differences for most commonly used measures. finally, either because the data sets were not originally intended as standard benchmarks, or even on purpose, they all are likely to cause coverage problems even for DSMs trained on very large corpora. Think of the presence of extremely rare nouns like casuarina in AP, of proper nouns in WordSim (it is not clear to us that DSMs are adequate semantic models for referring expressions – at the very least they should not be mixed up lightly with common nouns), or multi-word expressions in other data sets. 3 How we intend to BLESS distributional semantic evaluation DSMs measure the distributional similarity between words, under the assumption that proximity in distributional space models semantic relatedness, includ ing, as a special case, semantic similarity (Budanitsky and Hirst, 2006). However, semantically related words in turn differ for the type of relation holding between them: e.g., dog is strongly related to both animal and tail, but with different types of relations. Therefore, evaluating the intrinsic ability of DSMs to represent the semantic space of a word entails both (i) determining to what extent words close in semantic space are actually semantically related, and (ii) analyzing, among related words, which type of semantic relation they tend to instantiate. Two models can be equally very good in identifying semantically related words, while greatly differing for the type of related pairs they favor. The BLESS data set complies with both these constraints. The set is populated with tuples expressing a relation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth referred to as relatum). For instance, in the BLESS tuple coyote-hyper-animal, the concept coyote is linked to the relatum animal via the hypernymy relation (the relatum is a hypernym of the concept). BLESS focuses on a coherent set of basiclevel nominal concrete concepts and a small but explicit set of semantic relations, each instantiated by multiple relata. Depending on the type of relation, relata can be nouns, verbs or adjectives. Moreover, BLESS also contains, for each concept, a number of random “relatum” words that are not semantically related to the concept. Thus, it also allows to evaluate a model in terms of its ability to harvest related words given a concept (by comparing true and random relata), and to identify specific types of relata, both in terms of semantic relation and part of speech. A data set intending to represent a gold standard for evaluation should include tests items that are as little controversial as possible. The choice of restricting BLESS to concrete concepts is motivated by the fact that they are by far the most studied ones, and there is better agreement about the relations that characterize them (Murphy, 2002; Rogers and McClelland, 2004). As for the types of relation to include, we are faced with a dilemma. On the one hand, there is wide evidence that taxonomic relations, the best understood type, only represent a tiny portion of the rich spectrum covered by semantic relatedness. On the other hand, most of these wider semantic rela 4 tions are also highly controversial, and may easily lead to questionable classifications. For instance, concepts are related to events, but often it is not clear how to distinguish the events expressing a typical function of nominal concepts (e.g., car and transport), from those events that are also strongly related to them but without representing their typical function sensu stricto (e.g., car and fix). As will be shown in Section 4, the BLESS data set tries to overcome this dilemma by attempting a difficult compromise: Semantic relations are not limited to taxonomic types and also include attributes and events strongly related to a concept, but in these cases we have resorted to underspecification, rather than committing ourselves to questionable granular relations. BLESS strives to capture those differences and similarities among DSMs that do not depend on coverage, processing choices or lexical preferences. BLESS has been constructed using a publicly available collection of corpora for reference (see Section 4.4 below), which means that anybody can train a DSM on the same data and be sure to have perfect coverage (but this is not strictly necessary). For each concept and relation, we pick a variety of relata (see next section) in order to abstract away from incidental gaps of models or different lexical/topical preferences. For example, the concept robin has 7 hypernyms including the very general and non-technical animal and bird and the more specific and technical passerine. A model more geared toward technical terminology might assign a high similarity score to the latter, whereas a commonsense-knowledgeoriented DSM might pick bird. Both models have captured similarity with a hypernym, and we have no reason, in general semantic terms, to penalize one or the other. To maximize coverage, we also make sure that, for each concept and relation, a reasonable number of relata are frequently attested in our reference corpora (see statistics below), we only include single-word relata and, where appropriate, we include multiple forms for the same relatum (both sock and socks as coordinates of scarf – as discussed in Section 4.1, we avoided similar ambiguous items as target concepts). Currently, distributional models for attributional similarity and relational similarity (Turney, 2006) are tested on different data sets, e.g., TOEFL and SAT respectively (brieﬂy, attributional similarity pertains to similarity between a pair of concepts in terms of shared properties, whereas relational similarity measures the similarity of the relations instantiated by couples of concept pairs). Conversely, BLESS is not biased towards any particular type of semantic similarity and thus allows both families of models to be evaluated on the same data set. Given a concept, we can analyze the types of relata that are selected by a model as more attributionally similar to the target. Alternatively, given a concept-relatum pair instantiating a specific semantic relation (e.g., hypernymy) we can evaluate a model ability to identify analogically similar pairs, i.e., others conceptrelatum pairs instantiating the same relation (we do not illustrate this possibility here). finally, by collecting distributions of 200 similarity values for each relation, BLESS allows reliable statistical testing of the significance of differences in similarity within a DSM (for example, using the procedure we present in Section 5 below), as well as across DSMs (for example, via a linear/ANOVA model with relations and DSMs as factors – not illustrated here). 4 Construction 4.1 Concepts BLESS includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities. Concepts have been grouped into 17 broader classes: AMPHIBIAN REPTILE (including amphibians and reptiles: alligator), APPLIANCE (toaster), BIRD (crow), BUILDING (cottage), CLOTHING (sweater), CONTAINER (bottle), FRUIT (banana), FURNITURE (chair), GROUND MAMMAL (beaver), INSECT (cockroach), MUSICAL INSTRUMENT (violin), TOOL (i.e., manipulable tools or devices: hammer), TREE (birch), VEGETABLE (cabbage), VEHICLE (bus), WATER ANIMAL (including fish and sea mammals: herring), WEAPON (dagger). All 200 BLESS concepts are single-word nouns in the singular form (we avoided concepts such as socks whose surface form might change depending on lemmatization choices). The major source we used to select the concepts were the McRae Norms (McRae et al., 2005), a collection of living and nonliving basic-level concepts described by 725 sub 5 i.e., COORD: jects with semantic features, each tagged with its property type. As further constraints guiding our selection, we wanted concepts with a reasonably high frequency (cf. Section 4.4), we avoided ambiguous or highly polysemous concepts and we balanced interand intra-class composition. Classes include both prototypical and atypical instances (e.g., robin and penguin for BIRD), and have a wide spectrum of internal variation (e.g., the class VEHICLE contains wheeled, air and sea vehicles). 175 BLESS concepts are attested in the McRae Norms, while the remnants were selected by the authors according to the above constraints. The average number of concepts per class is 11.76 (median 11; min. 5 AMPHIBIAN REPTILE; max. 21 GROUND MAMMAL). 4.2 Relations For each concept noun, BLESS includes several relatum words, linked to the concept by one of the following 5 relations. the relatum is a noun that is a co-hyponym (coordinate) of the concept, they belong to the same (narrowly or broadly defined) semantic class: alligatorcoord-lizard; HYPER: the relatum is a noun that is a hypernym of the concept: alligator-hyperanimal; MERO: the relatum is a noun referring to a part/component/organ/member of the concept, or something that the concept contains or is made the relatum is of: alligator-mero-mouth; ATTRI: an adjective expressing an attribute of the concept: alligator-attri-aquatic; EVENT: the relatum is a verb referring to an action/activity/happening/event the concept is involved in or is performed by/with the concept: alligator-event-swim. BLESS also includes the relations RAN.N, RAN.J and RAN.V, which relate the target concepts to control tuples with random noun, adjective and verb relata, respectively. The BLESS relations cover a wide spectrum of information useful to describe a target concept and to qualify the notion of semantic relatedness: taxonomically related entities (hyper and coord), typical attributes (attri), components (mero), and associated events (event). However, except for hyper and coord (corresponding to the standard relations of class inclusion and co-hyponymy respectively), the other BLESS relations are highly underspecified. For instance, mero corresponds to a very broad notion of meronymy, including not only parts (dog-tail), but also the material (table-wood) as well as the members (hospital-patient) of the entity the target concept refers to (Winston et al., 1987); event is used to represent the behaviors of animals (dog-bark), typical functions of instruments (violin-play), and events that are simply associated with the target concept (car-park); attri captures a large range of attributes, from physical (elephant-big) to evaluative ones (carexpensive). As we said in section 3, we did not attempt to further specify these relations to avoid any commitment to controversial ontologies of property types. Note that we exclude synonymy both because of the inherent problems in this very notion (Cruse, 1986), and because it is impossible to find convincing synonyms for 200 concrete concepts. In BLESS, we have adopted the simplifying assumption that each relation type has relata belonging to the same part of speech: nouns for hyper, coord and mero, verbs for event, and adjectives for attri. Therefore, we abstract away from the fact that the same semantic relation can be realized with different parts of speech, e.g., a related event can be expressed by a verb (transport) or by a noun (transportation). 4.3 Relata The relata of the non-random relations are English nouns, verbs and adjectives selected and validated by both authors using two types of sources: semantic sources (the McRae Norms (McRae et al., 2005), WordNet (Fellbaum, 1998) and ConceptNet (Liu and Singh, 2004)) and text sources (Wikipedia and the Web-derived ukWaC corpus, see Section 4.4 below). These resources greatly differ in dimension, origin and content and therefore provide complementary views on relata. Their relative contribution to BLESS also depends on the type of relation and the target concept. For instance, the rich taxonomic structure of WordNet has been the main source of information for many technical hypernyms (e.g. gymnosperm, oscine), which instead are missing from more commonsense-oriented resources such as the McRae Norms and ConceptNet. Meronyms are rarer in WordNet, and were collected mainly from the latter two resources, with many technical terms (e.g., parts of ships, weapons) harvested from the Wikipedia entries for the target concepts. Attributes and events were collected from McRae 6 Norms, ConceptNet and ukWaC. In the McRae Norms, the number of features per concept is fairly limited, but they correspond to highly distinctive, prototypical and cognitively salient properties. ConceptNet instead provides a much wider array of associated events and attributes that are part of our commonsense knowledge about the target concepts (e.g., the events park, steal and break, etc. for car). ConceptNet relations such as Created by, Used for, Capable of etc. have been analyzed to identify potential event relata, while the Has property relation has been inspected to look for attributes. The most salient adjectival and verbal collocates of the target nouns in the ukWaC corpus were also used to identify associated attributes and events. For instance, the target concept elephant is not attested in the McRae Norms and has few properties in ConceptNet. Thus, many of its related events have been harvested from ukWaC. They include verbs such as hunt, kill, etc. which are quite salient and frequent with respect to elephants, although they can hardly be defined as prototypical properties of this animal. As a result of the combined use of such different types of sources, the BLESS relata are representative of a wide spectrum of semantic information about the target concepts: they include domain-specific terms side by side to commonsense ones, very distinctive features of a concept (e.g., hoot for owl) together with attributes and events that are instead shared by a whole class of concepts (e.g., all animals have relata such as eat, feed, and live), prototypical features as well as events and attributes that are statistically salient for the target, etc. In many cases, the concept properties contained in semantic sources are expressed with phrases, e.g., lay eggs, eat grass, live in Africa, etc. We decided, however, to keep only single-word relata in BLESS, because DSMs are typically populated with single words, and, when they are not, they differ in the kinds of multi-word elements they store. Therefore, phrasal relata have always been reduced to their head: a verb for properties expressed by a verb phrase, and a noun for properties expressed by a noun phrase. For instance, from the property lay eggs, we derived the event relatum lay. To extract the random relata, we adopted the following procedure. For each relatum that instantiates a true relation with the concept, we also randomly picked from our combined corpus (cf. Section 4.4) another lemma with the same part of speech, and frequency within 1 absolute logarithmic unit from the frequency of the corresponding true relatum. Since picking a random term does not guarantee that it will not be related to the concept, we filtered the extracted list by crowdsourcing, using the Amazon Mechanical Turk via the CrowdFlower interface (CF).1 We presented CF workers with the list of about 15K concept+random-term pairs selected with the procedure we just described, plus a manually checked validation set (a “gold set” in CF terminology) comprised of 500 concept+true-relatum pairs and 500 concept+random-term pairs (these elements are used by CF to determine the reliability of workers, and discard the ratings of unreliable ones), plus a further set of 1.5K manually checked concept+truerelatum pairs to make the random-true distribution less skewed. The workers’ task was, for each pair, to check a YES radio button if they thought there is a relation between the words, NO otherwise. The words were annotated with their part of speech, and workers were instructed to pay attention to this information when making their choices. Extensive commented examples of both related pairs and unrelated ones were also provided in the instruction page. A minimum of 2 CF workers rated each pair, and, conservatively, we preserved only those items (about 12K) that were unanimously rated as unrelated to their concept by the judges. See Table 1 for summary statistics about the preserved random sets (nouns: RAND.N, adjectives: RAN.J, verbs:RAN.V). 4.4 BLESS statistics For frequency information, we rely on the combination of the freely available ukWaC and Wackypedia corpora (size: 1.915B and 820M tokens, respectively).2 The data set contains 200 concepts that have a mean corpus frequency of 53K occurrences (min. 1416 chisel, max. 793K car). The relata of these concepts (26,554 in total) are distributed as reported in Table 1. Note that the distributions reﬂect certain “natural” differences between relations (hypernyms tend to be more frequent words than coordinates, but there are 1http://crowdflower.com/ 2http://wacky.sslmit.unibo.it/ 7 cardinality frequency avg max max min avg relation min 17.1 6 35 37K 1.7M 0 COORD 15 6.7 2 138K 1.9M 31 HYPER 2 14.7 53 133K 2M 0 MERO 4 13.6 27 501K 3.7M 0 ATTRI 6 19.1 40 517K 5.4M 0 EVENT 67 32.9 92K 2.4M 16 0 RAN.N 472K 4.5M 3 10.9 24 1 RAN.J 508K 7.7M 4 16.3 34 RAN.V 1 Table 1: Distribution (minimum, mean and maximum) of the relata of all BLESS concepts: the frequency columns report summary statistics for corpus counts across relata instantiating a relation; the cardinality columns report summary statistics for number of relata instantiating a relation across the 200 concepts, only considering relata with corpus frequency ≥ 100. more coordinates than hypernyms, etc.). Instead of trying to artificially control for these differences, we assess their impact in Section 5 by looking at the behavior of baselines that exploit the frequency and cardinality of relations as proxies to semantic similarity (such factors could also be entered as regressors in a linear model). 5 Evaluation This section illustrates one possible way to use BLESS to explore and evaluate DSMs. Given the similarity scores provided by a model for a concept with all its relata across all relations, we pick the relatum with the highest score (nearest neighbour) for each relation (see discussion in Section 3 above on why we allow models to pick their favorite from a set of relata instantiating the same relation). In this way, for each of the 200 BLESS concepts, we obtain 8 similarity scores, one per relation. In order to factor out concept-specific effects that might add to the overall score variance (for example, a frequent concept might have a denser neighborhood than a rarer one, and consequently the nearest relatum scores of the former are trivially higher than those of the latter), we transform the 8 similarity scores of each concept onto standardized z scores (mean: 0; s.d: 1) by subtracting from each their mean, and dividing by their standard deviation. After this transformation, we produce a boxplot summarizing the distribution of scores per relation across the 200 concepts (i.e., each box of the plot summarizes the distribution of the 200 standardized scores picked for each relation). Our boxplots (see examples in fig. 1 below) display the median of a distribution as a thick horizontal line within a box extending from the first to the third quartile, with whiskers covering 1.5 of the interquartile range in each direction from the box, and values outside this extended range – extreme outliers – plotted as circles (these are the default boxplotting option of the R statistical package).3 While the boxplots are extremely informative about the relation types that are best captured by models, we expect some degree of overlap among the distributions of different relations, and in such cases we might want to ask whether a certain model assigns significantly higher scores to one relation rather than another (for example, to coordinates rather than random nouns). It is difficult to decide a priori which pairwise statistical comparisons will be interesting. We thus take a conservative approach in which we perform all pairwise comparisons using the Tukey Honestly Significant Difference test, that is similar to the standard t test, but accounts for the greater likelihood of Type I errors when multiple comparisons are performed (Abdi and Williams, 2010). We only report the Tukey test results for those comparisons that are of interest in the analysis of the boxplots, using the standard α = 0.05 significance threshold. 5.1 Models Occurrence and co-occurrence statistics for all models are extracted from the combined ukWaC and Wackypedia corpora (see Section 4.4 above). We exploit the automated morphosyntactic annotation of the corpora by building our DSMs out of lemmas (instead of inﬂected words), and relying on part of speech information. Baselines. The RelatumFrequency baseline uses the frequency of occurrence of a relatum as a surrogate of its cosine with the concept. With this approach, we want to verify that the unequal frequency distribution across relations (see Table 1 above) is not trivially sufficient to differentiate relation classes in a semantically interesting way. For our second baseline, we assign a random number as cosine sur3http://www.r-project.org/ 8 rogate to each relatum (to smooth these random values, we generate them by first sampling, for each relatum, 10K random variates from a uniform distribution, and then averaging them). If the set of relata instantiating a certain relation is larger, it is more likely that it will contain the highest random value. Thus, this RelationCardinality baseline will favor relations that tend to have large relata set across concepts, controlling for effects due to different cardinalities across semantic relations (again, see Table 1 above). DSMs. We choose a few ways to construct DSMs for illustrative purposes only. All the models contain vector representations for the same words, namely, approximately, the top 20K most frequent nouns, 5K most frequent adjectives and 5K most frequent verbs in the combined corpora. All the models use Local Mutual Information (Evert, 2005; Baroni and Lenci, 2010) to weight raw co-occurrence counts (this association measure is obtained by multiplying the raw count by Pointwise Mutual Information, and it is a close approximation to the Log-Likelihood Ratio). Three DSMs are based on counting co-occurrences with collocates within a window of fixed width, in the tradition of HAL (Lund and Burgess, 1996) and many later models. The ContentWindow2 model records sentence-internal co-occurrence with the nearest 2 content words to the left and right of each target concept (the same 30K target nouns, verbs and adjectives are also employed as context content words). ContentWindow20 is like ContentWindow2, but considers a larger window of 20 words to the left and right of the target. AllWindow2 adopts the same window of ContentWindow2, but considers all co-occurrences, not only those with content words. The Document model, finally, is based on a (Local-Mutual-Information transformed) word-by-document matrix, recording the distribution of the 30K target words across the documents in the concatenated corpus. This DSM is thus akin to traditional Latent Semantic Analysis (Landauer and Dumais, 1997), without dimensionality reduction. The content-window-based models have, by construction, about 30K dimensions. The other models are much larger, and for practical reasons we only keep 1 million dimensions (those that account, cumulatively, for the largest proportion of the overall Local Mutual Information mass). 5.2 Results The concept-by-concept z-normalized distributions of cosines of relata instantiating each of our relations are presented, for each of the example models, in fig. 1. The RelatumFrequency baseline shows a preference for adjectives and verbs in general, independently of whether they are meaningful (attributes, events) or not (random adjectives and verbs), reﬂecting the higher frequencies of adjectives and verbs in BLESS (Table 1). The RelationCardinality baseline produces even less interesting results, with a strong preference for random nouns, followed by coordinates, events and random verbs (as predicted by the distribution in Table 1). We can conclude that the semantically meaningful patterns produced by the other models cannot be explained by trivial differences in relatum frequency or relation cardinality in the BLESS data set. Moving then to the real DSMs, ContentWindow2 essentially partitions the relations into 3 groups: coordinates are the closest relata, which makes sense since they are, taxonomically, the most similar entities to target concepts. They are followed by (but significantly closer to the concept than) events, hypernyms and meronyms (events and hypernyms significantly above meronyms). Next come the attributes (significantly lower cosines than all relation types above). All the meaningful relata are significantly closer to the concepts than the random relata. Similar patterns can be observed in the ContentWindow20 distribution, however in this case the events, while still significantly below the coordinates, are significantly above the (statistically indistinguishable) hypernym, meronym and attribute set. Again, all meaningful relata are above the random ones. Both content-window-based models provide reasonable results, with ContentWindow2 being probably closer to our “ontological” intuitions. The high ranking of events is probably explained by the fact that a nominal concept will often appear as subject or object of verbs expressing associated events (dog barks, fishing tuna), and thus the corresponding verbs will share even relatively narrow context windows with the concept noun. The AllWindow2 distribution probably reﬂects the fact that many contexts picked by this DSM are function 9 RelatumFrequency RelationCardinality ● ● ● ● ● ● ● ● ● ● ● ●●● ● ● 2 1 0 1 − 2 − ● ● ● ● ● ● ● ● ● ● 2 1 0 1 − 2 − ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● COORD HYPER ● ● ● ● ● ATTRI EVENT MERO RAN.N ContentWindow20 RAN.N RAN.J RAN.V COORD HYPER MERO RAN.J RAN.V COORD HYPER MERO ATTRI EVENT AllWindow2 ●● 2 1 0 ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● 1 − ● 2 − ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 2 1 0 1 − 2 − ContentWindow2 ● ●● ● ●● ATTRI EVENT Document ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● RAN.N RAN.J RAN.V ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ●● ● ● ●● ● ● ● ● ● ● 2 1 0 1 − 2 − 2 1 0 1 − 2 − COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after concept-by-concept znormalization). words, and thus they capture syntactic, rather than semantic distributional properties. As a result, random nouns are as high (statistically indistinguishable from) hypernyms and meronyms. Interestingly, attributes also belong to this subset of relations – probably due to the effect of determiners, quantifiers and other DP-initial function words, that will often occur both before nouns and before adjectives. Indeed, even random adjectives, although significantly below the other relations we discussed, are significantly above both random and meaningful verbs (i.e., events). For the Document model, all meaningful relations are significantly above the random ones. However, coordinates, while still the nearest neighbours (significantly closer than all other relations) are much less distinct than in the windowbased models. Note that we cannot say a priori that ContentWindow2 is better than Document because it favors coordinates. However, while they are both able to sort out true and random relata, the latter shows a weaker ability to discriminate among different types of semantic relations (co-occurring within a document is indeed a much looser cue to similarity than specifically co-occurring within a narrow window). Traditional DSM tests, based on a single qual ity measure, would not have given us this broad view of how models are behaving.</body>
  <conclusion>We introduced BLESS, the first data set specifically designed for the intrinsic evaluation of DSMs. The data set contains tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. Thus, BLESS can be used to evaluate both the ability of DSMs to discriminate truly related word pairs, and to perform in-depth analyses of the types of semantic relata that different models tend to favor among the nearest neighbors of a target concept. Even a simple comparison of the performance of a few DSMs on BLESS like the one we have shown here is able to highlight interesting differences in the semantic spaces produced by the various models. The success of BLESS will obviously depend on whether it will become a reference model for the evaluation of DSMs, something that can not be foreseen a priori. Whatever its destiny, we believe that the BLESS approach can boost and innovate evaluation in distributional semantics, as a key condition to get at a deeper understanding of its potentialities as a viable model for meaning.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Herv Abdi and Lynne Williams. 2010. Newman-Keuls and Tukey test. In N.J. Salkind, D.M. Dougherty, and B. Frey, editors, Encyclopedia of Research DesignSage, Thousand Oaks, CAEneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasc¸a, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approachesIn Proceedings of HLT-NAACL, pages 19–27, Boulder, COAbdulrahman Almuhareb. 2006. Attributes in Lexical Acquisition. Phd thesis, University of EssexMarco Baroni and Alessandro Lenci2010. Distributional Memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721Marco Baroni, Stefan Evert, and Alessandro Lenci, editors. 2008. Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI, HamburgMarco Baroni, Eduard Barbu, Brian Murphy, and Massimo Poesio. 2010. Strudel: A distributional semantic model based on properties and types. Cognitive Science, 34(2):222–254Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32:13–47D. A. Cruse. 1986. Lexical Semantics. Cambridge University Press, CambridgeStefan Evert. 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart UniversityChristiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MALev finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240Hugo Liu and Push Singh. 2004. ConceptNet: A practical commonsense reasoning toolkit. BT Technology Journal, pages 211–226Kevin Lund and Curt BurgessProducing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, 28:203–208Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547–559 1996 George Miller and Walter Charles. 1991. Contextual cor-relates of semantic similarity. Language and Cogni-tive Processes, 6(1):1–28.Gregory Murphy. 2002. The Big Book of Concepts. MIT Press, Cambridge, MA. Timothy Rogers and James McClelland. 2004. Seman-tic Cognition: A Parallel Distributed Processing Ap-proach. MIT Press, Cambridge, MA.Herbert Rubenstein and John Goodenough. 1965. Con-textual correlates of synonymy. Communications of the ACM, 8(10):627–633. Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-ing Natural Language Processing Systems: An Analy-sis and Review. Springer Verlag, Berlin. Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Jour-nal of Artificial Intelligence Research, 37:141–188. Peter Turney. 2006. Similarity of semantic relations.Computational Linguistics, 32(3):379–416. Morton E. Winston, Roger Chaffin, and Douglas Her-rmann. 1987. A taxonomy of part-whole relations.Cognitive Science, 11:417–444.</biblio>


  <preamble>C14-1212.pdf</preamble>
  <titre>Learning to Distinguish Hypernyms and Co-Hyponyms</titre>
  <auteurs>
    <auteur>
      <name>Julie Weeds</name>
      <mail>juliewe@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Daoud Clarke</name>
      <mail>D.Clarke@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Jeremy Reffin</name>
      <mail>J.P.Reffin@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>David Weir</name>
      <mail>davidw@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
    <auteur>
      <name>Bill Keller</name>
      <mail>billk@sussex.ac.uk</mail>
      <affiliation>Department of Informatics,
University of Sussex,
Brighton, UK</affiliation>
    </auteur>
  </auteurs>
  <abstract>This work is concerned with distinguishing different semantic relations which exist between distributionally similar words. We compare a novel approach based on training a linear Support Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional similarity. We show that the new supervised approach does better even when there is minimal information about the target words in the training data, giving a 15% reduction in error rate over unsupervised approaches.</abstract>
  <introduction>Over recent years there has been much interest in the field of distributional semantics, drawing on the distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris, 1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of “nearest neighbours”) automatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words. A central problem here is that whilst most measures of distributional similarity are symmetric, some of the important semantic relations are not. The hyponymy relation (and converse hypernymy) which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the cohyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the words cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms1. 1We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings of cat. footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2250 cat animal dog dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25, snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23 bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29, human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25 cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish 0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23 Table 1: Top 15 neighbours of cat, animal and dog generated using Lin’s similarity measure (Lin, 1998) considering all words and dependency features occurring 100 or more times in Wikipedia. Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can be useful to be able to distinguish between these different relationships. Consider the following two sentences. The cat ran across the road. The animal ran across the road. (1) (2) Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability to determine whether entailment holds between the sentences, and in which direction, depends on the ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of various supervised approaches, and makes the following contributions. first, we show that a SVM can distinguish the entailment and co-hyponymy relations, achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based on the notion of distributional generality. Second, by comparing two different data sets, one built from BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important insights into the requirements of a valid evaluation of supervised approaches, and provide a data set for further research in this area. Third, we show that when learning how to determine an ontological relationship between a pair of similar words by means of the word’s distributional vectors, quite different vector operations are useful when identifying different ontological relationships. In particular, using the difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding the vectors works well for the co-hyponym task.</introduction>
  <body>Related Work Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She found that this measure improved results in language modelling, when a word’s distribution is smoothed using the distributions of its nearest neighbours. Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible 2251 contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g., “the monstrous animal barked at the intruder”. Weeds et al. (2004) attempt to capture this asymmetry by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision and recall are defined as: Pww(u, v) = Σf∈F (u)∩F (v)I(u, f) Σf∈F (u)I(u, f) and Rww(u, v) = Σf∈F (u)∩F (v)I(v, f) Σf∈F (v)I(v, f) where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the set of all features f for which I(n, f) > 0. By comparing the precision and recall of one word’s retrieval of another word’s contexts, they were able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet. However, this was not significantly better than a baseline which proposed that the most frequent word was the most general. Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space. He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as: Σf∈F (u)∩F (v)min(I(u, f), I(v, f)) Σf∈F (u)∩F (v)min(I(u, f), I(v, f)) Pcl(u, v) = Σf∈F (u)I(u, f) and Rcl(u, v) = Σf∈F (v)I(v, f) Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should have high recall and low precision, which would thus make it possible to distinguish them from other related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure to identify hypernyms: q Pcl(u, v) ∗ (1 − Rcl(u, v)) invCL(u, v) = 2 Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distinguishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009). Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong correlation between complete inclusion of prominent features and lexical entailment. However, they were unable to assess this using an off-line corpus due to data sparseness. Szpektor and Dagan (2008) found that the Pww measure tends to promote relationships between infrequent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed using the geometric average of Pww and the symmetric similarity measure of Lin (1998) in order to penalise low frequency words. Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identifying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similarities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric similarity measures previously proposed perform much better than symmetric similarity measures on a directionality detection experiment, and that their method and that of Clarke (2009) outperform the others with statistical significance. They also show that their measure is superior when used for term expansion in an event detection task. Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate that support vector machines can generalise entailment relations between quantifier phrases to entailment involving unseen quantifiers. They compare the performance of their system with the balAPinc measure. The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large corpora to find patterns that are indicative of hyponymy. For example, the pattern “NP X and other NP Y ” is an indication that NP X is a NP Y , i.e. that NP X is a hyponym of NP Y . They use machine learning to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to find new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow 2252 et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms themselves. Our approach is therefore more generally applicable to systems which compose distributional representations of meaning. Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution. In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional similarity measure, WeightedCosine2, performs best. Also of note, Mikolov et al. (2013) propose a vector offset method to capture syntactic and semantic regularities between word representations learnt by a recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis. Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS dataset (Baroni and Lenci, 2011), this measure outperforms Pww at both discriminating hypernym test pairs from other types of relation and at determining the direction of the entailment relation. 3 Methodology The code used to perform our experiments has been open sourced, and is available online.3 3.1 Vector Representations Distributional information was collected for all of the nouns from Wikipedia provided they had occurred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted text using wp2txt4. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were extracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345. Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds between two similar words A and B. Support vector machines (SVMs), which are effective across a variety of classification scenarios, learn a boundary between two classes from a set of positive and negative example vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we do not start with a single vector, but with two distributional vectors vA and vB for the words A and B, respectively. These vectors must be combined in some way to produce the SVM’s input, and a number of ways were considered, defined in Table 2. Of these operations, the vector difference (used by svmDIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise multiplication (used by svmADD and svmMULT) are symmetric. We now motivate the use of each of these operations. first, we note that pointwise multiplication (svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to learn the features that nouns occurring in different semantic relations should share. However, it does not retain any information about non-shared features and it is symmetric so it is difficult to see how it would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD) effectively performs the union of the features, giving emphasis to the shared features. Whilst it does retain information about the non-shared features, it is also symmetric, making it difficult again to see how it would be useful in determining the direction of an entailment relation Vector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further, we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns. Further, considering the majority sign of features in this difference vector might indicate the direction of 2The details of this measure are unpublished. 3https://github.com/SussexCompSem/learninghypernyms 4https://github.com/yohasebe/wp2txt 2253 entailment. Using an SVM, we might expect to be able to effectively learn which of these features should be ignored and which should be combined, to decide the correct direction of entailment in the majority number of cases in our training data. However, note that if one uses vector difference it is impossible to distinguish between the case where a feature occurred with both nouns (to the same extent) and the case where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from the original vectors. finally, we consider the use of the single vector corresponding to the second word (svmSING) as a baseline. High performance by this operation would indicate that we can learn features of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the putative relationship. We also note that the behaviour of these methods may differ depending on the weighting used for vectors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded equal to zero), which is consistent with other work in this area, means that this correspondence is lost. Because of the nature of our datasets, we were concerned that systems could learn information about the taxonomy from the relations in the training data, without making use of information in the vectors themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially exceeds the average number (398) of non-zero features in the Wikipedia vectors. 3.2 Classifiers We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear SVMs for speed and simplicity, since the point is to compare the different vector representations of the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly supervised classifiers. These are listed in Table 2. For the linear SVMs and kNN classifier, we used the scikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter search, using nested cross-validation, varying k between 1 and 50. For weakly supervised approaches, we evaluated the measure on the training set, then found the best threshold p on the training set that best divides the two classes using that measure. When classifying, we determine that the relation holds if the value of the measure exceeds p. A linear SVM trained on the vector difference vB − vA A linear SVM trained on the pointwise product vector vB ∗ vA A linear SVM trained on the vector sum vB + vA A linear SVM trained on the vector concatenation vB ⊕ vA A linear SVM trained on the vector vB k nearest neighbours (knn) trained on the vector difference vB − vA.1 &lt; k &lt; 50 width(B) > width(A) → rel(A, B) where width(A) is number of non-zero features in A simcos(A, B) > p → rel(A, B) where simcos(A, B) is cosine similarity using PPMI simlin(A, B) > p → rel(A, B) (Lin, 1998) Pww(A, B) > Rww(A, B) → rel(A, B) (Weeds et al., 2004) Pcl(A, B) > Rcl(A, B) → rel(A, B) (Clarke, 2009) invCL(A, B) > p → rel(A, B) (Lenci and Benotto, 2012) balAPinc(A, B) > p → rel(A, B) (Kotlerman et al., 2010) The most frequent label in the training data is assigned to every test point. svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF widthdiff singlewidth width(B) > p → rel(A, B) cosineP linP CRdiff clarkediff invCLP balAPincP most freq Table 2: Implemented classifiers 2254 3.3 Data Sets One of key the challenges of this work has been to construct a data set which accurately and validly tests our hypotheses. All four of our datasets detailed below are available online 5. In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare the supervised approaches with the unsupervised approaches, but also make it unlikely that our results would be generalisable to other data. Here, we outline the requirements for the data sets, the importance of which is demonstrated by our initial results for a data set which does not satisfy all of them. There should be an equal number of positive and negative examples of a semantic relation. Thus, random guessing or labelling with the most frequently seen label in the training data will yield 50% accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in terms of simple accuracy (or error rate). It should not be possible to do well simply by considering the distributional similarity of the terms. Hence, the negative examples need to be pairs of equally similar words, but where the relationship under consideration does not hold. It should not be possible to do well by pre-supposing an entailment relation and guessing the direction. For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from WordNet, over 70% of the time the more frequent word is also the entailed word. It should not be possible to do well using ontological information learnt about one or both of the words from the training data that is not generalisable to their distributional representations. For example, it should not be possible for the classifier simply to learn directly from the training pairs hcat ISA mammali and hmammal ISA animali that hcat ISA animali. Furthermore, we must ensure that a classifier cannot learn that a particular word is near the top of the ontological hierarchy, and, as a result, do well by guessing that a particular pairing probably has an entailment relation. For example, given many pairs such as hcat ISA animali, hdog ISA animali, a system which guessed hrabbit ISA animali but not hanimal ISA rabbiti would do better than random guessing. Whilst both of these types of information could be useful in a hybrid system, they do not require any distributional information and therefore we would not be learning anything about the distributional features of animal which make it likely to be a hypernym. 3.3.1 BLESS We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of examples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete, largely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts. hyponymBLESS is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms were randomly selected to provide positive examples of entailment. The remaining hypernyms for the given concept were reversed and taken with the same number of co-hyponyms, meronyms and random words to form negative examples of entailment. A filter was applied to ensure that duplicate pairs were not included (e.g., if hcat, animali is a positive pair then hanimal, cati cannot be a negative pair). cohyponymBLESS is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms were taken as positive examples of this relation. The same total number of (and split evenly between) hypernyms, meronyms and random words was taken to form the negative examples. The order of 50% of the pairs was reversed and again duplicate pairs were disallowed. In both cases the pairs are labelled as positive or negative for the specified semantic relation and in both cases there are equal (±1) numbers of positive and negative examples. For 99% of the generated BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an associated vector, the classifiers use a zero vector. 5https://github.com/SussexCompSem/learninghypernyms 2255 3.3.2 WordNet We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS data sets they more adequately satisfy the requirements laid out above6. We constructed a list of all nonrare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet package) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous, the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor frequency information provided with WordNet. This led to a list of 7613 nouns. hyponymWN is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs hA, Bi were found in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already been added and provided that each word has not previously been used in that position. Co-hyponym pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and provided that neither word has already been seen in that position in a pairing (either in the entailment pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives is selected. This provides a balanced data set where half of the pairs are positive examples of entailment and the other half are semantically similar but not entailing. cohyponymWN is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponymWN except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of cohyponyms and the other half, the negative examples, are entailment pairs (with direction unspecified) In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance between co-hyponym pairs is 2. 3.4 Experimental Setup Most of our experiments were carried out using an implementation of five-fold cross-validation using each combination of data set, vector set and classifier. In this setup, the pairs are randomly partitioned into five subsets, one subset is held out for testing whilst the classifiers are trained on the remaining four, and this process is repeated using each subset as the test set. In initial experiments with the BLESS datasets, the SVM classifiers were able to achieve classification accuracy of over 95% for hyponymBLESS and over 90% for cohyponymBLESS. However, the results using random vectors were not significantly different from using the distributional vectors harvested from Wikipedia. This indicated that the classifiers were learning ontological information implicit in the training data. In order to address this, when using the BLESS datasets, we removed any pair from the training data if either word was present in the test data. In order to preserve a reasonable amount of training data, we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all datasets and classifiers, we found performance by the random vectors was no higher than 52%. This indicates that the performance seen in Table 3 is due to learning from distributional features rather than any ontological information implicit in the training set. 4 Results In Table 3, we compare average accuracy for a number of different classifiers on each of two tasks, distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets. Looking at the results for the hyponymBLESS data set, we can see that the SVM methods do generally outperform the unsupervised methods. However, the best performing model is svmSING, suggesting that, for this data set, it is best to try to learn the distributional features of more general terms, rather than comparing the vector representations of the two terms under consideration. 6Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only provided for 200 nouns. 2256 dataset hyponymBLESS cohyponymBLESS hyponymWN cohyponymWN dataset svmDIFF 0.74 0.62 0.75 0.37 most freq 0.54 0.61 0.50 0.50 svmMULT 0.56 0.39 0.45 0.60 cosineP 0.53 0.79 0.53 0.50 svmADD svmCAT 0.66 0.68 0.40 0.41 0.74 0.37 0.68 0.64 widthdiff linP 0.56 0.54 0.78 - 0.52 0.70 - 0.55 svmSING 0.75 0.40 0.69 0.58 singlewidth 0.58 - 0.65 - knnDIFF 0.54 0.58 0.50 0.50 CRdiff 0.52 - 0.70 - invCLP 0.54 - 0.66 - balAPincP 0.54 - 0.53 - hyponymBLESS cohyponymBLESS hyponymWN cohyponymWN Table 3: Accuracy figures for the data sets generated from BLESS and WordNet (standard errors &lt; 0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also omit results of clarkediff as these were consistently the same or less than CRdiff. On the corresponding co-hyponym task, using the cohyponymBLESS data set, we see the best performing classifier is the cosine measure. The cosine measure is able to perform relatively well here because a substantial proportion of the negative examples (25%) are random unrelated words which will have low cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests that measures such as the cosine measure “prefer” words in symmetric semantic relationships such as cohyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of the training data in this experimental set up with this data set. If, for example, our test concept is robin, our approach requires that we will not have any training pairs containing robin, or any training pairs containing any of the words to which robin is related in the test set. In a dataset as small as BLESS, this requirement effectively removes all knowledge of the distributional features of words in the target domain. Hence, the need for a larger dataset as we have extracted from WordNet. Looking at the results for the hyponymWN data set, the directional SVM methods (svmDIFF and svmCAT) substantially outperform the symmetric SVM methods, and their performance is significantly better (at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between svmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the linear SVM outperforms kNN by 19–25%. This may suggest that the shape of the vector space inhabited by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable. Looking at the results for the cohyponymWN data set, it is clear that the unsupervised methods cannot distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially better, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially retain information about all of the features of both words. svmMULT does much better than svmDIFF, which suggests that the shared features are more indicative than the non-shared features for this task. The reasonably high performance of svmSING on both data sets suggests that words which have cohyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which are included as entailed words in the data set. We hypothesise that there are specific features which more general words tend to share (regardless of their topic) which makes it possible to identify more general words from more specific words. This is completely consistent with very recent results using SLQS, a new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms, with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns typically have less informative contexts and are therefore likely to by hypernyms. the performance of the balAPincP measure is lower than expected on the Given prior work, hyponymWN dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are determining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is known that a relationship exists. It could be that the measure is particularly suited to the latter task. 2257</body>
  <conclusion>We have shown that it is possible to predict to a large extent whether or not there is a specific semantic relation between two words given their distributional vectors, using a supervised approach based on linear SVMs. The increase in accuracy over unsupervised methods is significant at the 0.01% level and corresponds to a substantial absolute reduction in error rate (over 15%). We have also shown that the choice of vector operation is significant. Whilst concatenating the vectors, and therefore retaining all of the information from both vectors including direction, generally performs well, we have also shown that different vector operations are useful in establishing different relationships. In particular, the vector difference operation, which loses information about the original vectors, achieved performance indistinguishable from concatenation on the entailment task, where the classifier is required to distinguish hyponyms from other semantically related words including hypernyms. On the other hand, the addition operation, which also loses information, outperformed concatenation by 4% (which is statistically significant at the 0.01% level) on the coordinate task, where the classifier is required to distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is trying to establish between words determines the nature of the operation one should perform on their associated vectors. We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when a data set has been constructed without ontological information, and when target words have not previously been seen in that position of a relationship in the training data. Hence, we believe the supervised methods are learning characteristics of the underlying feature space which are generalisable to new words (inhabiting the same feature space). In future work, we intend to apply this approach to the problem of labelling the distributional neighbours found for a given word with specific semantic relations. We also plan to investigate the use of bag-of-words (windowed) vectors instead of grammatical relations for this task. finally, we believe that the data sets constructed from WordNet, which we publish alongside this paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised and unsupervised methods. Acknowledgements This work was funded by UK EPSRC project EP/IO37458/1 “A Unified Model of Compositional and Distributional Compositional Semantics: Theory and Applications”.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language ProcessingMarco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23–32. Association for Computational LinguisticsJonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphsIn Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229, Uppsala, Sweden, July. Association for Computational LinguisticsShane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic compositionality of prefix verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 293–303, Cambridge, MA, October. Association for Computational LinguisticsDanushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011) 2258 In In Proceedings of the Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicographyIn Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ’89, pages 76–83, Stroudsburg, PA, USA. Association for Computational LinguisticsDaoud Clarke. 2009. Context-theoretic semantics for natural language: an overviewWorkshop of Geometric Models for Natural Language SemanticsJames Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of EdinburghGeorgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarityProceedings of the first Joint Conference on Lexical and Computational SemanticsChristaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, MassachusettsTrevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 466–476, Montr´eal, Canada, JuneMaayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceedings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107–114Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Conference on Computational Semantics (IWCS 2011), pages 125–134Zelig Harris. 1954. Distributional structure. Word, 10:146–162Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted WSD: finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1532–1541, Uppsala, Sweden, JulyAdam Kilgarriff and Colin Yallop. 2000. What’s in a thesaurus? Conference on Language Resources and Evaluation (LREC2000)Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics, 4(16):359–389Lillian Lee. 1999. Measures of distributional similarityAssociation for Computational Linguistics, pages 25–32, College Park, Maryland, USA, JuneAlessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the first Joint Conference on Lexical and Computational Semantics (*Sem)Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistics (COLING 1998)Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356–365, Cambridge, MA, October. Association for Computational LinguisticsTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representationsIn Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, JuneTristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguationIn Proceedings of COLING 2012, pages 1781–1796, Mumbai, India, DecemberJeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational LinguisticsJoakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on Incremental Parsing, pages 50–57 In Proceedings of the 37th Annual Meeting of the In Proceedings of the 2nd International 2259 Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 391–400, Atlanta, Georgia, June. Association for Computational LinguisticsEnrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38–42, Gothenburg, Sweden, AprilRion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. Advances in Neural Information Processing Systems 17Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidenceIn Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 801–808. Association for Computational LinguisticsRichard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211Gy¨orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1131–1141, Atlanta, Georgia, JuneIdan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templatesIn Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849–856, Manchester, UK, AugustJulie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81–88Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarityIn Proceedings of Coling 2004, pages 1015–1021, Geneva, Switzerland, Aug 23–Aug 27. COLINGJulie Weeds, David Weir, and Jeremy Reffin. 2014. Distributional composition using higher-order dependency vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality, EACL 2014, Gothenburg, Sweden, AprilDominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second Symposium on Quantum Interaction, Oxford, UK, pages 1–8Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, July. Association for Computational LinguisticsChen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756–766, Cambridge, MA, October. Association for Computational Linguistics</biblio>


  <preamble>Guy.pdf</preamble>
  <titre>Skill2vec: Machine Learning Approach for Determining the Relevant
Skills from Job Description</titre>
  <auteurs>
    <auteur>
      <name>Le Van-Duyet</name>
      <mail>me@duyet.net</mail>
      <afliation>N/A</afliation>
    </auteur>
    <auteur>
      <name>Vo Minh Quan</name>
      <mail>95.vominhquan@gmail.com</mail>
      <afliation>N/A</afliation>
    </auteur>
    <auteur>
      <name>Dang Quang An</name>
      <mail>an.dang1390@gmail.com</mail>
      <afliation>N/A</afliation>
    </auteur>
  </auteurs>
  <abstract>Unsupervise learned word embeddings have seen tremendous success in numerous Natural Language Processing (NLP) tasks in recent years. The main contribution of this paper is to develop a technique called Skill2vec, which applies machine learning techniques in recruitment to enhance the search strategy to nd candidates possessing the appropriate skills. Skill2vec is a neural network architecture inspired by Word2vec, developed by Mikolov et al. in 2013. It transforms skills to new vector space, which has the characteristics of calculation and presents skills relationships. We conducted an experiment evaluation manually by a recruitment company’s domain experts to demonstrate the effectiveness of our approach.</abstract>
  <introduction>Recruiters in information technology domain have met the problem nding appropriate candidates by their skills. In the resume, the candidate may describe one skill in different ways or skills could be replaced by others. The recruiters may not have the domain knowledge to know if one’s skills are t or not, so they can only nd ones with matched skills. In order to cope with the problem, one should try to nd the relatedness of skills. There are some approaches: building a dictionary manually, ontology approach, natural language processing methods, etc. In this study, we apply a word embedding method Word2Vec, using skills from online job post descriptions. We treat skills as terms, job posts as documents and nd the relatedness of these skills.</introduction>
  <body>To nd relatedness of skills, Simon Hughes [4] from Dice proposed an approach using Latent Semantic Analysis with an assumption that skills are related to skills which occur in the same context, and here contexts are job posts. This approach will build a term-document matrix and use Singular Value Decomposition to reduce the dimensionality. The cons of this approach is that when we have new data coming, we can not update the old term-document matrix, this leads to difculties in maintaining the model, as the change of trend in this domain is high. Google’s Data Scientists also face the same problems in Cloud Jobs API [7]. Their solution is to build a skill ontology dening around 50,000 skills in 30 job domain with relationships such as is a, related to, etc. This approach can represent complicate relationships between skills and jobs, but building such an ontology costs so much time and effort. To overcome the problem of relevant term, [3] present a new, effective log-based approach to relevant term extraction and term suggestion. The goal of [2] is to develop an automated system that discovers additional names for an entity given just one of its names, using Latent semantic analysis (LSA) [1]. In the example of authors, the city in India referred to as Bombay in some documents may be referred to as Mumbai in others because its name ofcially changed from the former to the latter in 1995. [5] is the introduction of an ontology-based skills management system at Swiss Life and the lessons learned from the utilisation of the methodology, present a methodology for application-driven development of ontologies that is instantiated by a case study. III. WORD2VEC ARCHITECTURE Word2Vec is a group of models proposed by Mikolov et al in 2013 [6]. It consists of 2 models: continuous bag-of-words and continuous Skip-gram, both are shallow neural networks that try to learn distributed representations of words with the target is to maximize the accuracy while minimizing the computational complexity. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. On the other hand, Skip-gram model try to predict surrounding context words based on the current word. In this work, we focus on Skipgram model as it is known to be better with infrequent words and it also give slightly better result in our experiment. The model consists of three layers: input layer, one hidden layer and output layer. The input layer take a word encoded using 1-of-V encoding (also known as one-hot encoding), where V is size of the dictionary. The word is then fed through the linear hidden layer to the output layer, which is a softmax classier. The output layer will try to predict words within window size before and after current word. Using stochastic gradient descent and back propagation, we train the model until it converges. This model takes vector dimensionality and window size as parameters. The author found that increasing the window size improves the quality of the word vector, and yet it increases the computational complexity. IV. METHODOLOGY A. Data collecting and processing Choosing a universal data set for the model is extremely important, the data should be large enough and should have balanced distributions over words (i.e. skills). of skills based on job descriptions. After crawling, we got a total of 5GB with more than 1,400,000 job descriptions. From these data, we extracted skills and performed as a list of skills in the same context, the context here includes skills in the same job description. The dataset is published at https://github.com/duyetdev/skill2vec-dataset The data structure is shown in table I. TABLE I DATA STRUCTURE Job description Context skills JD1 JD2 JD3 · · · Java, Spark, Hadoop, Python Python, Hive Python, Flask, SQL · · · B. Skill2vec architecture In this paper, for training the dataset, we used a neural network inspired by Word2Vec model as mentioned above. Here we treated our skills as words in Word2Vec model. In this study, with the documents contain only the skills, we chose the maximum window size, implied that every skills in the same job description are related to each other. For the vector dimensions, after some point, adding more dimensions provides diminishing improvements, so we chose this parameter empirically. To honour the work of Word2Vec model as it holds a big part in our study, we name our model Skill2Vec. gure 3 brieﬂy describes our Skill2Vec model. There are two dataset we need to concern, one (1) is the standard skills dictionary for the parser and another (2) is skills for training model; follow the gure 1. Career websites collect cleaning (1) Standard skills (*) parser collect Job descriptions Career websites (2) Training data g. 1. Pipeline of data collecting and processing rst, we collected and prepared a large dictionary of skills. With this dictionary, we can extract a set of skills from raw job descriptions. Skillss need to be cleaned into unique skills because there are many way to present a skill in job description (i.e. OOP or Object-oriented programming). gure 2 brieﬂy depicts the concept of the cleaning process. Raw skill (1) Remove punctuation (2) Dictionary replace (3) Regular expression (4) Lemmatization, Stemming Cleaned skill g. 2. Pipeline cleaning skills After that, we had the dictionary of skills ready for parsing. We collected a huge number of job descriptions from Dice.com one of the most popular career website about Tech jobs in USA. From these job descriptions, we extract skills for each one by using our skills dictionary (1). Now, the dataset is presented by a list of collections g. 3. Skill2vec architecture candidates and job post. In the other hand, candidates can nd the gap between the job post requirements and their ability, so they can nd the suitable trainings. A direction we can follow in the future: adding domain in training model, for example: Between Python, Java, and R, in Data Science domain, Python and R are more relevant than Java, however in Back End domain, Python and Java are more relevant than R. REFERENCES [1] Michael W Berry and Ricardo D erro. “Low-rank orthogonal decompositions for information retrieval applications”. In: Numerical linear algebra with applications 3.4 (1996), pp. 301–327. [2] Vinay Bhat et al. “nding aliases on the web using latent semantic analysis”. In: Data &amp; Knowledge Engineering 49.2 (2004), pp. 129–143. [3] Chien-Kang Huang, Lee-Feng Chien, and Yen-Jen Oyang. “Relevant term suggestion in interactive web search based on contextual information in query session logs”. In: Journal of the Association for Information Science and Technology 54.7 (2003), pp. 638–649. [4] Simon Hughes. How We Data-Mine Related Tech Skills. 2015. URL: http : / / insights . dice . com / 2015/03/16/how-we-data-mine-relatedtech-skills/ (visited on 09/12/2017). [5] Thorsten Lau and York Sure. “Introducing ontologybased skills management at a large insurance company”. In: Proceedings of the Modellierung. 2002, pp. 123–134. “Efcient Estimation of al. Word Representations in Vector Space”. In: CoRR abs/1301.3781 (2013). URL: http://dblp.unitrier . de / db / journals / corr / corr1301 . html#abs-1301-3781. [7] Christian Posse. Cloud Jobs API: machine learning goes to work on job search and discovery. 2016. URL: https : / / cloud . google . com / blog / big data/2016/11/cloud-jobs-api-machinelearning-goes-to-work-on-job-searchand-discovery (visited on 03/03/2018). [6] Tomas Mikolov et V. EXPERIMENTAL SETUP To evaluate our method, we have an expert team assesses the result following these steps: 1) Pick 200 skills randomly from our dictionary. 2) Our system will return top 5 ”nearest” skills for each. 3) The expert team will check if these top 5 ”nearest” skills are relevant or not. The experiment showed that 78% of skills returned by our model is truly relevant to the input skill. We present the experimental results in table II TABLE II QUERY TOP 5 RELEVANT SKILLS Query skill Top relevant skills HTML5 OOP Hadoop css3 bootstrap front end angular responsive OOD Objective Java Multithread Software Debug Pig Hive HBase Big Data Spark Zookeeper Spark Scala Data System Sqoop solrcloud Pig HDFS Hadoop Spark Impala Hive</body>
  <conclusion>In this paper, we developed a relationship network between skills in recruitment domain by using the neural net inspired by Word2vec model. We observed that it is possible to train high quality word vectors using very simple model architectures due to lower cost of computation. Moreover, it is possible to compute very accurate high dimensional word vectors from a much larger dataset. Using Skip-gram architecture and an advanced technique for preprocessing data, the result seems to be impressive. The result of our work can contribute to building the matching system between candidates and job post. In the other hand, candidates can nd the gap between the job post requirements and their ability, so they can nd the suitable trainings. A direction we can follow in the future: adding domain in training model, for example: Between Python, Java, and R, in Data Science domain, Python and R are more relevant than Java, however in Back End domain, Python and Java are more relevant than R.</conclusion>
  <discussion>N/A</discussion>
  <biblio>[1] Michael W Berry and Ricardo D erro. “Low-rank orthogonal decompositions for information retrieval applications”. In: Numerical linear algebra with applications 3.4 (1996), pp. 301–327. [2] Vinay Bhat et al. “nding aliases on the web using latent semantic analysis”. In: Data &amp; Knowledge Engineering 49.2 (2004), pp. 129–143. [3] Chien-Kang Huang, Lee-Feng Chien, and Yen-Jen Oyang. “Relevant term suggestion in interactive web search based on contextual information in query session logs”. In: Journal of the Association for Information Science and Technology 54.7 (2003), pp. 638–649. [4] Simon Hughes. How We Data-Mine Related Tech Skills. 2015. URL: http : / / insights . dice . com / 2015/03/16/how-we-data-mine-relatedtech-skills/ (visited on 09/12/2017). [5] Thorsten Lau and York Sure. “Introducing ontologybased skills management at a large insurance company”. In: Proceedings of the Modellierung. 2002, pp. 123–134. [6] Tomas Mikolov et al. “Efcient Estimation of Word Representations in Vector Space”. In: CoRR abs/1301.3781 (2013). URL: http://dblp.uni-trier . de / db / journals / corr / corr1301 .html#abs-1301-3781. [7] Christian Posse. Cloud Jobs API: machine learning goes to work on job search and discovery. 2016. URL: https : / / cloud . google . com / blog / big data/2016/11/cloud-jobs-api-machinelearning-goes-to-work-on-job-searchand-discovery (visited on 03/03/2018).</biblio>


  <preamble>infoEmbeddings.pdf</preamble>
  <titre>An Empirical Evaluation of Text Representation Schemes on
Multilingual Social Web to filter the Textual Aggression</titre>
  <auteurs>
    <auteur>
      <name>Sandip Modha</name>
      <mail>sjmodha@gmail.com</mail>
      <affiliation>DA-IICT Gandhinagar,India</affiliation>
    </auteur>
    <auteur>
      <name>Prasenjit majumder</name>
      <mail>prasenjit.majumder@gmail.com</mail>
      <affiliation>DA-IICT Gandhinagar,India</affiliation>
    </auteur>
  </auteurs>
  <abstract>Due to an exponential rise in the social media user base, incidents like hate speech, trolling, cyberbullying are also increasing and that lead hate speech detection problem reshaped into different tasks like Aggression detection, Fact detection. This paper attempt to study the effectiveness of text representation schemes on two tasks namely: User Aggression and Fact Detection from the social media contents. In User Aggression detection, The aim is to identify the level of aggression from the contents generated in the Social media and written in the English, Devanagari Hindi and Romanized Hindi. Aggression levels are categorized into three predefined classes namely: ‘Non-aggressive‘, ‘Overtly Aggressive‘, and ‘Covertly Aggressive‘. During the disaster-related incident, Social media like, Twitter is flooded with millions of posts. In such emergency situations, identification of factual posts is important for organizations involved in the relief operation. We anticipated this problem as a combination of classification and Ranking problem. This paper presents a comparison of various text representation scheme based on BoW techniques, distributed word/sentence representation, transfer learning on classifiers. Weighted F 1 score is used as a primary evaluation metric. Results show that text representation using BoW performs better than word embedding on machine learning classifiers. While pre-trained Word embedding techniques perform better on classifiers based on deep neural net. Recent transfer learning model like ELMO, ULMfiT are fine-tuned for the Aggression classification task. However, results are not at par with pretrained word embedding model. Overall, word embedding using fastText produce best weighted F 1 -score than Word2Vec and Glove. Results are further improved using pre-trained vector model. Statistical significance tests are employed to ensure the significance of the classification results. In the case of lexically different test Dataset, other than training Dataset, deep neural models are more robust and perform substantially better than machine learning classifiers.</abstract>
  <introduction>The Social Web is a great source for studying human interaction and behavior. In the last few years, there is an exponential growth in Social Media user base. Sensing content of Social Media like Facebook, Twitter, by the smart autonomous application empower its user community with real-time information which is unfolded across the different part of the world. Social media provide the easiest and anonymous platform for common people to voice their opinion or view on a various entity like celebrity, politician, product, stock market etc or any social movement. Sometime such opinions might be aggressive in nature and propagate hate in the social media community. With the unprecedented increase in the user base of the social media and its availability on the Smartphones, incidents like Hate speech, trolling, Cyberbullying, and Aggressive posts are increasing exponentially. A smart autonomous system is required which enable surveillance on the social media platform and detect such incidents. Some of the researchers look posts from the aspect like aggression Kumar Ritesh et al. (2018) to filter the contents. some of the posts contain words which might be qualified as either highly or overly aggressive or have hidden aggression. Sometimes posts do not have any aggression. Based on these, posts or comments are categorized into three classes namely: ‘Overtly Aggressive‘, ‘Covertly Aggressive‘ and ‘Non-aggressive‘ Kumar Ritesh et al. (2018). Henceforth, in the rest of the paper, we will denote these classes by these abbreviations namely: OAG, CAG, NAG respectively.Table 1 shows the sample posts belonging to these classes. Social Media, specifically Microblog has proved its importance during the disasterrelated incidents like an earthquake, Hurricane and floods 1 . Organizations involved in relief operation actively track posts related to situational information posted on Facebook and Twitter during the disaster. However, At the same time, social media is flooded with lots of prayer and condolence messages. Posts which contain factual information are extremely important for the organization involved in post-disaster relief operations for coordination. filtering and Ranking of the posts containing factual information will be very useful to them. We believe that this is the special problem of the Sentiment Analysis task. We consider this problem as a combination of twoclass classification problem: factual posts and nob-factual posts plus Ranking. Table 2 shows the example of the posts of belong to these class. The Text representation of social web content plays a pivotal role in any NLP task. Bag-of-word is the oldest and simple technique to represent the document or post into a fixed length vector. The BoW techniques generate very sparse and high dimensional space vector. Text representation using distributed word/sentence representation or word embedding is gain rapid momentum recently. In this paper, one of the objectives s to find the best text representation scheme to model social web content for the machine learning classifier and deep neural net. Various Text representation scheme based on BoW, word embedding and are studied empirically. We have reported result on popular word embedding technique like Word2vec, Glove and fastText on standard machine learning classifier like Multinomial Naive Bayes (MNB), Logistic Regression(LR), K-Nearest neighbors KNN, Support Vector Classifier (SVC), Decision Tree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost, Perceptron, Deep neural net based on LSTM, CNN and Bidirectional LSTM. Results are also reported on Doc2vec embedding, a popular sentence or paragraph embedding technique for the above classifiers. Transfer Learning is well practiced in the area of computer vision. However, in the NLP, transfer learning has limited application in the form of pre-trained word vector which is used to initialize the weights of the embedding layer of the deep neural network. With the advent of transfer learning method like ELMO (Peters et al. , 2018), ULMfiT (Howaard et al., 2018) claimed substantial improvement in the performance of various NLP tasks like Sentiment Analysis, Question/Answering, Textual Entailment empirically. The main idea behind these methods is to train language model on the large corpus and fine tune on the task-specific corpus. In this paper, We have evaluated the performance of these methods in the Aggression classification tasks. 1.1. Research Questions In this study, experiments are performed on the benchmark dataset with to answer the following questions • Which is the best Text Representation scheme to model text from the Social Web? • Does pre-trained language model based on transfer learning better than pretrained word embedding based on shallow transfer learning on Social media data? • Does Making too Deep Neural net make sense? To answer all research question listed above, experiments are performed on two tasks namely: Aggression detection (Trolling Aggression and Cyberbullying (TRAC) dataset) Kumar et al. (2018) and Fact detection (fiRE iRMDI Dataset)Basu et al. (2018). In this paper, we present exhaustive benchmarking of text representation schemes on these datasets. Our results reveal that fastText with pre-trained vector along with CNN outperform standard machine learning classifiers based on BoW Model and marginally perform better than Word2vec and Glove. Paragraph vector or Doc2vec Le, Quoc et al. (2014) perform very poor on our dataset and turn out to be the worst text representation scheme among all. We also found that model based on the deep neural net is more robust than machine learning classifier when tested on lexically different dataset than training Dataset. i.e. deep neural model substan3tially outperforms machine learning classifier on Twitter test Dataset while trained on Facebook Dataset in this evaluation. To validate our claims, statistical significance tests are performed on weighted F 1 score of the classifier for each text representing scheme. Statistical inference is used to check evidence to support or reject these claims. Significance tests like Wilcoxon signed-rank and Student t-test were carried out by comparing weighted F 1 score all the text representation scheme with the fastText pre-trained vector. In most of the cases, p-values are less than 0.05. The rest of the paper is organized as follows: In section 2, we review the relevant works in the area of Sentiment analysis and hate speech detection. Section 3 contains the detail information about the various benchmark Datasets used in the experiments. Various Text Representation schemes are described in section 4. We formally describe the evaluation task and models in section 5. We report results in section 6 and present detail result analysis in section 7. We conclude the discussion and provide insight for the future work in section 8.</introduction>
  <body>Bag-of-Words (BoW) (Harris et al. , 1954) is the oldest technique to represent the text of the documents in fixed-length vectors with high dimensionality. Mikolov et.al (2013) proposed two architecture namely: skip-gram(SG) and continuous-bag-of word (CBOW) to learn high quality low dimensional word embedding. However, to generate sentence vector often, average or mean of word vector are considered. Doc2vec or paragraph vector Le, Quoc et al. (2014) proposed Paragraph2vec (Doc2vec) which is the extension of the Word2vec to learning document level embedding. It is an unsupervised method which learns document vector from paragraph, sentence or document. Pennington et.al. (2014) proposed word embedding based on the co-occurrence matrix. Lau et al. (2016) have performed a comprehensive evaluation of Doc2Vec on two tasks namely: Forum Question Duplication and Semantic Textual Similarity (STS) task. Authors claimed that Doc2Vec performs better than Word2vec provided that models trained on large external corpora, and can be further improved by using pretrained word embedding. They have published the hyper-parameter for the Doc2Vec embedding. Our work is similar to this but we have reported the evaluation of all the text representation scheme including doc2vec on TRAC dataset(Kumar et al. , 2018) on each classifier. Hate speech is a type of language which is used to incite or spread violence towards the group of people based on the gender, community, race, religion. Sentiment analysis and hate-speech are closely related in fact sentiment analysis techniques are used in hate speech detection. Initially, Sentiment Analysis problem is formulated as a binary classification problem for predicting the election results or detecting political opinion (Conover et al. , 2011; Conover Micheal et al. , 2011; Maynard et al., 2011; Tumasjan et al. , 2010) on Twitter. Then after, It turned into the multi-class classification problem with the introduction of the neutral label. Soon, Researchers come with different notion like aggression (Kumar Ritesh et al., 2018), cyberbullying(xu et al. , 2012), sarcasm, trolling. Semeval (International workshop on semantic evaluation)(Rosenthal et al. , 2017) is one of the popular competition on sentiment analysis which is started since 2013. TRAC 2 (Trolling, aggression, cyberbullying) workshop 2 https://sites.google.com/view/trac1 4(Kumar Ritesh et al., 2018) co-located with the International Conference of Computational Linguistics (COLING 2018) redefine hate speech detection task in terms of three type of aggression namely: Non-Aggression (NAG), Overly-Aggression(OAG) and Covertly Aggression (CAG). 2.1. Sentiment Analysis During the initial year, there is a lack of standard dataset for comparative performance analysis. International Workshop on Semantic Evaluation 2013 (SemEval-2013) (Hltcoe et.al , 2013) was the first forum who developed standard tweet dataset for the benchmarking of the various sentiment analysis system. Most of the team who had participated in the competition used supervised approaches based on SVM, Naive Bayes, and Maximum Entropy. some of the team had used ensemble classifier and rule-based classifier. Mohammad et al. (2013) was the top team of the Semeval-2013 challenge. They have incorporated various semantic and lexicon based sentiment features for the experiment and SVM was used for the classification. Deep learning and word embedding had shown its footprints in SemEval-2015 (Rosenthal et al. , 2015). Team UNITN (Severyn et al. , 2015) was the second team in the message polarity task. They have build convolution neural network for the sentiment classification. They have used an unsupervised neural language model to initialize word embeddings that are further tuned by deep learning model on a distant supervised corpus (Severyn et al. , 2015). In fourth edition SemEval-2016 (Nakov et al. , 2016),Team SwissCheese (Deriu et al. , 2016) was the first ranked team with F 1 score around 63.3 %. Their approach was based on 2-layer convolution neural networks whose predictions are combined using a random forest classifier. SemEval-2017 (Rosenthal et al. , 2017) was the fifth edition, Team DataStories (Baziotis et al. , 2017) was the top-ranked team with AvgRec= 68.1 and F 1 around=67.7 %. They use Long Short-Term Memory (LSTM) networks augmented with two kinds of attention mechanisms, on top of word embedding pre-trained on a big collection of Twitter messages without using any hand-crafted features. 2.2. Hate Speech/Cyberbullying/Aggression Detection Hate Speech Detection research attracts researchers from the diverse background like Computational linguistic, computer science, social science. The actual term hate speech was coined by Warner et al. (2012). Various Authors used different notion like offensive language (Razavi et.al , 2010), Cyberbullying (xu et al. , 2012), Aggression (Kumar et al. , 2018). Davidson et al. (2017) studied tweet classification of hate speech and offensive language and defined hate speech as following: language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. Authors observed that offensive language often miss-classified as hate speech. They have trained a multi-class classifier on N-gram features weighted by its TF-IDF weights and PoS tags. In addition to these, features like sentiment score of each tweet, no of hashtags, URLS, mentions are considered. Authors concluded that Logistic regression and Linear SVM perform better than NB, Decision Tree, Random Forests. Schmidt et al. (2017) perform comprehensive survey on hate speech. They have identified features like Surface features, sentiment, word generalization,lexical, linguistics etc. can be used by classifier. Cyberbullying is the type bullying that occurs on social media platform or app via cellphone or any internet enabled device. xu et al. (2012) introduces Cyberbullying to 5the NLP community. They have performed various binary classification on tweets text with bullying perspective to determine whether the user is cyberbully or not. They reported binary classification accuracy around 81%. Kwok et al. (2013(@), authors performed classification using NB classifier on tweets based on two classes :racist and non-racists and achieved accuracy around 76 %. Burnap et al. (2015), authors studied cyber hate on Twitter. They have used various classifier like SVM, BLR, RFDT, Voting base ensemble for the binary classification achieved best F1-score of 0.77 in the voted ensemble.Malmasi, et al. (2017), authors have used NLP based lexical approach to address the multi-class classification problem. They have used character N-gram, word N-gram and word skip-gram feature for the classification. Schmidt et al. (2017), have described the key areas that have been explored to detect hate speech. They have surveyed different types of features used for hate speech classification. They have categorized features in Simple surface features, word generalization features, sentiment features, linguistic features, lexical resources features, Knowledgebased features, and Meta-Information features Simple surface features include features like character level unigram/n-gram, word generalization features include features like the bag-of-words, clustering, word embedding, paragraph embedding. Linguistic features include PoS tag of tokens. list of bad words or hate words can be considered as a lexical resource. Malmasi et al. (2018), tried to address the problem of discriminating profanity from the hate speech in the social media posts. n-grams, skip-gram and clustering based word representation features are considered for the 3-class classification.The Author use SVM and advance ensemble based classifier for this task and achieved 80 % accuracy. Aroyehun et al. (2018) performed translation as data augmentation strategy. TRAC Dataset (Kumar et al. , 2018) was also augmented using translation and pseudo labeled using an external dataset on hate speech. they have reported best performance with LSTM and F 1 score around 0.6415 on TRAC English dataset (Kumar et al. , 2018). Arroyo et al. (2018) implement ensemble of the Passive-Aggressive (PA) and SVM classifiers with character n-grams. TF-IDF weighting used for feature representation. fiRE initiative also gave importance text representation in Indian language since its inception. (Majumder et al. , 2008)(Majumder et al. , 2007). 3. Dataset Experiments are performed on standard benchmarked Datasets to evaluate the performance of various text representation scheme. For User Aggression detection problem, Trolling, Aggression and Cyberbullying TRAC (Kumar et al. , 2018) is considered for the experiments which contain post in English and code-mixed Hindi. For the Factual Detection task, experiments are performed on fiRE IRMiDis Dataset. 3.1. TRAC Dataset TRAC (Trolling, Aggresion and Cyberbullying) consist of 15,001 aggression-annotated Facebook Posts and Comments each in Hindi (Romanized and Devanagari script) and English for training and validation Kumar et al. (2018). 6Table 3. Class distribution in the Training Dataset English Corpus NAG CAG OAG Total Hindi Corpus # Training # Validation # Training # Validation 5,052 4240 2, 708 12, 000 1, 233 1, 057 711 3, 001 2, 275 4, 869 4, 856 12, 000 538 1, 246 1217 3, 001 Table 4. Test Data Corpus statistics Test Dataset # of posts Facebook English Corpus Twitter English Corpus Facebook mixed script Hindi Corpus Twitter mixed script Hindi Corpus 916 1, 257 970 1, 194 3.2. fiRE IRMiDis Dataset Forum for Information Retrieval Evaluation, have introduced Microblog track since 2016 as Information Retrieval from Microblogs during Disasters (IRMiDis). IRMiDis track (Basu et al. , 2018) of fiRE is organized with the objective to extract factual or fact-checkable tweets during the disaster which might be helpful to the victims or the people who are involved in the relief operation. Dataset contain tweets which are downloaded from the Twitter during Nepal earthquake 2015. Following are the example of factual or fact-checkable and non-fact-checkable tweet.Table 5 shows a detail statistics of fiRE IRMiDis Dataset. As we look at the table, There are only 83 tweets is annotated with objective class. not a single tweet is annotated from the subjective class. 4. Text Representation Schemes The main objective of this paper is an identification of the best text representation scheme for the Social media text which is very sparse and noisy in nature. Text representation is about representing documents in a numerical way so that they can be feed as an input to the classifier. This numerical representation is in the form of the vectors which together form matrices. Essentially, There are two types of text representation scheme :(i) Bag-of-words(BoW) (ii) Distributed Word/sentence representation. BoW with count vector and TF/IDF weighting , various word embedding techniques(Word2Vec, Glove, fastText), and sentence or paragraph embedding (Doc2Vec) are studied. Table 5. fiRE IRMiDis Dataset statistics Particulars # tweets Remark Number of Tweets Labelled Tweets Classes 50000+ 83 2 only tweets belong to Factual class 74.1. Bag-of-Word Model for Text Representation The Bag-of-words is the simple technique to represent the document or social media posts in the vector form and also a very common feature extraction method from the text. Word count or TF/IDF weight of each n-gram word can be used as a features. The dimension of the vector is equal to the size of vocabulary of the text corpus or dataset which results in very high dimensional sparse document vector. It is the common method used for the text representation in order to perform various NLP task like text classification, clustering. However,the BoW methods ignore the word order which may lead to loss of the context. 4.2. Word Embedding for Text representation Word Embedding is the text representation technique to represent the word in the low dimensional space so that semantically similar word have similar representation. Major word embedding techniques like Word2vec learn word embedding using shallow neural network. The fastText, extension of Word2vec, consider the morphological structure of the word. 4.2.1. Word2Vec Word2vec (Mikolov et.al , 2013) is the unsupervised and predictive neural word embedding technique to learn the word representation in the low dimensional space. Word2vec is a two-layer neural net that take text corpus as an input and output is a set of vectors. two novel model architectures: Skipgram and CBOW(Continuous bag of words) are proposed for computing continuous vector representations of words from very large data sets. 4.2.2. Glove GloVe stands for Global vector for [Word Representation] (Pennington et.al. , 2014)is an unsupervised method for learning word embedding. A Co-occurrence word matrix is created from the text corpus for the training and is reduced in low dimensional space which explain the variance of high dimensional data and provide word vector for each word. 4.2.3. fastText fastText (Bojanowski et al. , 2017) is the neural word embedding technique which learn distributed low dimensional word embedding. Word2vec, Glove consider each word as single unit and ignore the morphological structure of the word. They are not able generate word embedding for the unseen or out of vocabulary word during the training. fastText overcome this limitation of Word2vec and GLOVE by considering each word as N-gram of characters. A word vector for a word is computed from the sum of the n-gram characters. The range of N is typically 3 to 6. Since user on social media often make spelling error, typos, fastText will be more effective then rest of two. 4.3. Paragraph vector/Doc2vec Paragraph Vector is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and 8documents (Le, Quoc et al. , 2014). Paragraph vector represents each document by a dense vector which is trained to predict words in the document. Authors believe that Paragraph vector have the potential to overcome the weaknesses of bag-of-words models and claimed that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Paragraph vector model is also referred as doc2vec model. Henceforth, we will refer paragraph and Doc2vec interchangeably. Doc2vec model have two architecture namely : (i) DM: This is the Doc2Vec model analogous to CBOW model in Word2vec. The paragraph vectors are obtained by training a neural network on the task of inferring a center word based on context words and a context paragraph. (ii) DBOW: This is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph. 4.4. Transfer Learning Transfer Learning in NLP is not as matured as compare to in Computer Vision. Transfer learning is a method in which model is trained on large corpus for a particular task and use this pre-trained model for the similar task. There are two way to use transfer learning in NLP (i) Use of Pre-trained word embedding to initialize first layer of neural network model which can be called as shallow representation. (ii) Use the full model and fine tune for the task specific in supervise learning way. Word2vec, Glove and fastText provide pre-trained word vector trained on the large corpus. Google Word2vec pre-trained model have word vector for 3 million words with size 300 and trained on Google news. Glove pre-trained model available with different embed size and trained on common crawl, Twitter. We have use Glove pre-trained model with vocabulary size 2.2 million and trained on common crawl. fastText pretrained models are available in 157 language. We have use fasttext pre-trained vector for Englsih and Hindi language trained on commnon crawl and wikipedia. Recently, transfer learning in NLP done in new way; first language model is trained on large text corpus in unsupervise way and fine tune on specific task like text classification on labeled data. Peters et al. (2018) author argued that word representation is depend upon the context. So each word has different word vector depending upon the position of the word in the sentence. Essentially Each word has dynamic word vector with respect to the context as opposed to the traditional word embedding techniques which always give same word vector ignoring the context. Embedding from Language Models (ELMos) use languge model for the word embedding. Howaard et al. (2018) author propose Universal Language Model fine-Tuning for Text Classification (ULMfiT) which is bi-LSTM model that is trained on a general language modeling (LM) task and then fine tuned on text classification. Results are reported on both transfer learning model on TRAC dataset (Kumar et al. , 2018). 5. Evaluation Tasks We have benchmarked various text representation scheme on two specialized NLP task namely: aggression detection and fact detection. Text Representation scheme are evaluated on machine learning and deep neural model. 95.1. Aggression Detection task The objective of this task is to identify type of aggression present in the text in both Englsih and code-mixed Hindi language. Aggression are classified into three level namely: ‘Overtly Aggressive‘ (OAG), ‘Covertly Aggressive‘ (CAG) and ‘Nonaggressive‘ (NAG). We have implemented all standard machine learning classifiers like Multinomial Naive Bayes (MNB), Logistic Regression(LR), K-Nearest neighbors (KNN), Support Vector Classifier (SVC), Decision Tree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost, Perceptron, and various voting based ensemble with different text representation schemes like count based, TF/IDF and word embedding to prepare baseline results. Various word embedding techniques like Word2Vec, Glove, fastText, Paragraph2Vec are studied. 5.1.1. Problem statement Basically Aggression detection is a Text classification problem. Formally, the task of Text Classification is stated as follows. Given a set of social media feed and a set of classes, We need to compute a function of the form: C = f (T, Ω) where f is the multi-class classifier that is computed using training data, T is the numeric representation of the text of the dataset, Ω is the set of parameters of the classifier and C is the pre-define class-labels. 5.1.2. Model Architectures and Hyperparameters In this subsection, we will discuss the architecture and hyperparameters of our deep neural model used for the classification. Model learns feature from the input texts Ther is no need to design hand-crafted features which used to encode text into feature vector. 5.1.2.1. Bidirectional LSTM. The first model is based on the Bidirectional LSTM include embedding layer with embed size 300, convert each word from the post into a fixed length vector. short posts are padded with zero values. Subsequent layers includes Bidirectional LSTM layer with 50 memory units followed by one-dimensional global max pooling layer, a hidden layer with size 50 and output layer with softmax activations. ReLU activation function is used for the hidden layer activation. A drop out layer is added between the last two layers to counter the overfitting with parameter 0.1. Hyperparameters are as follows: Sequence length is fixed at 1073 word; maximum length of posts in the dataset. No of features is equal to half of total vocabulary size. Models are trained for 10 epoch with batch size 128. Adam optimization algorithm is used to update network weights. 5.1.2.2. Single LSTM with higher dropout. This model is based on the Long Short Term Memory, a type of recurrent neural network with higher dropout. This model is having one embedding layer, one LSMT layer with a size 64 memory unit, and one fully connected hidden layer with Relu activation and size 256 and an output layer with softmax activation. Hyperparameters are same as discussed in the previous model. A dropout layer is added between the hidden layer and an output layer with 10drop out rate 0.2 to address the overfitting issue. 5.1.2.3. CNN Model. This model includes one embedding layer whose weights are initialized with fastText pre-trained vector with embed size is 300, followed by one-dimensional convolution layer with 100 filters of height 2 and stride 1 to target biagrams. In addition to this, Global Max Pooling layer added to fetch the maximum value from the filters which are feed to the fully connected hidden layer with size 256, followed by output layer. ReLU and softmax activation function are used for the hidden layer and output layer respectively. 5.1.2.4. CNN model with Multiple Convolution layer. This model includes embedding layer with embed size 300. Three one dimensional convolution layers with size 100 and different filters with height 2,3,4 to target bigrams, trigrams, and fourgrams features, followed by max pooling layer which concatenate max pooled result from each of one-dimensional convolution layer. The final two layers include a fully connected hidden layer with size 250 and output layer with ReLu and softmax activation. A Drop out layer is added between the last two layer with rate 0.2. Hyperparameters are same as discussed in the first model. This model is similar to proposed by (Zhang et al. , 2015). 5.2. Factual Post/Tweet Detection from Social Media During the emergency situation like earthquake or floods, Microblog plays a very important role as an anonymous communication medium. The various entity like, Volunteers, NGOs involved in relief operation always look for real-time information which contains facts instead of prayer and condolence messages. In more technical term, these agencies are looking for factual information from Microblog instead of the subjective information. In addition to this, the system should generate rank-list of the tweets based upon the worthiness of facts. we considered this problem as a binary classification problem plus pure IR Ranking problem. two classes can be labeled as factual and non-factual. the IRMiDis dataset 3 ,which was prepared from the tweet posted during Nepal earthquake 2015 Basu et al. (2018) is considered for the experiment. There are only 83 fact checkable tweets in the dataset. Non-factual tweets are not available. Total no of tweets in the dataset is more than 50000. 5.2.1. Preparation of Training Data Due to the unavailability of adequate training data, The first task is to prepare training data to train the deep neural model. We randomly choose 100 tweets from the dataset and labeled as a non-fact-checkable tweet and 83 fact-checkable tweets present in the dataset labeled as fact-checkable. We have trained our Convolution neural network on these training data and tested the model on the remaining 50000 tweets. At this stage we are not interested in the class but, we have sorted all the tweets based upon the predicted probability of the fact-checkable class and selected top 2000 tweets. We have randomly selected tweets and gave relevance judgment based upon availability of factual information in first 1000 tweets and manually 3 https://sites.google.com/site/irmidisfire2018/ 11extracted 300 tweets as non-fact-checkable tweets to minimize the false positives. Remaining 1700 tweets labeled as fact-checkable tweets. We selected the last 1700 tweets with the least probability of the class fact-checkable and labeled them as non-fact-checkable tweets. So our Training corpus has 1783 fact-checkable and 2000 non-fact-checkable tweets. 5.2.2. Proposed Approach We have used word embedding to represent the text instead of bags-of-words. fastText (Mikolov et al. , 2018) pre-trained vector with 300 dimensions is used to initialize the weight matrix of the embedding layer of the network. We trained our CNN model on this training corpus with 10-fold cross-validation.The Model gives validation accuracy around 94%. finally, we run the model on the entire corpus and sorted the tweet based upon the predicted probability of the Fact-checkable class. Essentially this approach termed as weakly-supervise classification. 6. Results In this section, we first present results of classifiers TRAC dataset Kumar et al. (2018) with different text representation scheme. Latter we present result on fiRE IRMiDis 2018 Dataset. Tweets are very noisy in nature contains user mentions, Hashtags, Emojis, and URLs. We do not perform any kind of text pre-processing on tweets in experiments with deep neural models. In experiments with machine learning classifier, before classification, Hashtag symbol # and User mentions are dropped from the tweets. Non-ASCII characters and stop-words are removed from tweet text (Modha et al. , 2016). 6.1. Results On TRAC Dataset Precision, Recall, and F1-score are the standard metrics which are used to evaluate the classifier performance. We have evaluated 16 classifiers performance on 4 Datasets (2 English+2 Hindi) with 10 Text Representation scheme(8 in the case of Hindi Dataset). Looking at such massive experiment, it is difficult to report results in all the above metrics. Therefore, Results are reported in terms of weighted F1-score only which is the function of Precision and Recall. Classifiers results based on LSTM and CNN on BoW text representation schemes are not possible due to the high dimensionality. Bernoulli classifier is used instead of Naive Bayes Classifier in case of text representation schemes other than BoW. Since word vectors might have negative weights, it is impossible to calculate probabilities with negative weights. Skip-gram variant of Word2Vec and fastText is used in this experiment instead of continuous bag-of-word. Table 6 and 7 shows results on Facebook and Twitter English Dataset with BoW and word embedding while Table 8 present result with pre-trained word embedding with same dataset. Table 9 and 10 shows results on Facebook and Twitter code-mixed Hindi Dataset. Only fastText provide pre-trained word vector (Mikolov et al. , 2018) for the Hindi language. Exhaustive evaluation is performed with all classifiers with respect to each text representation schemes. Experiments are also performed with the new transfer learning model like ELMO and ULMfiT. Table 11 presents results on both Facebook and Twitter English Datasets. figure 1 and figure 2 display the heatmap of 12Table 6. F1-score on TRAC Facebook English Dataset Classifier Countvector TF/IDF W2Vec Glove Fasttext doc2vecdmc doc2vecdbow NB LR KNN SVC DT SGD RF Ridge AdaB Perce. ANN Ensemble LSTM BLSTM CNN NCNN 0.5571 0.5953 0.5466 0.5801 0.5269 0.5706 0.5621 0.6009 0.6210 0.5387 0.5703 0.58 0.5596 0.6046 0.5428 0.5902 0.5055 0.5938 0.5582 0.5999 0.6141 0.5491 0.5350 0.5900 0.3873 0.5358 0.5130 0.5037 0.4067 0.3571 0.4752 0.5336 0.4932 0.4020 0.5037 0.4067 0.5454 0.4760 0.5365 0.5488 0.5035 0.5400 0.5113 0.5137 0.5002 0.5167 0.5513 0.5225 0.5644 0.4848 0.5380 0.5617 0.5062 0.5641 0.5638 0.4849 0.4585 0.5266 0.5114 0.5388 0.4198 0.5060 0.4230 0.5385 0.4689 0.3800 0.4980 0.4980 0.4634 0.5139 0.5095 0.5033 0.4198 0.3521 0.4210 0.5083 0.4852 0.3253 0.4401 0.4401 0.4870 0.5675 0.5061 0.5369 0.4468 0.4647 0.5199 0.5347 0.5491 0.5230 0.5350 0.5558 0.5649 0.5759 0.5515 0.5919 Table 7. F1-score on TRAC Twitter English Dataset Classifier Countvector TF/IDF W2Vec Glove Fasttext doc2vecdmc doc2vecdbow NB LR KNN SVC DT SGD RF Ridge AdaB Perce. ANN Ensemble LSTM BLSTM CNN NCNN 0.5102 0.4849 0.3539 0.4642 0.4229 0.4682 0.4199 0.4703 0.3343 0.4930 0.4912 0.495 0.4528 0.4890 0.2891 0.4853 0.4111 0.5020 0.3917 0.5003 0.3696 0.4778 0.5164 0.4842 0.3936 0.3959 0.3607 0.3627 0.3673 0.4182 0.3634 0.3877 0.3552 0.3938 0.3552 0.3938 0.5156 0.3860 0.5377 0.4984 0.5495 0.3871 0.3997 0.2858 0.3948 0.3838 0.4069 0.3180 0.4215 0.3340 0.4532 0.4471 0.5335 0.4985 0.4849 0.5179 0.3254 0.3041 0.3225 0.3019 0.3326 0.3251 0.3301 0.2994 0.3288 0.3015 0.3230 0.3230 0.3536 0.3274 0.3191 0.3274 0.3326 0.3350 0.3293 0.3243 0.3223 0.2990 0.3281 0.3281 0.5551 0.3457 0.3843 0.3078 0.3884 0.4512 0.4333 0.3352 0.4485 0.3521 0.5111 0.4500 0.5385 0.5314 0.5012 0.5120 the results achieved by classifiers on each text representation scheme. 6.2. Information Retrieval from Microblogs during Disasters (IRMiDis) Dataset As discussed in the previous section 1, This task is classification plus Ranking task. Table 13 shows our system results on IRMiDis dataset (Basu et al. , 2018) along with the rest of teams. nDCG overall is the primary metric for the evaluation. Our system substantially outperforms rest of team in the most of the metrics which justifies our claim established on TRAC dataset (Kumar et al. , 2018) 7. Result Analysis In this section, we will present the comprehensive result analysis and try to answer the research questions which framed before the experiments were performed. As we look at the table 6 7, and 8, Overall, LSTM and CNN with pre-trained fastText word embedding marginally outperform (around 2 % to 4%) standard machine learning 13Table 8. F1-score on TRAC Facebook and Twitter English Dataset:Using Pre-trained word vectors. Facebook Test Dataset Twitter Test Dataset Classifier p-Word2vec p-Glove p-Fasttext p-Word2vec p-Glove p-Fasttext NB LR KNN SVC DT SGD RF Ridge AdaB Perce. ANN Ensemble LSTM BLSTM CNN NCNN 0.5342 0.5799 0.4981 0.5832 0.4700 0.5019 0.5402 0.5829 0.5713 0.5114 0.5025 0.5300 0.4979 0.5501 0.4749 0.5169 0.5373 0.6050 0.5103 0.5678 0.4515 0.5521 0.5338 0.5952 0.5781 0.5201 0.5498 0.5500 0.4979 0.6062 0.5405 0.5883 0.5519 0.6045 0.4819 0.6120 0.4900 0.5360 0.5505 0.6140 0.5907 0.5660 0.5722 0.5558 0.6178 0.6000 0.6407 0.5600 0.4152 0.4197 0.3405 0.4446 0.3640 0.3692 0.3394 0.4092 0.4241 0.4224 0.3728 0.3728 0.5537 0.5359 0.5226 0.5384 0.4527 0.4527 0.3959 0.4581 0.3949 0.3793 0.3716 0.4530 0.4261 0.4118 0.3722 0.3722 0.5518 0.5466 0.5667 0.5067 0.4276 0.4441 0.3912 0.4350 0.3632 0.3852 0.3687 0.4461 0.4033 0.4049 0.4842 0.4500 0.5541 0.5423 0.5520 0.5407 Table 9. F1-score on TRAC Facebook Code-mixed Hindi Dataset Classifier Countvector TF/IDF W2Vec Glove Fasttext pfastText doc2vecdmc doc2vecdbow NB LR KNN SVC DT SGD RF Ridge AdaB Perce. ANN Ensemble LSTM BLSTM CNN NCNN 0.5535 0.5855 0.3340 0.5556 0.5307 0.5533 0.5473 0.5780 0.5373 0.5213 0.5703 0.5700 0.6031 0.6134 0.1721 0.5862 0.5025 0.5922 0.5473 0.5850 0.5233 0.5598 0.5350 0.6087 0.372 0.464 0.425 0.373 0.388 0.393 0.440 0.381 0.479 0.364 0.5037 0.4067 0.590 0.527 0.566 0.573 0.2959 0.5457 0.5106 0.5186 0.4392 0.3670 0.5047 0.5092 0.5336 0.3763 0.5842 0.534 0.6021 0.5770 0.5950 0.5912 0.3176 0.5518 0.4909 0.5442 0.4288 0.4746 0.4788 0.5544 0.4913 0.4873 0.5190 0.5612 0.5916 0.5900 0.6081 0.5965 0.3459 0.3894 0.3768 0.3879 0.3485 0.3331 0.3512 0.3866 0.3751 0.2661 0.4091 0.4980 0.4736 0.4380 0.4038 0.4344 0.3485 0.4134 0.3477 0.4292 0.4214 0.3282 0.4440 0.4401 0.3001 0.5779 0.4998 0.4806 0.4629 0.3912 0.5374 0.5293 0.5342 0.4232 0.5455 0.5558 0.5649 0.5759 0.5515 0.5919 Table 10. F1-score on TRAC Twitter Code-mixed Hindi Dataset Classifier Countvector TF/IDF W2Vec Glove Fasttext p-fastText doc2vecdmc doc2vecdbow NB LR KNN SVC DT SGD RF Ridge AdaB Perce. ANN Ensemble LSTM BLSTM CNN NCNN 0.2970 0.3787 0.2527 0.3781 0.3685 0.3996 0.3585 0.3616 0.1886 0.3931 0.430 0.4400 0.2902 0.3724 0.2553 0.3886 0.3936 0.3993 0.3737 0.3872 0.1903 0.3868 0.44 0.4600 0.273 0.279 0.334 0.261 0.326 0.287 0.344 0.242 0.362 0.329 0.3552 0.3938 0.376 0.318 0.317 0.380 0.3359 0.3184 0.3381 0.3087 0.3475 0.2739 0.3449 0.3346 0.3261 0.2787 0.2399 0.3426 0.3667 0.3005 0.2669 0.3494 0.2897 0.3524 0.2917 0.3472 0.3473 0.3163 0.3288 0.3361 0.3441 0.3835 0.3593 0.3555 0.4600 0.4600 0.4992 0.4600 0.3270 0.2438 0.3051 0.2580 0.2988 0.2605 0.2981 0.2549 0.2614 0.2616 0.2419 0.3230 0.3205 0.2833 0.3299 0.2905 0.2988 0.2588 0.2988 0.2875 0.2933 0.2752 0.3132 0.3281 0.3215 0.2819 0.3704 0.2821 0.3572 0.2822 0.3286 0.2811 0.3256 0.2802 0.3163 0.4500 0.3840 0.2846 0.3323 0.3338 14Table 11. F1 score on TRAC Facebook English Test Dataset using Transfer Learning methods Transfer learning Model English Facebook Dataset Twitter Dataset 0.3699 0.4725 0.3854 0.4664 ELMO ULMfiT Table 12. weighted F1-score TRAC Test Dataset: comparison with peers System English Our system result DA-LD-hildesheim saroyehun EBSI-LIA-UNAM TakeLab taraka rama vista.ue na14 Hindi Facebook Dataset Twitter Dataset Facebook Dataset Twitter Dataset 0.6407 0.6178 0.6425 0.6315 0.5920 0.6008 0.5812 0.5920 0.5541 0.552 0.5920 0.5715 0.5651 0.5656 0.6008 0.5663 0.6081 0.6081 NA NA NA 0.6420 0.5951 0.6450 0.4992 0.4992 NA NA NA 0.40 0.4829 0.4853 figure 1. Heatmap on English Facebook Test Dataset Results. Table 13. Results Comparison with rest of team on fiRE 2018 IRMiDis Dataset. System p@100 R@1000 MAP@100 MAP nDCG @100 nDCG Our System MIDAS-semiauto MIDAS-1 FAST NU Run2 UEM DataMining iitbhu irlab2 0.4 0.9600 0.8800 0.7000 0.6800 0.3900 0.2002 0.1148 0.1292 0.0885 0.1427 0.0447 0.0129 0.0740 0.0581 0.0396 0.0378 0.0144 0.1471 0.1345 0.1329 0.0801 0.1178 0.0401 0.4021 0.6007 0.5649 0.5723 0.5332 0.3272 0.7492 0.6899 0.6835 0.6676 0.6396 0.6200 15figure 2. Heatmap on English Twitter Test Dataset Results. Table 14. Results Comparison with CNN model and Logistic Regression TRAC Facebook English Test Dataset Class NAG CAG OAG overall CNN model Logistic Regression P R Weighted F1 P R Weighted F1 0.86 0.28 0.42 0.70 0.64 0.46 0.61 0.61 0.73 0.35 0.50 0.64 0.83 0.23 0.46 0.68 0.60 0.54 0.39 0.56 0.70 0.32 0.42 0.60 #Posts 630 142 144 916 classifiers and ensemble of classifier with respect to weighted F 1 score on Facebook English corpus and substantially outperforms on Twitter English corpus. By and large similar results observed on code-mixed Hindi corpus as shown in table 9 and 10. Table 14 present the detailed comparative results of two classifiers: CNN model with fastText pre-trained vector and the logistic regression with TF/IDF weighting on TRAC Facebook English dataset. The CNN Model classify Facebook posts better than logistic regression at the individual class level and overall. It has been quite evident that posts belong to CAG class are hard to classify and Malmasi, et al. (2017) reported that the same observation. Table 15 show posts which are miss-classified by logistic regression however, CNN model correctly classified them into the CAG class. 7.1. Significance Test To support our claim drawn in the previous section, significance tests, like Wilcoxon signed-rank test and Student t-test were carried out by comparing Weighted F 1 score of each classifier for each text representation scheme with fastText pre-trained vector scheme. Table 16 and 17 summarizes the p-values of statistical significance tests on 16Table 15. sample post for the CAG class. no Post text Gold Label CNN LR 1 Mauni singh trying very hard to convince himself what is written in script... body language says it all Indian govt is all Abt giving money to Bangladesh on the terms of Bangladesh ll give that all projects to amabani n adani for thier benefits lol who cares Abt soldiers or India they r just puppets of thier owners feku or pappu When asked to speak in Parliament ran away. Speaks only in TV,radio or in election rally. Can we expect Another crying drama after Demonetisation disaster ? #cryBaby” CAG CAG NAG CAG CAG NAG CAG CAG NAG 2 3 Table 16. p-values of Significance test on F 1 -score on TRAC Facebook English Dataset Text Rep. scheme Count Vector TF/IDF Word2vec Glove fastText doc2vec-dmc doc2vec-dbow p-Word2vec P-Glove Facebook English Twitter English Wilcoxon T-test Wilcoxon T-test 0.001 0.0001 0.00002 0.00001 0.00003 0.000009 0.000009 0.00001 0.0002 0.12 0.1465 0.0001 0.000002 0.0004 0.0001 0.00003 0.0006 0.02 0.004 0.0061 0.69 0.01 0.12 0.0004 0.0004 0.02 0.08 0.016 0.0391 0.30 0.0009 0.08 0.000004 0.000006 0.01 0.30 English and Hindi Dataset respectively. In Wilcoxon signed-rank test, p-values of the results is less than 0.05 for Facebook English dataset and Twitter Hindi Dataset. However, On Twitter English dataset and Facebook Hindi Dataset, some of the pvalues are higher than 0.05. In student t-test, we get mixed bag results. By and large, our results are statistically significant. In the following subsection, we will try to answer all the research questions framed during the experiments were planned. 7.2. Best Text Representation scheme to model the text from Social web Text Representation is the primary task for to address any NLP task like Question/answering, classification etc. As dicusses in section 4, There are basically two text representing scheme:Bag-of-Word(BoW) with countvector, TF/IDF weighting and word embedding. Word2Vec (Mikolov et.al , 2013), Glove (Pennington et.al. , 2014), and fastText (Mikolov et al. , 2018), an extension of Word2vec are popular word embedding techniques. Table 17. p-values of Significance test on F 1 -score on TRAC code-mixed Hindi Test Dataset Text Rep. scheme Count Vector TF/IDF Word2vec Glove fastText doc2vec-dmc doc2vec-dbow Facebook Code-mixed Hindi Twitter Code-mixed Hindi Wilcoxon T-test Wilcoxon T-test 0.003 0.003 0.40 0.001 0.35 0.005 0.003 0.05 0.14 0.17 0.0005 0.15 0.00001 0.002 0.01 0.007 0.043 0.015 0.022 0.001 0.001 0.20 0.12 0.017 0.004 0.006 0.0008 0.002 17Results clearly show that models with fastText pre-trained vector outperform Glove pre-trained vector on Facebook test dataset as well as the Twitter test dataset. the main reason behind the outperformance of fastText over Glove and Word2vec is that The fastText consider each word as N-gram characters. A word vector for a word is computed from the sum of the n-gram characters. Glove and Word2vec consider each word as a single unit and provide a word vector for each word. Since Facebook users make a lot of mistakes in spelling, typos, fastText is more convenient than Glove (Majumder et al., 2018). from figure 1 and 2 shows that BoW is still effective text representation scheme for the standard machine learning classier which takes handcrafted feature and n-grams as inputs. Logistic Regression and Support Vector perform better than other classifiers in English as well as Hindi Dataset. Adaboost performs better than LR and SVC on Facebook English Dataset but substantially underperform them on rest of three Datsets. Our participation (Majumder et al., 2018) in TRAC competition (Kumar Ritesh et al., 2018) fiRE Information Retrieval from Microblogs during Disasters (Basu et al. , 2018) track where our team performed well and secured top position. 7.3. Transfer Learning Model vs Pre-trained Word Embedding Model Transfer learning is focused on storing knowledge gained while solving one problem and applying it to a different but related problem. On many occasion, NLP researchers face the problem of unavailability of sufficient labeled data to train the model. With the advent of new transfer learning method like ELMO (Peters et al. , 2018) and Universal language model fine-tuning for Text Classification (ULMfiT) (Howaard et al., 2018) attract interest among NLP Researchers. These models are trained or large text corpus. Howaard et al. (2018) claimed that these model can be fine-tuned on the task-specific corpus. We have used these transfer learning model on TRAC English dataset Kumar et al. (2018) and results are presented in table 11. one can observe that results are substantially lower than the results reported in Table 6, 7, and 8 where pre-trained word vectors are used to initialize the first layer of deep neural model and rest of the network is trained from scratch achieve better results than transfer learning model. Howaard et al. (2018) termed use of pre-trained vector as shallow representation. In these experiments, we trained different classifier models on Facebook posts. Table 7 10 shows the results on Twitter dataset Kumar et al. (2018). There is lexical difference between Facebook and Twitter posts. From the results shown in Table 7 8 and table 10, one can conclude that weighted F 1 score of standard machine learning classifiers are substantially lower in Twitter Dataset as compare to Facebook Dataset. While deep learning models perform better than machine learning classifiers for the Twitter Dataset. Thus, Deep learning models are more robust than machine learning classifier across diverse datasets. 7.4. Does Deeper Neural Net make Sense To answer this question, we designed first CNN model with one convolution layer and other CNN model with 3 convolution layer with different filters height. As we look at results shown in Table 6,7,8, and 9, one can conclude that by and large weighted F 1 score lower for CNN model with multiple convolution layer than CNN model with single convolution layer.</body>
  <conclusion>In this Paper, Multilingual Social media stream is studied with special kind of text features: Aggression and fact perspective. Exhaustive experiments are performed to benchmark the text representation scheme on machine learning classifiers and deep neural nets. From the results, we conclude that deep Neural model with pre-trained word embedding is the better choice than machine earning classifier and transfer learning model. Word embedding is the better text representative scheme than Bag-of-words for the deep neural models. In fact, performance can be improved with the help of fastText pre-trained vector. However, machine learning classifiers perform better in BoW with TF/IDF weighting than word embedding. We also concluded that higher drop out will help to counter model overfitting and improvise a standard evaluation metrics. CNN and LSTM are the better models for these datasets. On the English test corpus, we obtained a better weighted F1 score for NAG class and poor weighted F1 score for CAG class which supports the previous (Malmasi, et al. , 2017) findings. For the Facebook Hindi test corpus, the same seems not to be true. We obtained a better F1 score for CAG class than NAG class. It is also to be noted that the model leads to poor result on Twitter test data since the training corpus was created from Facebook. In such cases, deep neural models substantially outperform machine learning classifiers. Significance test confirms these claims with 95 % confidence interval in most the cases. Our work shows what kind of problems are moving into the center of attention for research in machine learning. Using deep learning models, there is great potential to solve some of these problems, yet still, the performance is far from perfect. Model transfer between problems and the application of derived knowledge in user interfaces are areas directions for future work.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media: Using deep neural networks, data augmentation, and pseudolabeling.Proceedings of the fist Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp.90–97 Arroyo-Fern´andez, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel and Carrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). Cyberbullying Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1.Proceedings of the first Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018), pp 140–149Basu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the fiRE 2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis). Proceedings of fiRE 2018 Forum for Information Retrieval Evaluation, Gujrat, India, December Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysisProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp.747–754 Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) Enriching word vectors with subword information, Transactions of the Association for Computational Linguistics, vol-5,pp.135–146, MIT Press Burnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision makingPolicy &amp; Internet, vol-7 number 2, pp.223–242, Wiley Online Library Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gon¸calves , Bruno and Menczer, filippo and Flammini, Alessandro (2011). Political polarization on twitter.,Icwsm, vol-133, pp.89–96 19 Conover, Michael D and Gon¸calves, Bruno and Ratkiewicz, Jacob and Flammini, Alessandro and Menczer, filippo (2011). Predicting the political alignment of twitter users, Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on, pp.192–199 Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar (2017). Automated Hate Speech Detection and the Problem of Oﬀensive Langemuage. Proceedings of ICWSM Deriu, Jan and Gonzenbach, Maurice and Uzdilli, Fatih and Lucchi, Aurelien and Luca, Valeria De and Jaggi, Martin (2016). Swisscheese at semeval-2016 task 4: Sentiment classification using an ensemble of convolutional neural networks with distant supervision. Proceedings of the 10th international workshop on semantic evaluation,pp.1124–1128 Hltcoe, J (2013).Semeval-2013 task 2: Sentiment analysis in Twitter,vol-312 Atlanta, Georgia, USA Howard, Jeremy &amp; Ruder, Sebastian 2018. Universal language model fine-tuning for text classification”,arXiv preprint arXiv:1801.06146, Harris, Zellig S (1954),Distributional structure Word,10, number=2-3,pp.146–162, 1954, Taylor &amp; Francis Kumar, Ritesh and Reganti, Aishwarya N. and Bhatia, Akshit and Maheshwari,Tushar (2018), Aggression-annotated Corpus of Hindi-English Code-mixed Data, Proceedings of the 11th Language Resources and Evaluation Conference (LREC), Miyazaki, Japan Kumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri Marcos (2018). Benchmarking Aggression Identification in Social Media, Proceedings of the first Workshop on Trolling, Aggression and Cyberbulling (TRAC), Santa Fe, USA Kwok, Irene and Wang, Yuzhou (2013),Locate the hate: Detecting Tweets Against Blacks ,Twenty-Seventh AAAI Conference on Artificial Intelligence Lau, Jey Han and Baldwin, Timothy (2016). An empirical evaluation of doc2vec with practical insights into document embedding generation. arXiv preprint arXiv:1607.05368 Le, Quoc and Mikolov, Tomas (2014) Distributed representations of sentences and documents.International Conference on Machine Learning, pp.1188–1196 Majumder, Prasenjit and Mandl, Thomas and Modha Sandip (2018)filtering Aggression from the Multilingual Social Media Feed Proceedings of the first Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp. 199–207 Malmasi, Shervin and Zampieri, Marcos (2017)Detecting Hate Speech in Social Media Proceedings of the International Conference Recent Advances in Natural Language Processing (RANLP), pp.467–472 Malmasi, Shervin and Zampieri, Marcos (2018).Challenges in Discriminating Profanity from Hate Speech Journal of Experimental &amp; Theoretical Artificial Intelligence pp.1–16, vol-30, issue-2,Taylor &amp; Francis Maynard, Diana and Funk, Adam (2011) Automatic detection of political opinionsin tweets, Extended Semantic Web Conference,pp. 88–99, Springer Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeﬀrey (2013). Eﬃcient estimation of word representations in vector space,arXiv preprint arXiv:1301.3781 Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand (2018). Advances in Pre-Training Distributed Word Representations,Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018) Modha, Sandip and Agrawal, Krati and Verma, Deepali and Majumder, Prasenjit and Mandalia, Chintak 2016. DAIICT at TREC RTS 2016: Live Push Notification and Email Digest.,TREC Mohammad, Saif M and Kiritchenko, Svetlana and Zhu, Xiaodan (2013). NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.arXiv preprint arXiv:1308.6242 Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov, Veselin (2016). SemEval-2016 task 4: Sentiment analysis in Twitter,Proceedings of the 10th international workshop on semantic evaluation (semeval-2016), pp. 1–18 Pennington, Jeﬀrey and Socher, Richard and Manning, Christopher Glove: Global vectors for 20 word representation Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp.1532–1543 Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke (2018). Deep contextualized word representations arXiv preprint arXiv:1802.05365 Razavi, Amir H and Inkpen, Diana and Uritsky, Sasha and Matwin, Stan (2010) Oﬀensive language detection using multi-level classification, Canadian Conference on Artificial Intelligence pp.16–27, Springer Rosenthal, Sara and Nakov, Preslav and Kiritchenko, Svetlana and Mohammad, Saif and Ritter, Alan and Stoyanov, Veselin (2015). Semeval-2015 task 10: Sentiment analysis in twitter Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pp.451–463 Rosenthal, Sara and Farra, Noura and Nakov, Preslav (2017). SemEval-2017 task 4: Sentiment analysis in Twitter Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp.502–518 Schmidt, Anna and Wiegand, Michael (2017).A Survey on Hate Speech Detection Using Natural Language Processing,Proceedings of the fifth International Workshop on Natural Language Processing for Social Media. Association for Computational Linguistics Valencia, Spain, pp.1–10, Severyn, Aliaksei and Moschitti, Alessandro (2015) Unitn: Training deep convolutional neural network for twitter sentiment classification, Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pp. 464–469 Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp G and Welpe, Isabell M (2010). Predicting elections with twitter: What 140 characters reveal about political sentiment.,Icwsm, vol-10, number-1, pp. 178–185 Warner, William and Hirschberg, Julia (2012) Detecting hate speech on the world wide web, Proceedings of the Second Workshop on Language in Social Media,pp 19–26,Association for Computational Linguistics Xu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy (2012). Learning from bullying traces in social media, Proceedings of the 2012 conference of the North American chapter of the association for computational linguistics: Human language technologies, pp.656–666, Association for Computational Linguistics Zhang, Ye and Wallace, Byron (2015). A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification,arXiv preprint arXiv:1510.03820Majumder, Prasenjit and Mitra, Mandar and Pal, Dipasree and Bandyopadhyay, Ayan and Maiti, Samaresh and Mitra, Sukanya and Sen, Aparajita and Pal, Sukomal.Text collections for fiRE, Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval,pp.699–700,ACM Majumdar, P and Mitra, Mandar and Parui, Swapan K and Bhattacharya, Initiative for indian language ir evaluation, The first International Workshop on Evaluating Information Access (EVIA) 21</biblio>


  <preamble>L18-1504.pdf</preamble>
  <titre>A New Annotated Portuguese/Spanish Corpus for the Multi-Sentence Compression Task</titre>
  <auteurs>
    <auteur>
      <name>Elvys linhares-pontes</name>
      <mail>elvys.linhares-pontes@univ-avignon.fr</mail>
      <afliation>CERI/LIA Université d’Avignon et des Pays de Vaucluse, Avignon, France</afliation>
    </auteur>
    <auteur>
      <name>Juan-manuel torres</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <afliation>CERI/LIA Université d’Avignon et des Pays de Vaucluse, Avignon, France ́École Polytechnique de Montréal, Montréal, Canada</afliation>
    </auteur>
    <auteur>
      <name>Stephane huet</name>
      <mail>stephane.huet@univ-avignon.fr</mail>
      <afliation>CERI/LIA Université d’Avignon et des Pays de Vaucluse, Avignon, Franc</afliation>
    </auteur>
    <auteur>
      <name>Andrea Carneiro linhares</name>
      <mail>andrea.linhares@ufc.br</mail>
      <afliation>Universidade Federal do Ceará, Sobral-CE, Brasil</afliation>
    </auteur>
  </auteurs>
  <abstract>Multi-sentence compression aims to generate a short and informative compression from several source sentences that deal with the same topic. In this work, we present a new corpus for the Multi-Sentence Compression (MSC) task in Portuguese and Spanish. We also provide on this corpus a comparison of two state-of-the-art MSC systems. Keywords: Annotated Corpus, Multi-Sentence Compression, Multilingual Corpus.</abstract>
  <introduction>Among the various applications of Natural Language Processing, Automatic Text Summarization (ATS) aims at summarizing one or more texts automatically. Summarization systems identify relevant data and create a summary from key information. The (Multi-)Sentence Compression task can be seen as a subproblem of ATS with the objective to generate a shorter, informative and correct sentence from source sentence(s). In many cases, state-of-the-art NLP systems are evaluated with experiments restrained to the English language, in part because there are a lot of available English resources for most NLP tasks. As regards Multi-Sentence Compression (MSC), the available resources are unfortunately limited; to our knowledge, only one dataset is freely available and it is conned to the French language (Boudin and Morin, 2013). In this work, we present a new annotated corpus in the Portuguese and Spanish languages for the MSC task. Using this corpus, we evaluate two state-of-the-art systems and show that the use of several languages leads to more mitigated results on the superiority of one system than the use of the French corpus alone. The remainder of this paper is organized as follows. In Section 2, we characterize MSC with respect to related tasks from the perspective of the available corpora. Section 3 describes the creation and the features of our corpus. In Section 4 we analyze the results achieved by state-of-theart methods using our dataset. nally, conclusions are set out in Section 5.</introduction>
  <body>Sentence Compression (SC) aims at producing a reduced grammatically correct sentence from a source sentence. SC can be used in the context of the abstractive summarization of documents, the generation of article titles or the simplication of complex sentences, using diverse methods (optimization, syntactic structure, deletion of words and/or generation of sentences). The corpora for SC can be divided in two categories: deletion-based and summarization-based SC. In the case of SC by deletion of words, sentences are compressed by removing irrelevant words (lippova et al., 2015; Ive and Yvon, 2016). Knight and Marcu (2002) developed a SC corpus by aligning abstracts and sentences extracted from the Ziff-Davis corpus, which is a collection of newspaper articles announcing computer products. Clarke and Lapata (2008) provided two manually created two-reference corpora for deletion-based compression. lippova and Altun (2013), and lippova et al. (2015) extracted and released deletion-based compressions by aligning news headlines to the rst sentences. nally, Ive and Yvon (2016) developed an English-French parallel corpus for the compression and simplication tasks. SC by generations of sentences analyzes a whole sentence and generates a new shorter sentence with the core information of the source sentence (Rush et al., 2015; Ganitkevitch et al., 2011; Cohn and Lapata, 2008; Toutanova et al., 2016). Ganitkevitch et al. (2011) created a corpus of compression paraphrases composed of parallel EnglishEnglish sentences obtained from multiple reference translations. Rush et al. (2015) produced compression pairs made up of the headline of each article and its rst sentence; they released their code to extract data from the annotated Gigaword (Graff et al., 2011). Cohn and Lapata (2008) and Toutanova et al. (2016) describe two manually created abstractive compression corpora that are publicly available. The dataset presented in Cohn and Lapata (2008) comprises a single-reference sentence pairs for abstractive summary, while the corpus developed by Toutanova et al. (2016) has multiple references for short paragraph compressions. Multi-Sentence Compression (MSC), also known as MultiSentence Fusion, is a variation of SC. MSC aims at analyzing a cluster of similar sentences to generate a new sentence, which is shorter than the average length of source sentences and has the key information of the cluster (Barzilay and McKeown, 2005; lippova, 2010). MSC enables summarization and question-answering systems to gener 3192 Characteristics #tokens #vocabulary (tokens) #sentences avg. sentence length (tokens) type-token ratio sentence similarity [0,1] Spanish Source Reference 3,694 30,588 881 4,390 160 800 38.2 23.1 43.4% 35.2% 0.47 0.64 French Portuguese Source Reference 1,425 17,998 533 2,438 80 544 33.1 17.8 67.9% 33.7% 0.51 0.59 Source Reference 2,362 20,224 636 2,867 120 618 33.0 19.7 50.1% 38.8% 0.46 0.67 Table 1: Statistics of the corpora. ate outputs combining fully formed sentences from one or several documents. Various corpora have been developed for MSC and are composed of clusters of similar sentences from different source news in English, French, Spanish or Vietnamese languages (Barzilay and McKeown, 2005; lippova, 2010; Boudin and Morin, 2013; Thadani and McKeown, 2013; Luong et al., 2015). lippova’s corpus as well as Boudin and Morin’s contain clusters of similar sentences, each cluster composed of at least 7 or 8 sentences, whereas the datasets introduced in (McKeown et al., 2010) and (Luong et al., 2015) have only a pair of source sentences per cluster. McKeown et al. (2010) collected 300 English sentence pairs taken from newswire clusters using Amazon’s Mechanical Turk. Likewise, the dataset built by Luong et al. (2015) contains 250 Vietnamese sentences divided into 115 groups of similar sentences with 2 sentences per group. Thadani and McKeown (2013) presented an English corpus with 1,858 clusters having between 2 and 4 sentences; this dataset was built using automatic methods from annotations made for the DUC1 and TAC2 evaluations. The corpora presented in (McKeown et al., 2010), (Boudin and Morin, 2013) and (Luong et al., 2015) are publicly available, but among these three datasets only the second one is more suited to multi-document summarization or question-answering tasks because the documents to analyze are usually composed of many similar sentences. 3. Dataset Description We introduce a novel annotated corpus collected from Portuguese and Spanish Google News.3 This corpus is composed of clusters of similar sentences along with reference compressions for each cluster. The data are described in the following subsections. Table 1 summarizes the characteristics of the corpus and Table 2 shows a small example of our Portuguese dataset. 3.1. Source Sentences In keeping with the methodology introduced by lippova (2010), we collected links from Google News in Spanish and Portuguese between July and September 2016. These links redirect international news sites in Spanish (La Jornada, Milenio, El Economista, BBC Mundo, El Colombiano, El Pa´ıs, CNN en espa˜nol, etc.) and in Portuguese 1http://duc.nist.gov 2http://www.nist.gov/tac 3The Spanish and Portuguese MSC datasets are freely available, under GPL license on the DOI website: http://dev. termwatch.es/˜fresa/CORPUS/MSF2/. (G1, Uol Not´ıcias, Estad˜ao, O Globo, etc.). Each cluster is composed of related sentences describing a specic event and was chosen among the rst sentence from different articles about Science, Sports, Economy, Health, Business, Technology, Accidents/Catastrophes, General Information and other subjects. During the collection period, sentences were gathered among news threads that had at least 8 different sources. The source sentences of each cluster were manually selected so that they best describe the news, while sentences dealing with less relevant information were discarded. Each source sentence is composed of at least 8 tokens and a verb. In order to ensure the variability of source sentences inside a cluster, we removed all duplicated sentences, by assuming that sentences were too similar when the cosine similarity4 computed from one-hot vectors was higher that 0.8. We used the TreeTagger system5 to tag the source sentences with Parts-of-Speech. 3.2. Reference Compressions Like in (lippova, 2010; Boudin and Morin, 2013), reference compressions are edited by human annotators, all native Portuguese or Spanish speakers, who analyzed the most relevant facts of a cluster and generated a condensed sentence of this cluster. We suggested that the annotators should use the same vocabulary and n-grams as the source sentences and only select the most relevant information about the topic. We also recommended that they should generate compressions that are shorter than the length average of the source sentences. The following sections provide details about the Portuguese and Spanish parts of the corpus and, as a matter of comparison, brieﬂy recalls the main characteristics of the French corpus built by Boudin and Morin. 3.2.1. Portuguese Dataset The Portuguese corpus is composed of 40 clusters. Each cluster has at least 10 similar sentences by topic and 2 reference compressions made by 2 human annotators. This corpus contains 17,998 tokens and has a vocabulary of 2,438 tokens. Source sentences have an average of 33.1 tokens per sentence with a standard deviation of 9.9 tokens. The Type-Token Ratio (TTR) indicates the reuse of tokens in the cluster and is dened by the number of unique tokens divided by the number of tokens in each cluster; the lower 4The cosine similarity between two vectors u and v associated with two sentences is dened by u·v ||u|| ||v|| in the [0,1] range. 5Website: http://www.cis.uni-muenchen.de/ ˜schmid/tools/TreeTagger/ 3193 Source sentences : A Tesla fez uma oferta de compra `a empresa de servic¸os de energia solar SolarCity por mais de 2300 milh˜oes de euros. A Tesla Motors , fabricante de carros el´etricos , anunciou aquisic¸˜ao da SolarCity por US$ 2,6 bilh˜oes . A fabricante de carros el´etricos e baterias Tesla Motors disse nesta segunda-feira ( 1 ) que chegou a um acordo com a SolarCity para comprar a instaladora de pain´eis solares por US$ 2,6 bilh˜oes , em um grande passo do bilion´ario Elon Musk para oferecer aos consumidores um neg´ocio totalmente especializado em energia limpa , informou a Reuters . Reference compressions : A Tesla Motors anunciou acordo para comprar a SolarCity por US$ 2,6 bilh˜oes . A fabricante Tesla Motors vai adquirir a instaladora de pain´eis solares da SolarCity . Table 2: Small example of our Portuguese dataset. the TTR, the greater the reuse of tokens in the cluster. The sentence similarity represents the average cosine similarity of the sentences in a cluster. Using these metrics, references have an average length of 17.8 tokens and a standard deviation of 1.5 tokens, while the Portuguese source corpus has a TTR of 33.7%. The Portuguese annotators generated the compressions with a TTR of 67.9% and a sentence similarity of 0.59. nally, the average compression ratio between the reference and source sentences is 54%. 3.2.2. Spanish Dataset The Spanish part is also composed of 40 clusters. It has 30,588 tokens and a vocabulary of 4,390 tokens. Each cluster has 20 similar sentences on the same topic and 4 reference compressions made by 4 human annotators. Source sentences have an average of 38.2 tokens per sentence with a standard deviation of 10.7 tokens and an average TTR of 35.2%. Reference compressions contain the same vocabulary as source sentences while keeping an average size of 23.1 tokens, a standard deviation of 2.4 tokens and a TTR of 43.4%. The sentence similarity between the compressions is 0.64. The average compression rate is 61%. 3.2.3. French Dataset We used in the following experiments the French corpus developed by Boudin and Morin (2013). This corpus also has 40 clusters composed of 618 sentences (33 tokens on average). The clusters are composed of 15 sentences on average and the TTR of the corpus is 38.8%. Reference compressions have a compression rate of 60%. 4. Experimental Evaluation We used our corpus to provide a more thorough evaluation of state-of-the-art approaches for MSC than the study on the French corpus alone. We tested on our dataset a simple baseline, as well as (lippova, 2010) and (Boudin and Morin, 2013) methods. lippova modeled clusters of similar sentences as Word Graphs based on the cohesion of tokens and their Part-of-Speech (PoS). Inspired by the good results of the lippova’s method, Boudin and Morin used the TextRank method as a re-rank method to analyze the sentences generated by lippova’s method in order to produce well punctuated and hopefully more informative compressions. The baseline system creates a Word Graph (WG) like lippova’s method, but this time all arcs have the same weight. Then, the system generates a compression represented by the shortest path in the WG that has at least 8 tokens. Algorithms were implemented using the Python programming language and the takahe6 library. 4.1. Automatic and Manual Metrics The most important features of MSC are informativeness and grammaticality. Informativeness is the percentage of the main information retained in the compression, while grammaticality analyzes whether a sentence is correct or not. References are assumed to contain the most important information. Thus we calculated informativeness scores based on the common information between the output of the MSC system and the references using ROUGE (Lin, 2004). In particular, we used the f-measure metrics ROUGE-1, ROUGE-2 and ROUGE-SU4. Like in Boudin and Morin (Boudin and Morin, 2013), ROUGE metrics are calculated using stop words removal and stemming.7 We also led a manual evaluation with 4 native speakers for each language. The native speakers of each language judged the compression in two aspects: informativeness and grammaticality. In the same way as (lippova, 2010; Boudin and Morin, 2013), the native speakers evaluated the grammaticality in a 3-point scale: 0 point for an ungrammatical compression, 1 point for compression with minor mistakes; and 2 points for a correct compression. The informativeness evaluation process is similar for grammaticality: 0 point if the compression is not related to the main topic, 1 point if the compression misses some relevant information and 2 points if the compression conveys the gist of the main event. 4.2. Results with Automatic Metrics Table 3 shows f-score ROUGE scores for the French, Portuguese and Spanish datasets.8 Boudin and Morin’s system generated better compressions with higher ROUGE scores than lippova’s and the baseline for all datasets. publications.html http://www.florianboudin.org/ 6Website: 7http://snowball.tartarus.org/ 8Although we used the same system and data as (Boudin and Morin, 2013) for the French corpus, we were not able to reproduce exactly their results. The ROUGE scores given in their article are close to ours for their system: 0.6568 (ROUGE-1), 0.4414 (ROUGE-2) and 0.4344 (ROUGE-SU4), but using lippova’s system we measured higher scores than them: 0.5744 (ROUGE1), 0.3921 (ROUGE-2) and 0.3700 (ROUGE-SU4). 3194 RG-1 0.3681 0.6384 0.6674 Spanish RG-2 0.0990 0.2983 0.2960 Portuguese RG-2 0.1273 0.2971 0.3029 French RG-2 RG-SU4 0.1758 0.1904 0.4423 0.4297 0.4672 0.4602 Method RG-SU4 0.0984 Baseline 0.2847 lippova (2010) 0.2801 Boudin and Morin (2013) Table 3: ROUGE f-scores measured on the French, Portuguese and Spanish datasets. The best ROUGE results are in bold. RG-SU4 0.1309 0.2938 0.2868 RG-1 0.3199 0.5388 0.5532 RG-1 0.2700 0.5004 0.5140 Table 4 provides statistics on the length and the compression ratio of the sentences generated by the systems. The baseline system output the shortest compressions, which translated into the worst ROUGE scores. For the three tested datasets, lippova’s method generated shorter compressions with a smaller standard deviation than Boudin and Morin’s system. Let us note that for this last system the lengths of the outputs are less regular across the three languages. The Portuguese and Spanish languages derive from Latin and are closely related languages. However, they differ in many details of their grammar and lexicon. Moreover, the datasets produced for the three languages are unlike according to several features. rst, our corpus contains a smaller (Portuguese corpus) and a larger (Spanish corpus) dataset in terms of sentences than the original French corpus. Besides, the compression rates of the three datasets (see Section 3.) indicates that the Portuguese source sentences have more irrelevant tokens. The sentence similarity (Table 1, last line) describes the variability of sentences in the source sentences and in the references, and reﬂects here that the sentences are slightly more diverse for the Portuguese corpus. It can be noticed that the references are more similar too each other than source sentences since they only retain the main information. nally, the French corpus has a TTR of 38.8% whereas the Portuguese and Spanish datasets have TTRs of 33.7% and 35.2%, respectively. The baseline system generated the shortest compression because all arcs of the WG have the same weights. However, this system analyzes neither the grammaticality nor the most used n-grams in the clusters. Consequently, the baseline system generated compressions with the worst ROUGE scores. 4.3. Human Evaluation ROUGE only analyzes the overlapping between the candidate compression and the references. Since this analysis is not reliable enough, we led a further manual evaluation to study the informativeness and grammaticality of compressions, as described in Section 4.1.. Given the poor results of the baseline with ROUGE, we only analyzed the lippova’s and Boudin and Morin’s methods (Table 5). We measured inter-rater agreement on the judgments we collected, obtaining values of Fleiss’ kappa of 0.418, of 0.305 and 0.364 for French, Portuguese and Spanish respectively. These results show that human evaluation is rather subjective. Questioning evaluators on how they proceed to rate sentences reveals that they often made their choice by comparing outputs for a given cluster. As the differences of the grammaticality and the informativeness scores for the methods are not statistically signicant, we move our investigation on the average and standard deviation of the results. Both methods generated compressions of good quality (scores higher than 1) for all datasets, especially for the French and the Portuguese parts where scores above 1.5 for grammaticality and above 1.2 for informativeness were obtained. lippova’s method generated more correct compressions (except for the Portuguese corpus where both methods obtained almost the same results), which shows that the re-ranking step tends to moderately deteriorate grammaticality. By contrast, Boudin and Morin’s method consistently improves informativeness, which validates the interest of integrating the analysis of key phrases inside candidate compressions. This reranking method combines the cohesion score of lippova and the relevance of key phrases9 to generate more informative compression. This method selects the path of Word Graph that has relevant key phrases even if this path has a lower cohesion quality. All in all, Boudin and Morin’s method generated more informative but also longer compressions than lippova’s, CR showing a relative increase of 18% between both systems (Table 4).</body>
  <conclusion>Multi-Sentence Compression aims to generate a short informative text summary from several sentences with related and redundant information. This task can be used in the domain of multi-document summarization or question answering to provide more informative and concise texts. In this paper, we presented a new annotated corpus in two languages that extends the French data made available in (Boudin and Morin, 2013). We also compared two stateof-the art systems on this new dataset. We hope this corpus will help the NLP community to develop and validate multi-language methods for multi-sentence compression. In order to extend the multi-language resources to more diverse languages, we plan to create a similar MSC dataset for Arabic. We also want to use our corpus to test other competitive MSC systems, such as the one based on integer linear programming we introduced in (Linhares Pontes et al., 2016).</conclusion>
  <discussion>N/A</discussion>
  <biblio>Barzilay, R. and McKeown, K. R. (2005). Sentence fusionfor multidocument news summarization. ComputationalLinguistics, 31(3):297–328, September.Boudin, F. and Morin, E. (2013). Keyphrase extractionfor N-best reranking in multi-sentence compression. InNAACL, pages 298–305.Clarke, J. and Lapata, M. (2008). Global inference forsentence compression: An integer linear programmingapproach. Journal of Articial Intelligence Research(JAIR, 31:399–429.Cohn, T. and Lapata, M. (2008). Sentence compressionbeyond word deletion. In COLING, pages 137–144.lippova, K. and Altun, Y. (2013). Overcoming the lack ofparallel data in sentence compression. In EMNLP, pages1481–1491.lippova, K., Alfonseca, E., Colmenares, C. A., Kaiser,L., and Vinyals, O. (2015). Sentence compression bydeletion with LSTMs. In EMNLP, pages 360–368.lippova, K. (2010). Multi-sentence compression: nd-ing shortest paths in word graphs. In COLING, pages322–330.Ganitkevitch, J., Callison-Burch, C., Napoles, C., andDurme, B. V. (2011). Learning sentential paraphrasesfrom bilingual parallel corpora for text-to-text genera-tion. In EMNLP, pages 1168–1179.Ive, J. and Yvon, F. (2016). Parallel sentence compression.In COLING, Technical Papers, page 1503–1513.Knight, K. and Marcu, D. (2002). Summarization beyondsentence extraction: A probabilistic approach to sentencecompression. Articial Intelligence, 139(1):91–107.Lin, C.-Y.(2004). ROUGE: A Package for AutomaticEvaluation of Summaries. In Workshop Text Summariza-tion Branches Out (ACL’04), pages 74–81.Linhares Pontes, E., Gouveia da Silva, T., Linhares,A. C., Torres-Moreno, J.-M., and Huet, S.(2016).M´etodos de otimizac¸˜ao combinat´oria aplicados ao prob-In Anais do XLVIIIlema de compress˜ao multifrases.Simp´osio Brasileiro de Pesquisa Operacional (SBPO),pages 2278–2289.Luong, A. V., Tran, N. T., Ung, V. G., and Nghiem, M. Q.(2015). Word graph-based multi-sentence compression: Re-ranking candidates using frequent words. In Sev-enth International Conference on Knowledge and Sys-tems Engineering (KSE), pages 55–60. McKeown, K., Rosenthal, S., Thadani, K., and Moore, C.(2010). Time-efcient creation of an accurate sentence fusion corpus. In HLT-NAACL, pages 317–320. Rush, A. M., Chopra, S., and Weston, J. (2015). A neural attention model for abstractive sentence summarization. In EMNLP, pages 379–389. Thadani, K. and McKeown, K. (2013). Supervised sen-tence fusion with single-stage inference. In Sixth Inter-national Joint Conference on Natural Language Process-ing, IJCNLP, pages 1410–1418. Toutanova, K., Brockett, C., Tran, K. M., and Amershi,S. (2016). A dataset and evaluation metrics for abstrac-tive compression of sentences and short paragraphs. In EMNLP, pages 340–350. Boudin, Florian and Morin, Emmanuel. (2013). Keyphrase Extraction for N-best Reranking in Multi-Sentence Compression. NAACL (2013). Available on https://github.com/boudinfl/lina-msc. Graff, David and Cieri, Christopher and Kong, Junbo and Chen, Ke and Maeda, Kazuaki. (2011). English Gi-gaword. Linguistic Data Consortium, 5th, ISLRN 911-942-430-413-0.</biblio>


  <preamble>On_the_Morality_of_Artificial_Intelligence.pdf</preamble>
  <titre>On the Morality of Artiﬁcial Intelligence</titre>
  <auteurs>
    <auteur>
      <name>Alexandra Luccioni</name>
      <mail>N/A</mail>
      <affiliation>Université de Montréal, Mila</affiliation>
    </auteur>
    <auteur>
      <name>Yoshua Bengio</name>
      <mail>N/A</mail>
      <affiliation>Université de Montréal, Mila</affiliation>
    </auteur>
  </auteurs>
  <abstract>Much of the existing research on the social and ethical impact of Artificial Intelligence has been focused on defining ethical principles and guidelines surrounding Machine Learning (ML) and other Artificial Intelligence (AI) algorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for helping define the appropriate social norms of AI, we believe that it is equally important to discuss both the potential and risks of ML and to inspire the community to use ML for beneficial objectives. In the present article, which is specifically aimed at ML practitioners, we thus focus more on the latter, carrying out an overview of existing high-level ethical frameworks and guidelines, but above all proposing both conceptual and practical principles and guidelines for ML research and deployment, insisting on concrete actions that can be taken by practitioners to pursue a more ethical and moral practice of ML aimed at using AI for social good.</abstract>
  <introduction>N/A</introduction>
  <body>Progress in ML in the last decade has been extraordinary and has rekindled the notion that AI systems could eventually reach human levels of performance, which was abandoned for several decades. Even if we are still currently far from this achievement, technological progress in ML has passed a threshold which enables it to have a huge economic impact, estimated to be close to 16 trillion US dollars by 2030 [Szczepański, 2019]. This contrasts with the first few decades of ML progress, when researchers had the luxury of focusing purely on the fundamental aspects of their work, not worrying too much about its potential societal impacts – an object recognition algorithm could be tested on a common dataset like MNIST [LeCun et al., 1998] or ImageNet [Deng et al., 2009], and an objective performance metric would be obtained in order to measure progress, without having to think about the messiness and complexity of deployment and social impact. Something crucial has changed in recent years, as algorithms initially developed in the lab are increasingly being improved and deployed in society, in real-world applications such as healthcare, transportation and industrial production with real-life consequences, and we are likely seeing just the tip of the iceberg in terms of social impact. Along the way, this deployment in society has forced the realization that these algorithms have social impacts which could be positive or negative. For example, we have realized that biases hidden in data and algorithms could lead to more discrimination, in the simplest case simply because of the data imbalance: facial recognition algorithms have been found to underperform on gender and racial minorities [Buolamwini and Gebru, 2018]. Furthermore, above and beyond hidden biases, given the high impact potential of ML research, the question stands of whether practitioners are acting with the best interests of humanity and society in mind when developing their tools and applications. As ML researchers and engineers, we believe that we have a shared responsibility to consider both ethics and moral values when we choose what we work on, for what organization, and whether the products we contribute to directly or indirectly will be beneficial to humanity or more likely to end up hurting more than helping. Unfortunately, very few of us have been trained to think about these questions. Instead, most of us have focused from a very young age on mathematics and computer science and not so much on philosophy and other humanities. A good step towards learning about these issues is to consult the documents proposing ethical guidelines for AI, which we will cover in Section 2. Furthermore, in order 1to offer a guiding direction for such debates and soul-searching within the scope of ML, we propose the following self-directed questions: 1. How is the technology that I am working on going to be used? Who will beneﬁt or suffer from it? 3. How much and what social impact will it have? 4. How does my job ﬁt with my values? We are conscious that the questions listed above are subjective and the answers will depend highly on the values and ethics of the individual answering them. Nonetheless, we hope that work on some applications, such as the design and deployment of lethal autonomous weapons and automatic surveillance, will clearly be seen to contradict fundamental rights and dignity, as deﬁned in, among other places, the UN Declaration of Human Rights [1948]. Other applications of ML, such as those increasing the efﬁciency of advertising or beating the stock market, are less clear-cut in their moral value, and merit informed debate and discussion within the scientiﬁc community and society at large. As some of us become more conscious of the potential or deﬁnite social impact of ML, we have the opportunity, if not the duty, to make our voices heard. A good example of this is a recent letter signed by numerous scientists calling for an international treaty banning lethal autonomous weapon systems, e.g., killer drones which can decide to shoot at a person without human involvement, which would make it possible to take the broad social, moral and psychological context into account and potentially decide to abort the mission (for instance, when the target is in a school or at a family dinner surrounded by women and children). Finally, while the legal frameworks to oversee and limit research and development violating these principles are often and unfortunately updated in a reactive rather than a proactive manner, we believe that we should not wait until all of the dots between ML and ethics are formally connected by legislation and regulation. We believe that we have a responsibility to educate ourselves, to think ahead about potential consequences, to use our internal moral compasses and to consciously choose the direction of the research or engineering that we practice. This is important because we believe that we are faced with a wisdom race: as technology becomes more powerful, its impact can be proportionally greater, either positively or negatively. To curb the negative impact, we need to become wiser individually (as reﬂected in our personal decisions) and collectively (through social norms, laws and regulations). Unfortunately, technological progress in AI has accelerated faster than the current rate of progress of personal and social wisdom, ultimately making it possible for unwise humans or organizations, even those with good intentions and acting legally, to have large-scale, major destructive effects. This is comparable to a world in which nuclear bombs (i.e. very powerful technology) were accessible and usable by children (i.e., persons with insufﬁcient maturity and wisdom), which could easily result in global nuclear war. This highlights the importance of the discussions still to be had by large numbers of ML practitioners about ethics and social impact, as well as the safeguards that need to be put in place to protect especially the most vulnerable members of our society. We will discuss some of the most advanced efforts to introduce these safeguards in the next section, followed by some examples of socially beneﬁcial applications of ML. 2 Ethics and AI Existing Initiatives In recent years, there have been numerous initiatives which have taken one of two major approaches to fostering the ethical practice of AI: (1) Proposing principles guiding the socially responsible development of AI or (2) Raising concerns about the social impact of AI. We will describe both approaches in the current section, as well as giving examples of notable initiatives and projects which have adopted either of the approaches.1 1For a more complete overview of different global ML ethics initiatives, see a recent review in Jobin et al. [2019] 2 2.1 Deﬁning Principles for Practicing AI Responsibly The topic of ethical research and practice in technology has been gaining momentum in different corners of the computing community in recent years, and the various initiatives that have been proposed are indicative of the interest and the concern that many members share. For instance, in the United States, the Association for Computing Machinery (ACM) has proposed a Code of Ethics and Professional Conduct, to be followed by all members of the association and to guide them in their usage of computer science [Gotterbarn et al., 2018]. A similar initiative was undertaken by the Royal Statistical Society (RSS) in the United Kingdom, which has created a practical guide for practitioners regarding the ethical use of mathematics [RSS, 2019]. In the present section, we will address the two most relevant and extensive initiatives to establish ethical guidelines for AI research and practice: the Montreal Declaration for Responsible Development of AI and the IEEE report for Ethically Aligned Design. 2.1.1 The Montreal Declaration for a Responsible Development of Artiﬁcial Intelligence One of the most notable approaches to establishing guidelines for AI deployment is the Montreal Declaration for a Responsible Development of Artiﬁcial Intelligence, developed in 2017 and revised in 2018 based on public feedback2. It was elaborated under the premise that given the assumption that since AI will eventually affect all sectors of society, it requires principles to guide its development to ensure its adherence to human values and social progress. The resulting Declaration has ten principles, ranging from protection of privacy to equal representation, with some principles touching responsibility and ethics directly; for instance, the principle of Prudence stipulates that “Every person involved in AI development must exercise caution by anticipating, as far as possible, the adverse consequences of AIS [Artiﬁcial Intelligent Systems] use and by taking the appropriate measures to avoid them.” These principles were deﬁned after extensive debates and dialogue between both specialists and non-specialists from different domains and parts of the world to ensure representability and cohesion. The overall aim of the declaration was to spark public debate and to encourage a progressive and inclusive orientation to the development of AI. However, the Montreal declaration goes further than theoretical ethical principles, proposing recommendations to accomplish an ethical digital transition that includes all of the different levels of society, from researchers to policy-makers. For instance, it includes a proposition for auditing and validating the use of AIS using concrete frameworks and certiﬁcations in order to prevent biases and discrimination. Speciﬁc steps were also proposed for ensuring the protection of democracy and reducing the environmental footprint of AI, all within the framework of a democratic and citizen-led process. This is important given that the effects of AI will permeate all levels of society, from the programmers and engineers who write the code, to the leaders who will legislate it, and the businesses who will make products with it that will be used by all. The process of creation of the Montreal declaration was consequently the keystone to building a way of including all of these different stakeholders in the elaboration of an ethical AI, and paves the way for subsequent work on the topic. 2.1.2 IEEE Ethically Aligned Design A more recent effort, initiated by the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems, carried out an in-depth study on the issue of the ethics surrounding the design of AI systems [IEEE, 2017]. In particular, aspects that are relevant to the topics covered in the present paper include: the usage of A/IS [autonomous and intelligent systems] in service to sustainable development for all, and more specifically for the attainment of the United Nations Sustainable Development Goals (SDGs) [UNHCR, 2017]. The authors of the study speciﬁcally underline the potential of AI to contribute to resolving some of the world’s most urgent problems, such as climate change and poverty, given the necessary will and orientation towards these problems. Furthermore, they highlight the fact that despite their great potential, current AI deployment and development is currently not aligned with these goals and impacts [IEEE, 2017, p. 144], which is unsettling given the myriad of ML project and initiatives worldwide. The IEEE report also lays down principles to guide “the ethical and values-based design, development, and implementation of autonomous and intelligent systems”, many of which are similar to those deﬁned 2 https://www.montrealdeclaration-responsibleai.com/ 3 by the Montreal Declaration: respect of human rights, data agency, transparency, accountability, etc. They go further in proposing that “A/IS creators shall adopt increased human well-being as a primary success criterion for development” instead of focusing on isolated metrics such as accuracy, and, from a deployment perspective, offering alternative metrics to quantify meaningful progress, for instance by evaluating social, economic and environmental factors instead of proﬁt and other common success metrics. The report also includes propositions for policymakers, legislators and other stakeholders from the extended AI community and, as such, represents the most extensive effort of establishing ethical boundaries and guidelines for AI research to date. In a recent survey of the various global ethics guidelines proposed around AI, the authors observed that despite a conceptual overlap between the many existing guidelines, including the two mentioned above, there are major differences regarding how the principles are interpreted [Jobin et al., 2019]. This underlines the complexity and nuance of applying theoretical, philosophical principles in practice, and raises questions such as: what aspects of the AI research and deployment pipeline do ethics principles affect? how would it be possible to resolve conﬂicts between, for instance, fairness and sustainability (i.e. training an algorithm longer and with more data thus potentially leading to more greenhouse gas emissions to ensure that it is not discriminatory and covers all demographic groups equally well)? And, above all, how is it possible to translate ethical principles into a programming language? In any case, the bridge between theory and practice has yet to be built and there are different ways in which that can happen. This underlines the necessity of involving actors from different levels of the AI ecosystem (and neighboring ones) in order to ensure that experts in policy-making work in tandem with experts in coding and engineering to create tools and frameworks that are coherent and usable by all. Identifying Ethical Concerns of AI Applications 2.2 There are several types of ethical concerns regarding AI applications and, in this paper, we will focus more concretely on bias leading to potential discrimination. While it is true that on the one hand, AI-infused technology such as computer vision can enhance public security, for instance by identifying crime in realtime based on CCTV cameras, but the trade-off is that can also be abused to track individuals and to establish a surveillance state where privacy is greatly threatened by those who control the technology. On the military side, similar technology can be used to design autonomous drones which use computer vision to identity their target, representing a grave threat to global security and democracy due to the lack of human oversight. In addition to the security risk, such weapons would be moral and legal hazard: AI technology is not yet capable to comprehend and represent the social and psychological context in which such a targeted attack could take place in a manner that is coherent with international laws regarding war as well as with human morality. Unfortunately, the most common argument brought in favour of developing lethal autonomous weapons is that they are needed as a precautionary measure (i.e. since other countries are undeniably working on them, each country needs to do the same). In reality, the weapons needed to defend against killer drones would be very different from the drones themselves, and do not need to be lethal autonomous weapons since they would be designed to destroy weapons rather than to target people, similar to the Iron Dome used by Israel. Another common argument is that an international treaty would be useless since some countries will refuse to sign it. But we have seen in the past that even when major powers do not sign a treaty (such as the one on anti-personnel mines, signed by 133 countries, excluding the US, in 1997), the treaty can still be used to create a moral stigma, as well as a decline in demand; in the case of anti-personnel mines, the result has been that U.S. companies have stopped building them, even though their government never signed the treaty. Another ﬂawed argument is that regulating lethal autonomous weapons could threaten the innovation in AI, whereas in fact AI has been developed very successfully in a civilian setting (mostly in academia and major technology companies) and its continued development does not require neither data nor engineering which would come from AI military development. Another potential threat to democracy stemming from AI could come not simply from the increased ability to monitor and to target individuals, but also from the more subtle power to inﬂuence them, e.g. via AI-driven advertising, automated online trolls and other psychological manipulations via the internet and social media. The recent use of AI to inﬂuence political campaigns such as the 2016 US election or Brexit is just the beginning of what can be done when machines learn how to “press our buttons” in a personalized way. This is due to the fact that micro-targeting makes it possible for ads to be truly bespoke depending 4 on your political views, network of friends and personal history. While we may not mind being inﬂuenced when it comes to choosing a brand of soft drinks, when the proﬁt or power motives of a corporation or political organization go against our individual and collective interests, it becomes important to establish social norms, laws and regulations to protect us from such psychological manipulation. But where should the line be drawn between, for example, manipulation and education? These are difﬁcult questions but there are clues which can be used (like whether the organization that stands to proﬁt is paying for the advertisement or social network inﬂuence), so human judgement remains key for judging the ethical aspect, e.g. in balancing different values (like autonomy vs well-being, when considering an ad campaign against cigarettes, for example). In the case of advertising, what is interesting is that in addition to the moral hazard associated with psychological manipulation, it is not even clear that advertising is beneﬁcial to society from a purely economic perspective, as it tends to favour established brands and thus slow down innovation. Closely related to the political misuse and manipulation with AI is also increasing concern about AIgenerated false images, videos and news. Thanks to rapid progress in generative neural networks such as the GANs [Goodfellow et al., 2014], it is becoming possible to synthesize images and sounds in a controlled way, e.g., using “deep fakes” for making a video of a president declaring war, or with the face of a celebrity seamlessly integrated on the body and behavior of a pornography actor. Other commonly discussed concerns of AI deployment include the effect on the job market [Perisic, 2018], which means that governments and communities must prepare, e.g. by adapting the education system and the social safety net, which can take decades, as well as the potential concentration of power which it may lead to in speciﬁc individuals, corporations and countries, and the bias and discrimination it may contribute to increase, as we discuss next. 2.2.1 Identifying and Mitigating Bias In recent years, we have been confronted numerous times with the fact that biased algorithmic systems can perpetuate injustice and discrimination, whether we are aware of it or not. There are many different ways that this kind of bias can creep into algorithms: it can be from the data itself, or the implicit bias that the creator programmed into the system, and even the way the problem is framed3. Therefore, in order to ensure that the models that we develop and the systems that they are later used in are as fair and ethical as possible, there are steps to take to identify bias and to reduce it as much as possible. Numerical Bias A major challenge in designing ML systems is understanding how they work during training and deployment, and what factors and features they use to make decisions. However, diagnosing the presence of bias in these systems is not a straightforward task, since it is not always obvious during a model’s construction what the downstream impacts of design choices may be; therefore, upstream efforts are needed to reduce this risk as much as possible. To this end, there have been several proposals to help practitioners identify and mitigate bias in ML models, some of which we will describe in the current section. More concretely, exploring, analyzing and visualizing the data used for training a model is a key part of the ML process. But it is not straightforward to identify bias simply by looking at the data; often, more in-depth probing is needed to ﬁgure out what features and implicit information is present and, once a model is developed, how this will inﬂuence the model’s behavior. For instance, it was recently found that the COMPAS system, a criminal risk assessment tool developed widely used in the United States, is often biased with respect to race [Angwin et al., 2016]. Whereas the bias in the COMPAS system was identiﬁed after its deployment, once the data was made public, this bias is an aspect of the model that should have been identiﬁed much earlier, during development and certainly before deployment. Similarly, off-the-shelf facial recognition technology used by police forces has been shown to perform much worse on racial and gender minorities, with a difference of up to 34.4% in error rate between lighter-skinned males and darker-skinned females, mostly due to the lack of reliable training data [Buolamwini and Gebru, 2018]. To address these types of issues, several approaches exist: for instance, researchers have recently released a tool called ‘What-If’, an open-source application that lets practitioners not only visualize their data, but also test the performance of their ML model in hypothetical situations, for instance modifying 3For a more hands-on presentation of bias and fairness in AI, we suggest Google’s Online course designed speciﬁcally for ML practitioners 5 some characteristics of data points and analyzing subsequent model behavior, by measuring fairness metrics such as Equal Opportunity and Demographic Parity [Wexler et al., 2019]. Other approaches address bias by changing the training procedure or the structure of ML models themselves, for instance by transforming the raw data in a space in which discriminatory information cannot be found [Zemel et al., 2013] or using a variational autoencoder to learn the latent structure from the dataset and using this structure to re-weight the importance of speciﬁc data points during model training [Ribeiro et al., 2016]. Whatever the approach chosen, using these kinds of tools during ML model development and deployment can change the life of individual people, who could go from unfairly spending decades in prison to having the chance of a better life – an immensely important difference when multiplied by the thousands of people whose lives can be affected by the deployment of these tools. This multiplication of bias is especially important to consider since ML is being used more and more, and therefore even edge cases and small minorities can be ampliﬁed in real-world applications. Textual Bias Bias is not always in numbers, it can also manifest itself in the words that we use to describe the world around us. For instance, in 2018, Reuters reported that Amazon was forced to decommission an ML-powered recruiting engine when it was discovered that it penalized any mention of female-related vocabulary, including applicants who attended all-women colleges [Dastin, 2018]. This is not surprising given the gender disparity that exists in the technology sector and since the data used to develop this tool was comprised of resumes submitted (and accepted) to Amazon over a 10-year period. It is nonetheless disturbing in terms of algorithmic fairness, especially if algorithms such as this one make ﬁltering or hiring decisions that can ultimately affect an entire gender’s lives and careers. This can potentially create a negative feedback loop, as such a system would reduce the number of female workers and thus the number of positive role models for girls interested in technology. A similar type of gender bias was also found in pretrained word embedding models, which were found to exhibit gender stereotypes in terms of higher cosine similarity between, for instance, ‘woman’ and ‘homemaker’ or ‘receptionist’ as opposed to ‘woman’ and ‘doctor’ or ‘lawyer’, notably due to these biases existing in the corpus that they were trained on, which consisted of mainstream news articles [Bolukbasi et al., 2016]. In order to reduce and eventually remove gender bias in written text, researchers have proposed approaches such as identifying the gender subspace of vectors and adjusting the dimensions in a way that either neutralizes or entirely removes gender bias [Bolukbasi et al., 2016]. Others have deﬁned a formal gender bias taxonomy in order to capture gender bias and to train ML models to later identify this bias in texts [Hitti et al., 2019]. Debiasing the computational representation of language, notably word embedding models, is especially important because of the extent of their usage; pretrained embedding models trained on corpora such as Google News and the Common Crawl are used in a variety of applications and systems, and can therefore continue perpetuating gender bias in downstream usages in Natural Language Processing (NLP) applications such as dialogue systems. This is a challenge given the complex and sub-symbolic nature of modern NLP, which makes it difﬁcult to analyze speciﬁc features and aspects of data and identify latent connections and bias between words and concepts. Therefore, more work is needed to explore and analyse these issues, which constitutes an interesting research direction in itself, and one that is important to pursue and to integrate into mainstream ML research. Despite the research initiatives described above to carve appropriate social norms about AI, there remains a noticeable gap between the recommendations they make and ways to ensure that these are respected. Legislation of AI is still catching up to the progress made in research and practice, and there have not yet been any country-level laws governing AI research speciﬁcally. However, there have been, on the one hand, more high-level legislative frameworks such as the European Union (EU) General Data Protection Regulation (GDPR), which aims to ensure data privacy and protection and, on the other hand, more local initiatives such as San Francisco’s Facial Recognition Software Ban. Nonetheless, more complete legal frameworks are needed to control nefarious use of AI and to ensure that the principles deﬁned in theory are applied and enforced in practice. 6 3 AI for Good Initiatives Whereas the proﬁt motive is the main driver behind much of the commercial deployment of AI today, there are nonetheless many projects going on in academia, government organizations, civil society and industry labs motivated by more noble objectives, often called AI for Social Good (AISG) projects. In addition to the speciﬁc projects being undertaken in areas such as healthcare, education or the environment, it is interesting to highlight higher-level efforts which aim to foster and facilitate these projects. For example, the AI Commons projec aims to construct a hub where different kinds of actors can connect and collaborate on AISG projects, e.g., ML graduate students or engineers, problem owners in NGOs or local governments, philanthropy organizations, or startups which could deploy the ML solutions. Their interaction is to be facilitated by online tools and datasets as well as a standardized description of the status, progress and expected impact of each project. We hope that initiatives like this will help solidify and amplify the impact of AISG; in the meantime, there are also many profoundly positive uses of AI that are emerging and we would like to highlight and applaud such efforts in the present section. 3.1 AI in Healthcare Achieving universal health coverage is one of the 17 UN Sustainable Development Goals [UNHCR, 2017] and although major progress has been made in numerous domains, such as maternal health as well as HIV/AIDS reduction, there are still many problems that are far from being solved. While ML is not a cureall, there are many challenges that it can help with such as personalized medicine, diagnosis of medical imagery, and improved drug discovery [Ghassemi et al., 2018]. ML in the health sector is in fact a thriving domain of research, with its own workshops at major ML conferences and research published in major medical journals read by practitioners worldwide. In the last ﬁve years alone, groundbreaking work has been done in improving the diagnosis of diabetic retinopathy from a single visit [Arcadu et al., 2019], detecting breast cancer in lymph nodes [Golden, 2017] and large-scale discovery of diseases based on health records [Pivovarov et al., 2015]. There is also an increasing number of startups and companies working in the space, either by commercializing research done in academia or by developing products speciﬁcally catered to the medical sector, with the most advanced applications harnessing the power of deep learning for analyzing and classifying medical imagery. Despite the many exciting advances that are being made, there are many hurdles in ML research in healthcare, starting from data privacy and control (who owns the data? Can patients share their own data, or should the process be centralized? How to ﬁnd the right balance between privacy and the lives which will be saved by applying ML on the aggregated health records from many different sources?), to the manner in which medical data should be processed (Should it contain information such as race and postal/zip code, which can impact diagnoses, be included in electronic heath records, or does that open the door to discrimination and bias?) and how should such systems be deployed (human-in-the-loop or fully automated?)4. There are also often questions of responsibility and interpretability that arise, given the high stakes of deploying ML systems in situations of life and death. In order to make meaningful progress in this sector, it is therefore important to continue existing research on fair and ethical usage of ML in healthcare [Wiens et al., 2019] and to ensure that Hippocratic principles are a solid part of the research and development process, as well as working with stakeholders of the domain (e.g. radiologists, clinicians, patient organizations and hospital administrators) to propose solutions to the hurdles proposed above. 3.2 AI for Education The promise of using adaptive intelligent systems and agents for education has been around since the 1960s [Suppes and Morningstar, 1969], but access to personalized digital education tools has yet to become a reality in most countries, especially in the developing world, where it could have the most impact to democratize education and knowledge [Nye, 2015]. In recent years, given the increasing global shortage of qualiﬁed teachers along with the increasing number of students, the issue of access to education has become a global one, a fact highlighted by its presence among the UN SDGs. And yet, the usage of ML in the education sector has been limited to speciﬁc, narrow applications such as predicting the probability 4For a more extensive overview of the opportunities and challenges of using ML in healthcare, see Ghassemi et al. [2018] 7 of learner attrition [Chaplot et al., 2015] or improving learner evaluation [Abbott, 2006]. There are many reasons for this, starting from the difﬁculty in representing learning content in a domain-agnostic way to facilitate scalability, to overcoming cultural and linguistic barriers to deploying tutors worldwide, but also more fundamental issues such as the lack of large-scale educational datasets and the inherent technological constraints in developing countries. Despite these hurdles, there are many new and longstanding efforts to create intelligent tutors, be it using symbolic AI approaches such as ontologies and knowledge modeling [Nkambou et al., 2010], educational data mining [Dutt et al., 2017] or, more recently, ML-driven approaches [Conati et al., 2018]. However, there are very high stakes in the ﬁeld, since technological interventions have the potential to make considerable, long-term impact on human livelihoods, for example lifting people out of poverty by endowing them with linguistic and numerical literacy, but these can be hindered by bias and technological constraints. We therefore agree with recent proposals to improve and support human learning at scale and believe that ML has a key role to play in this endeavour. This can be done, for instance, by partnering up with existing education initiatives and organizations in order to learn what their speciﬁc needs are and how ML can be used to meet them, or else by collaborating with Massive Open Online Course (MOOC) creators in order to gather data and make it available to the ML community, and ﬁnally by sharing learning materials and activities used in local education initiatives (e.g. university courses in Machine Learning) so that they proﬁt learners in places where access to high-quality technical education is limited. 3.3 AI for the Environment Climate change is, without a doubt, one of the biggest challenges that humanity has faced, and we are at an important point in history when we are both aware of the issue and still have the possibility to change its course. Climate change has been described as a ‘wicked’ problem, due to features such as the difﬁculty in deﬁning the problem itself and in developing and deploying solutions to it, the lack of central authority that can solve it, the incentives for individual countries or companies to not do their share, and the cognitive biases that discount the future impacts of our actions [Head et al., 2008, Levin et al., 2012]. Furthermore, while we do not know of any single technological silver bullet as solution to climate change, there are nonetheless numerous technical challenges for which ML can be helpful, and which can be combined to make a signiﬁcant impact on the overall issue. These challenges and the ongoing ML approaches to tackle them were presented in a recent survey paper [Rolnick et al., 2019]. We will not go into all of these at length, but we will focus on a few examples that are particularly salient and that we hope will give an idea of both the relevance of deploying ML in environmental applications and the opportunities that this can generate. Energy and Transportation Together, electricity and transportation systems are estimated to produce close to half of anthropogenic greenhouse gas (GHG) emissions [Allen et al., 2019] and both sectors have their own unique challenges for decarbonization. For instance, one of the major obstacles to building and using renewable energy sources such as solar and wind is the variability of their output, which is inherently problematic since the power generated by an energy grid must equal the power used by its consumers at any given moment. Currently, this means that despite the existence of solar panels and wind turbines, these must be complemented by controllable but highly polluting energy sources such as coal and natural gas plants. ML methods that are appropriate for time-series predictions, such as Recurrent Neural Networks are particularly suited for these types of tasks [Voyant et al., 2017] and can dramatically lower the barrier to entry for renewable energy globally. Furthermore, even in cases where controllable energy sources are used, demand on the energy grid will still ﬂuctuate based on usage; in this case, ML techniques such as Reinforcement Learning and Dynamic Scheduling can be used to balance the grid in real time [Vázquez-Canteli and Nagy, 2019]. In transportation, reducing activity is a key part in reducing GHG emissions; however, given the highly regional nature of transportation methods (i.e. high-speed trains are only an option in Europe, whereas many major US cities have limited public transportation), custom solutions are needed to make a signiﬁcant impact. ML can be of particular help in estimating and predicting vehicle ﬂow to minimize it, for example by helping to optimize the design of new roads and hubs [Sommer et al., 2017] and monitoring trafﬁc [Kaack 8 et al., 2019], as well as estimating carbon emissions in real-time [Nocera et al., 2018]. ML can also be used for designing more energy-efﬁcient batteries [Hoffmann et al., 2019] which will become an increasingly important concern as more people switch to electric vehicles. In both cases of energy and transportation, ML can be used to make systems more efﬁcient and to improve predictions of complex phenomena based on large amounts of data; nonetheless, it remains only one part of the solution, and as tempting as it is to halt research projects once a theoretically plausible solution has been found (and a research paper published), what is key here is working with domain experts to bring projects towards deployment, where concrete impact can be made. Transversal connections between disciplines are therefore key, and must be established and fostered for projects to ﬂourish. Individuals and Societies While changes in our climate can be abstract, quantiﬁed in degrees of warming or tons of CO2, climate change will also have very concrete impacts on society, for instance by decreasing crop yield, increasing the frequency of extreme weather events such as hurricanes and storms, and impacting biodiversity. There are a myriad of ways in which ML can help face these, whether it be by analyzing real-time images and recordings of ecosystems to detect species [Duhart et al., 2018] and deforestation [McDowell et al., 2015], improving disaster preparation and response by generating real-time maps from satellite imagery [Voigt et al., 2007] and even setting an optimal price on carbon to accelerate the transition to a low-carbon energy economy [Wei et al., 2018]. Finally, while we are far from being able to predict the exact impact that increasing the carbon tax will have on the different levels of society and industry (i.e. federal and regional governments, local and international companies, and individuals), this is a worthwhile area of research and exploration, with potentially huge consequences in helping political leaders make more informed choices in addressing the climate crisis. It is therefore useful to continue gathering data and building trust between members of the political ecosystem and ML practitioners to learn from each other and to facilitate the deployment of technological solutions in setting government policies. On an individual level, there are many reasons why individuals cannot, or will not, act on climate change, either common misconceptions regarding the fact that individuals cannot make meaningful impact on a global problem, or cognitive biases that increase an individual’s psychological distance to climate change. In the ﬁrst case, ML-infused tools to estimate the carbon footprint of individuals and households [Jones and Kammen, 2011] and to model individual behavior with regards to sustainable lifestyle choices and technologies [Carr-Cornish et al., 2011] can be very useful if they are sufﬁciently accurate and deployed on a large scale. Finally, minimizing psychological distance to the future effects of climate change is a promising way to reduce cognitive bias – in this regard, it is possible to use images generated using Generative Adversarial Networks (GANs) which represent the impacts of extreme events on locations that have personal value to the viewer [Schmidt et al., 2019]. A crucial part of developing ML tools for individuals is, once again, working with multidisciplinary experts in psychology, scientiﬁc communication, and user design to ensure that the tools created reach the largest possible audience and maximize their positive impact.</body>
  <conclusion>Technology in general, and ML more speciﬁcally, carries a great potential for change and disruption. While neither of these is guaranteed to make the world a better place, this potential can most deﬁnitely be used to have a positive impact on the world. In the present article, we have illustrated some inspiring projects that aim to make the world a better place and by using the powerful techniques and approaches that ML has brought forward. We believe that as ML researchers and practitioners, we have the responsibility to leverage our (super)powers to contribute to these efforts. This can be done by connecting with established actors from industry and policy or experts from other relevant disciplines, by learning from their past experiences, and by working together to propose innovative solutions to major problems, deployed in places where they will have a positive impact. We live in a world with many global and local challenges and issues that are in constant evolution, and it is easy to be overwhelmed by this ﬂux of information and focus on a small sandbox in which we feel safe and in control, in order to develop and study the aspects of ML that interest us most. But it is naive 9 and Lauren Kirchner. Jeff Larson, Surya Mattu, Machine bias, propub https://www.propublica.org/article/machine-bias-risk-assessmentsto believe that our sandbox is an isolated isle that is not connected to the rest of the world – since even in the case of theoretical work, communication and cross-pollination are unavoidable – and each of us is also a citizen who is concerned collective debates, while many of us could worry about the world in which our descendants will live. We believe that there are thought processes that should take place in the head of every ML practitioner regarding the nature of the work they are doing and the potential pitfalls and impacts of this work in the world around them, some of which we have listed in the ﬁrst part of the current paper. And while we do not claim to have all the answers to all of these tough questions, we hope that we can start a conversation that will accompany ML research and practice throughout its infancy towards its tumultuous teenage years in the coming decades, and eventually towards mature adulthood beyond that.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Robert G Abbott. Automated expert modeling for automated student evaluation. In International Conference on Intelligent Tutoring Systems, pages 1–10. Springer, 2006M Allen, P Antwi-Agyei, F Aragon-Durand, M Babiker, P Bertoldi, M Bind, S Brown, M Buckeridge, I Camilloni, A Cartwright, et al. Technical summary: Global warming of 1.5c. an ipcc special report on the impacts of global warming of 1.5c above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty, 2019Julia Angwin, licain-criminal-sentencing, 2016. Accessed: 2019-11-25Filippo Arcadu, Fethallah Benmansour, Andreas Maunz, Jeff Willis, Zdenka Haskova, and Marco PrunottoDeep learning algorithm predicts diabetic retinopathy progression in individual patients. NPJ digital medicine, 2(1):1–9, 2019UN General Assembly. Universal declaration of human rights. UN General Assembly, 302(2), 1948Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems, pages 4349–4357, 2016Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classiﬁcation. In Conference on fairness, accountability and transparency, pages 77–91, 2018Simone Carr-Cornish, Peta Ashworth, John Gardner, and Stephen J Fraser. Exploring the orientations which characterise the likely public acceptance of low emission energy technologies. Climatic change, 107(3-4):549–565, 2011Devendra Singh Chaplot, Eunhee Rhim, and Jihie Kim. Predicting student attrition in moocs using sentiment analysis and neural networks. In AIED Workshops, volume 53, pages 54–57, 2015Cristina Conati, Kaska Porayska-Pomsta, and Manolis Mavrikis. Ai in education needs interpretable machine learning: Lessons from open learner modelling. arXiv preprint arXiv:1807.00154, 2018Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women, reuters business newshttps://www.reuters.com/article/us-amazon-com-jobs-automationinsight/amazon-scraps-secret-ai-recruiting-tool-that-showed-biasagainst-women-idUSKCN1MK08G, 2018. Accessed: 2019-11-25Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255Ieee, 2009Clement Duhart, Gershon Dublon, Brian Mayton, and Joseph Paradiso. Deep learning locally trained wildlife sensing in real acoustic wetland environment. In International Symposium on Signal Processing and Intelligent Recognition Systems, pages 3–14. Springer, 2018 10 Ashish Dutt, Maizatul Akmar Ismail, and Tutut Herawan. A systematic review on educational data miningIEEE Access, 5:15991–16005, 2017Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, and Rajesh Ranganath. Opportunities in machine learning for healthcare. arXiv preprint arXiv:1806.00388, 2018Jeffrey Alan Golden. Deep learning algorithms for detection of lymph node metastases from breast cancer: helping artiﬁcial intelligence be seen. Jama, 318(22):2184–2186, 2017Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680, 2014Don W Gotterbarn, Amy Bruckman, Catherine Flick, Keith Miller, and Marty J Wolf. Acm code of ethics: a guide for positive action, 2018Brian W Head et al. Wicked problems in public policy. Public policy, 3(2):101, 2008Yasmeen Hitti, Eunbee Jang, Ines Moreno, and Carolyne Pelletier. Proposed taxonomy for gender bias in text; a ﬁltering methodology for the gender generalization subtype. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 8–17, 2019Jordan Hoffmann, Louis Maestrati, Yoshihide Sawada, Jian Tang, Jean Michel Sellier, and Yoshua BengioData-driven approach to encoding and decoding 3-d crystal structures. arXiv preprint arXiv:1909.00949, 2019IEEE. Ieee standard review — ethically aligned design: A vision for prioritizing human wellbeing with artiﬁcial intelligence and autonomous systems. In 2017 IEEE Canada International Humanitarian Technology Conference (IHTC), pages 197–201. IEEE, 2017Anna Jobin, Marcello Ienca, and Effy Vayena. Artiﬁcial intelligence: the global landscape of ethics guidelines. arXiv preprint arXiv:1906.11668, 2019Christopher M Jones and Daniel M Kammen. Quantifying carbon footprint reduction opportunities for us households and communities. Environmental science &amp; technology, 45(9):4088–4095, 2011Lynn H Kaack, George H Chen, and M Granger Morgan. Truck trafﬁc monitoring with satellite images. In Proceedings of the Conference on Computing &amp; Sustainable Societies, pages 155–164. ACM, 2019Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998Kelly Levin, Benjamin Cashore, Steven Bernstein, and Graeme Auld. Overcoming the tragedy of super wicked problems: constraining our future selves to ameliorate global climate change. Policy sciences, 45 (2):123–152, 2012Nate G McDowell, Nicholas C Coops, Pieter SA Beck, Jeffrey Q Chambers, Chandana Gangodagamage, Jeffrey A Hicke, Cho-ying Huang, Robert Kennedy, Dan J Krofcheck, Marcy Litvak, et al. Global satellite monitoring of climate-induced vegetation disturbances. Trends in plant science, 20(2):114–123, 2015Roger Nkambou, Riichiro Mizoguchi, and Jacqueline Bourdeau. Advances in intelligent tutoring systems, volume 308. Springer Science &amp; Business Media, 2010Silvio Nocera, Cayetano Ruiz-Alarcón-Quintero, and Federico Cavallaro. Assessing carbon emissions from road transport through trafﬁc ﬂow estimators. Transportation Research Part C: Emerging Technologies, 95:125–148, 2018Benjamin D Nye. Intelligent tutoring systems by and for the developing world: A review of trends and approaches for educational technology in a global context. International Journal of Artiﬁcial Intelligence in Education, 25(2):177–203, 2015 11 I Perisic. How artiﬁcial intelligence is shaking up the job market. https://www.weforum.org/ agenda/2018/09/artificial-intelligence-shaking-up-job-market/, 2018. Accessed: 2019-11-25Rimma Pivovarov, Adler J Perotte, Edouard Grave, John Angiolillo, Chris H Wiggins, and Noémie ElhadadLearning probabilistic phenotypes from heterogeneous ehr data. Journal of biomedical informatics, 58: 156–165, 2015Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predictions of any classiﬁer. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135–1144. ACM, 2016David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luccioni, et al. Tackling climate change with machine learning. arXiv preprint arXiv:1906.05433, 2019RSS. A guide for ethical data science. https://www.actuaries.org.uk/system/files/ field/document/An%20Ethical%20Charter%20for%20Date%20Science%20WEB% 20FINAL.PDF., 2019. Accessed: 2019-11-25Victor Schmidt, Alexandra Luccioni, S Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, Jennifer Chayes, and Yoshua Bengio. Visualizing the consequences of climate change using cycle-consistent adversarial networks. arXiv preprint arXiv:1905.03709, 2019Lars Wilko Sommer, Tobias Schuchert, and Jürgen Beyerer. Fast deep vehicle detection in aerial imagesIn 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 311–319. IEEE, 2017Patrick Suppes and Mona Morningstar. Computer-assisted instruction. Science, 166(3903):343–350, 1969M Szczepa´nski. Economic impacts of artiﬁcial intelligence (ai), european parliamentary research service, pe 637.967. http://europarl.europa.eu/RegData/etudes/BRIE/2019/637967/ EPRS_BRI(2019)637967_EN.pdf, 2019. Accessed: 2019-11-25UNHCRThe sustainable development goals and addressing statelessnesswww.refworld.org/docid/58b6e3364.html, 2017. Accessed: 2019-11-25José R Vázquez-Canteli and Zoltán Nagy. Reinforcement learning for demand response: A review of algorithms and modeling techniques. Applied energy, 235:1072–1089, 2019Stefan Voigt, Thomas Kemper, Torsten Riedlinger, Ralph Kieﬂ, Klaas Scholte, and Harald Mehl. Satellite image analysis for disaster and crisis-management support. IEEE transactions on geoscience and remote sensing, 45(6):1520–1528, 2007Cyril Voyant, Gilles Notton, Soteris Kalogirou, Marie-Laure Nivet, Christophe Paoli, Fabrice Motte, and Alexis Fouilloy. Machine learning methods for solar radiation forecasting: A review. Renewable Energy, 105:569–582, 2017Sun Wei, Zhang Chongchong, and Sun Cuiping. Carbon pricing prediction based on wavelet transform and k-elm optimized by bat optimization algorithm in china ets: the case of shanghai and hubei carbon markets. Carbon Management, 9(6):605–617, 2018James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, and Jimbo Wilson. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics, 26(1):56–65, 2019Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez, Kenneth Jung, Katherine Heller, David Kale, Mohammed Saeed, et al. Do no harm: a roadmap for responsible machine learning for health care. Nature medicine, 25(9):1337–1340, 2019Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. In International Conference on Machine Learning, pages 325–333, 2013 https:// 12</biblio>


  <preamble>surveyTermExtraction.pdf</preamble>
  <titre>AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT
SYSTEMS</titre>
  <auteurs>
    <auteur>
      <name>M. Teresa Cabré Castellví</name>
      <mail>teresa.cabre@trad.upf.es</mail>
      <affiliation>Institut Universitari de Lingüística Aplicada, Universitat Pompeu Fabra
La Rambla, 30-32
Barcelona - Spain - 08002</affiliation>
    </auteur>
    <auteur>
      <name>Rosa Estopà Bagot</name>
      <mail>rosa.estopa@trad.upf.es</mail>
      <affiliation>Institut Universitari de Lingüística Aplicada, Universitat Pompeu Fabra
La Rambla, 30-32
Barcelona - Spain - 08002</affiliation>
    </auteur>
    <auteur>
      <name>Jordi Vivaldi Palatresi</name>
      <mail>jorge.vivaldi@info.upf.es</mail>
      <affiliation>Institut Universitari de Lingüística Aplicada, Universitat Pompeu Fabra
La Rambla, 30-32
Barcelona - Spain - 08002</affiliation>
    </auteur>
  </auteurs>
  <abstract>In this paper we account for the main characteristics and performance of a number of recently developed term extraction systems. The analysed tools represent the main strategies followed by researchers in this area. All systems are analysed and compared against a set of technically relevant characteristics.</abstract>
  <introduction>In the late 80s there was an acute need, from different disciplines and goals, to automatically extract terminological units from specialised texts. In the 90s large computerised textual corpora have been constructed resulting in the first programs for terminology extraction1 (henceforth TE) which have showed encouraging results. Throughout the current decade computational linguists, applied linguists, translators, interpreters, scientific journalists and computer engineers have been interested in automatically isolating terminology from texts. There are many goals that have led these different professional groups to design software tools so as to directly extract terminology from texts: building of glossaries, vocabularies and terminological dictionaries; text indexing; automatic translation; building of * In Bourigault, D.; Jacquemin, C.; L’Homme, M-C. (2001) Recent Advances in Computational Terminology, 53-88. 1 In order to give a broader view of TE we use both extractor and detector to refer to the same notion. However, we are aware of the fact that some scholars attribute different meanings to these words. 2 Automatic Term Detection: a Review of Current Systems knowledge databases; construction of hypertext systems; construction of expert systems and corpus analysis. From the appearance of TERMINO (the first broadly known term detector) in 1990 until today a number of projects to design different types of automatic terminology detectors have been carried out to assist terminological work. However, despite the large number of studies in progress, the automatisation of the terminological extraction phase is still fraught with problems. The main problems encountered by term extractors are: (1) identification of complex terms, that is, determining where a terminological phrase begins and ends; (2) recognition of complex terms, that is, deciding whether a discursive unit constitutes a terminological phrase or a free unit; (3) identification of the terminological nature of a lexical unit, that is, knowing whether in a specialised text a lexical unit has a terminological nature or belongs to general language and (4) appropriateness of a terminological unit to a given vocabulary (this has scarcely been addressed from the point of view of automatization). Systems for TE are based on three types of knowledge: (a) linguistic; (b) statistical; (c) hybrid (statistical and linguistic). Hence, there are different approaches to automatic term detection. All systems analyse a corpus of specialised texts in electronic form and extract lists of word chunks (i.e. candidate terms) that are to be confirmed by the terminologist. To make the terminologist’s task easier the candidate term is provided with its context and, when available, with any other further information (frequency, relationship between terms, etc.) Two relevant aspects regarding the nature of terms are termhood and unithood2; TE systems may be designed based on only one of these two aspects. Some practical experiments following each scheme for ranking a set terms extracted from Japanese texts are presented in (Nakagawa &amp; Mori, 1998). They show that results in precision and recall are very close but the set of terms extracted are a somewhat different. This is still a research issue. Alongside term detection we find the task of automatic document indexing (i.e. information retrieval, IR). This applied field of natural language processing (NLP) techniques has an interesting common point with automatic term detection, that is, word chunks that index a given document are often terminological units. This same goal explains why many extraction systems are rooted on IR as well as on the analysis of a specific IR system with no application whatsoever to TE. he difference between these two approaches lies in the fact that a tool for TE should extract all terminological units from a text, whereas IR focuses on the extraction of only words or word sequences that better describe the contents of the document regardless of their grammatical features. The standard approach to IR consists in processing documents so as to extract the so-called indexing terms. These terms are usually isolated words containing enough semantic load to provide information about its goodness when describing documents. Queries are processed in a similar fashion to extract query terms. With regard to queries the relevance of documents is based exclusively on their representing terms. This is the reason why their choice is crucial. Often these indexing terms are single words although it is known that isolated words are seldom relevant enough to decide the semantic value of a document with regard to the query. This fact has given rise to the ever-growing appearance, in the TREC 3 assessments, of word and word-sequence indexing systems using NLP techniques. Statistically based systems function by detecting two or more lexical units whose occurrence is higher than a given level. This is not a random situation, but it is related to a particular usage of these lexical units. This principle, called Mutual Information, also applies to other science domains such as telecommunications and physics. Term detectors based on hybrid knowledge tend to use this idea prior to a linguistic-based processing. The problem with this kind of approach is that there are low-frequency terms difficult to be managed by extraction systems. Here it is important to note that these systems use basically numerical information and thus are prone to be language-independent. The two most frequently used measures in the assessment of these systems are found in IR: recall and precision. Recall is defined as the relationship between the sum of retrieved terms and the sum of existing terms in the document that is being explored. In contrast precision accounts for the relationship between those extracted terms that are really terms and the aggregate of candidate terms that are found. These measures can be interpreted as the capacity of the detection system to extract all terms from a document (recall) and the capacity to discriminate between those units detected by the system which are terms and those which are not (precision). The fact that recall accounts for all terms from a document implies that it is a figure much more difficult to estimate and improve than precision. In contrast with this traditional approach, other approaches attempt to solve the problem by using linguistic knowledge, which may include two types of information: ) Term specific: it consists in the detection of the recurrent patterns from complex terminological units such as noun-adjective and noun-preposition-noun. This calls for the use of regular expressions and techniques of finite state automata. b) Language generic: it consists in the use of more complex systems of NLP that start with the detection of more basic linguistic structures: noun phrase (NP), prepositional phrase (PP), etc. In both approaches each word is associated to a morphological category. In order to do so different strategies are proposed: from coarse systems that do not make use of any dictionary to complex systems that have an extremely detailed morphological analysis and a final phase of disambiguation. Systems that harness structural information resort to techniques of partial analysis to detect potentially terminological phrasal structures. There are also systems that benefit from their understanding of what is a non-term so they are at some point in between those systems already mentioned. Other systems try to reutilize current terminological databases to find terms, variants or new terms. Systems based on linguistic knowledge tend to use noise and silence as a measure of its efficiency. Noise attempts to assess the rate between discarded candidates and accepted ones; silence attempts to assess those terms contained in an analysed text that are not detected by the system. Noise is common problem of those systems using this approach. Errors in the assignation of morphological category are also shared by these systems. The type of knowledge used leads to language-specific systems and therefore it requires a prior linguistic analysis and probably a redesign of many parts of the system. Knowledge in artificial intelligence has been traditionally obtained from experts in each domain. This has yielded several difficulties so that some scholars have focused on automatization and systematisation in knowledge acquisition. This strategy seems to show the benefits of a terminological approach. Thus some researchers (e.g. Condamines, 1995) have proposed the construction of terminological knowledge databases so as to include linguistic knowledge in traditional databases. Although this is a recent approach, there is no database yet containing all the features that could be used in TE, i.e. there is hardly any semantic information. Thus closed lists of words containing sparse semantic information within a given specialised domain have been proposed. In this paper we attempt to analyse the main systems of terminology extraction in order to describe its current status and thus be able to enrich them. This paper is divided up into two main parts: firstly, the largest part is devoted to describe various systems of terminology extraction together with a short evaluation in which weak and strong points have been outlined. Secondly, the terminology extraction systems have been classified according to some parameters.</introduction>
  <body>In the following sections we offer a critical description of number of semiautomatic terminology extraction systems. In all cases, the following information is given: a) The reference data of the system, that is, the author and the publication where the tool is first mentioned and the system goal. b) A brief description of the system. c) A short evaluation of the most relevant aspects. This evaluation is mainly based on papers, oral presentations in congresses and working papers, etc. 2.1. ANA Reference publication: Enguehard and Pantera (1994) Main goal: Term extraction ANA (Automatic Natural Acquisition) has been developed in accordance with the following design principles: non-utilisation of linguistic knowledge, dealing with written and oral texts (interview transcripts) and non-concern about syntactic errors. According to the current trend of harnessing statistical techniques in the study of natural language, scholars use Mutual Information as a measure of lexical association4. In order to avoid the involvement of linguistic knowledge the concept of “flexible string recognition” is created, which generates a mathematical function so as to determine the degree of similarity between words. Thus, no tool for morphological analysis is needed. For instance, the string colour of painting represents other similar strings like: colour of paintings, colour of this painting, colour of any painting, etc. The system has neither a dictionary nor a grammar. The architecture of ANA is composed of 2 modules: a familiarity module and a discovery module. The first module determines the following 3 groups of words, which constitute the only required knowledge for term detection: a. function words (i.e. empty words): a, any, for, in, is, of, to... b. scheme words (i.e. words establishing semantic relationships) such as box of nails, where the preposition shows some kind of relationship between box and nails. c. bootstrap (i.e. set of terms that constitutes the kernel of the system and the starting point for term detection). The second module consists in a gradual acquisition process of new terms from existing ones. Further, links between detected terms are automatically generated 4 Remarkable examples of the use of these techniques are the works of Church &amp; Hanks (1989) on word association and Smadja (1991) on collocation extraction from large corpora. 6 Automatic Term Detection: a Review of Current Systems shade to build a semantic network. This module is based on word co-occurrence that can have 3 types of interpretations: • expression: high-frequency existing terms (TEXP) in the same window. The new word is considered a new term and thus is included in the semantic network. For instance if the system has diesel and engine as a known terms and finds sequences like: ... the diesel engine is... or ... this diesel engine has... Then the sequence diesel engine is accepted as a new term and is included in the semantic network as a new node with links to diesel and engine (see figure below). • candidate: an existing term appears frequently (TCAND) together with another word and a scheme word as in: ... any shade of wood... or ... this shade of colour... Here shade becomes a new term and is placed in a new node of the semantic network (see figure below). • expansion: an existing term appears frequently (TEXPA) in the same word sequence, without including any scheme word: ... use any soft woods to... or ... this soft woods or... As a result, soft wood is incorporated into the term list and the semantic network as a new node with a link to woods (see fig. 1 below). The system keeps on recursively seeking elements with the three diesel interpretations already mentioned until a new found. is Enguehard and Pantera (1994) tested it by processing a document in English of around 25,000 words and 29 reference terms. The system managed to extract 200 terms with an error rate of 25%. Figure 1 Term candidates interpretation Diesel engine Soft wood Evaluation Minimising linguistic resources is an extremely interesting issue, since it is difficult to compile them. Likewise flexible string recognition may well apply to actual texts. A negative aspect of the system is that those terminological units added to the list of valid terms after each cycle are not validated. Thus ANA allows for the inclusion of non-valid terms that add up to the term list. However, no data about the efficiency of this proposal are reported. 2.2. CLARIT5 Reference publication: Evans and Zhai (1996) Main goal: Document indexing 5 Further information can be found at: http://www.clarit.com. engine wood term AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 7 Document indexing for IR is an important field of application of NLP techniques. This branch holds common points with term detection since the word sequences that help in document indexing are normally terminological units too. CLARIT belongs to the group of systems that advocate an elaborated textual processing to detect complex terms in order to reach a more appropriate description of documents. This is the reason why we have included this system amongst terminology detectors. Evans and Zhai (1996) propose the following kind of phrases for indexation purposes: 1. lexical atoms (hot dog, stainless steel, data base, on line, ...) 2. head modifier pairs (treated strip, ...) 3. subcompounds (stainless steel strip, ...) 4. cross-preposition modification pairs (quality surface vs. quality of surface) The methodology starts with the morphological analysis of words and the detection of noun phrases (NPs). The system distinguishes simplex noun phrases from cross-preposition simplex phrases. What is behind this is the introduction of statistics to corpus linguistics. Statistics here focuses on documents, that is, there is no prior training corpus. Linguistic knowledge facilitates the calculation weeding out irrelevant structures, improves the reliability of statistical decisions and adjusts the statistical parameters. The whole process is showed in the figure below: Raw Text First, the raw text is parsed so as to extract NPs. Then each NP is recursively parsed with the purpose of secure groupings. In this phase lexical atoms are also detected and NPs are structured. Finally at the generation phase the remaining compounds are obtained. Lexical atoms are defined as sequences of two or more words constituting a single semantic unit such as space shuttle, part of speech and hot dog. Since the detection of these units is fraught with problems two heuristical rules are proposed: a. The words that constitute a lexical atom establish a close relationship and tend to lexicalise as if they were a single-word lexical unit. b. When acting as NP, lexical atoms hardly allow the insertion of words. Figure 2 Whole process in CLARIT Lexical atoms Attested terms Subcompound generator meaningful subcompounds CLARIT NP Extractor Simplex NP Parser Structured NPs the most finding NPs 8 Automatic Term Detection: a Review of Current Systems The first condition takes place if the frequency of the target pair W1W2 is higher than any other pair from the NP that is being processed. In the second condition the frequencies of grouped and separated occurrences are compared and there is a threshold beyond which the association is weeded out. This threshold is variable according to the function of sentence morphological category. In English texts, the most favoured sequence is that of noun-noun. NP analysis is also a recursive process. At every new phase the most recent lexical atoms are used for finding new associations that will be used in the following phase. The process keeps going until the whole NP is analysed. Let us consider the example below: general purpose high performance computer general purpose [high performance] computer [general purpose] [high performance] computer [general purpose] [[high performance] computer] [[general purpose] [[high performance] computer]] The grouping order shows those sequences with a more reliable association score. In order to determine the association score a number of rules are taken into account: • Lexical atoms are given score 0 as well as adverb combination with adjective, past participles and progressive verbs, • Syntactically impossible pairs are given score 100 (noun-adjective, nounadverb, adjective-adjective, etc.). • As to the remaining pairs, there is a formula that account for the frequency of each word, the association score of this word with other words from the NP and of two random parameters. To increase its reliability the association score is recomputed after every assignation association. The system has been tested in an actual retrieval task of document indexing substituting the default NLP module in the CLARIT system. The corpus and the queries were the standards used in the TREC conferences. There have been noticed some improvements in recall as well as in precision, which, in the author’s opinion, justifies the use of these techniques. Then in the TREC-5 report a more detailed evaluation of the system is made (Zhai et al. 1996). All in all it is concluded that the use of these techniques is effective, which enforces the similarities between term indexing and terminology extraction. Evaluation This seems to be an interesting system and the applicability of some basic ideas to terminology detection appears to be feasible. Actually CLARIT holds similarities with the Daille’s (1994) proposal (a linguistically-driven statistics). AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 9 It should be borne in mind, however, that problems of terminology extraction and document indexing are similar but no identical so that many decisions should be re-considered strictly from the point of view of term detection. It is also noteworthy that this system only extracts NP terminological units and the data provided about how this system works are related to the application for which it has been designed. 2.3. Daille-94 (ACABIT) Reference publication: Daille (1994) Main goal: Term extraction The main idea behind this system is to combine linguistic knowledge with statistical measures. Here the corpus should contain all the morphological information. Then a list of candidate terms is created according to text sequences that provide syntactic patterns of term formation. This information uses statistical methods to filter out this list. This final process is different from other systems in that it only uses linguistic resources. Assuming the fact that all terminological banks are basically composed of compound nouns, the program focuses on the detection of binary compound nouns and disregards other co-occurring categories. This assumption lies in the fact that there is a large number of this kind of nouns in specialised languages. Further, most of these compounds of 3 or more constituents can be treated in a binary form. Those patterns considered relevant for French are N1 PREP (DET) N2 and N ADJ PREP à (DET) N2, together with right and left coordination. Statistical algorithms are applied to these patterns. The author is aware of the fact that the application of statistical measures leads to some noise rate, that is, low-frequency terms will not be recognised. The technique used for pattern recognition is that of finite state automata. Automata are represented by a subset of grammatical tags to which some lemmas, inflected forms and a punctuation mark are added. Thus we can regard automata as linguistic filters that select defined patterns and also determine their occurrence frequency, distance and variation. Each morphosyntactic pattern is associated with a specific finite automaton. The corpus is given a statistical treatment based on a large number of statistical measures, which are grouped in the following classes: frequency measures, association criteria, diversity criteria and distance measures. The starting point is considering the two lemmas that constitute a pair within a pattern as two variables on which the dependence degree is measured. Data are represented in a standard contingency table: 10 Automatic Term Detection: a Review of Current Systems L2 L1 A Lm C Ln b d where a = L1L2 occurrences b = L1+Ln (n≠2) occurrences c = Lm+L2 (m≠1) occurrences d = Lm+Ln (m≠1 and n≠2) cubed (IM3), criterion6 likelihood association Eighteen measures are applied with the aim of establishing the degree of independence of the variables in the contingency table. The analysis of the results shows that only four of these measures are relevant to the purpose: frequency, criterion, Fager/MacGowan criterion. Evaluation Unlike in other systems, in ACABIT frequency has turned out to be one of the most important measures for term detection from a given area. However, the classification resulting from the application of this frequency shows an important number of frequent sequences that are not terms and, in contrast, does not suggest the low-frequency terms. Daille (1994) believes that the best measure is the likelihood criterion, since it is a real statistical test, it proposes a classification that accounts for frequency, it behaves adequately with large and medium size corpora and it is not defined in those cases that are not to be considered. In any case, this measure yields some noise due to several reasons: a. Errors in the morphological mark-up. b. Some combinations that are never of a compounding nature: ko bits (kilobits), à titre d’exemple (as an example)... ... c. Combinations of 3 or more elements, related to the problems of composition and modification: bande latérale -unique(-singleside band), service fixe -par satellite(-satellitefixed service), etc. 2.4. FASTR7 Reference publication: Jacquemin (1996) Main goal: Term variation detection The aim of this tool is to detect terms variants from a set of previously known terms. These terms may be available from a reference database or a term acquisition software. What is crucial in this system that it is not needed to start from scratch every time. Optionally Fastr can also be used for TE. The first step for applying Fastr is to obtain and analyse a set of existing terms and thus having a set of rules of a given grammar. The FASTR grammatical 6 The formula was experimentally obtained by the autor from the association number described in Brown et al. (1988) in the aim of favouring the most frequent pairs: IM3=log2 (a3/(a+b)(a-b)) 7 Further information can be obtained at http://www.limsi.fr/Individu/jacquemi/FASTR/index.html AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 11 formalism is an extension of that of PATR-II (Shieber, 1986). A partial parser based on the unification mechanism is responsible for the application of these rules. Term variants are obtained through a metarule mechanism that is dynamically calculated. For instance, the term serum albumin corresponds to the Noun-Noun sequence and is associated with the following rule: rule 1: N1 → N2 N3 &lt;N1 lexicalization>= ‘N2’ &lt;N2 lemma>=serum &lt;N3 lemma>=albumin. Metarule Coor(X1 (cid:198) X2 X3) = X1 (cid:198) X2 C4 X5 X3 The value indicated by the feature “lexicalization” will be use just before partial parsing to selectively activate the target rules. Thus the above rule is linked to the word serum and so is activated when this word occurs in the sentence that is being parsed. At a different level several metarules generate new rules in order to describe all possible variations of each term from the reference list. Each metarule presents a particular structure and a specific pattern type. For instance, the following metarule can be applied to the previous rule: which leads to the new rule: N1 (cid:198) N2 C4 X5 N3 This latter rule allows new constructions that substitute C4 for a conjunction and X5 for an isolated word such as serum and egg albumin. The candidate term is not the whole new construction but the coordinated term (i.e., egg albumin). The words that have given way to the new rule (egg and albumin) maintain their function of constricted equations of the original rule. Further, they are the anchoring point for the application of the metarule. A metarule can be associated with specific restrictions, as for instance: (&lt;C4 lemma>≠but) or (&lt;X5 cat>≠ Dd). In this way, those sequences with no lexical relationship such as serum and the albumin are rejected. The above rule is a coordination rule and it should be noted that there are also other types of rules that account for different kinds of variations: 1. insertion rules: medullary carcinoma (cid:206) medullary thyroid carcinoma 2. permutation rules: control center The FASTR metagrammar for English contains 73 metarules altogether: 25 coordination rules, 17 insertion rules and 31 permutation rules. In any case, for efficiency reasons the new rules are dynamically generated. Each rule is linked to a pattern extractor that permits a very quick acquisition of information. As has been pointed out, the FASTR grammatical formalism is a PATR-II extension (Shieber, 1986). This language allows to write grammars using feature structures. The rules describing terms are composed of a free-context part (N1 → N2 N3) and (cid:206) center for disease control 12 Automatic Term Detection: a Review of Current Systems a number of restriction equations (e.g. &lt;N2 lemma>=serum). First, the system filters the rules that are to be applied according to the given text and then an analysis take place. When Fastr is applied for term acquisition the process is gradual: from a given set of terms the system detects new ones, which allows the beginning of a new cycle and the detection of new candidates. The loop goes on until new terms cannot be detected. The author presents an experiment carried out on a medicine corpus of 1,5 million words and a reference list of 70,000 terms from different specialised domains. After 15 cycles 17,000 terms were detected of which 5,000 were new. The text was processed at a 2,562 word/minute speed. However, the number of recognised terms decreases when the reference list has fewer items. For instance, if the reference sublist of medicine drops to 6,000 terms, then only 3,800 new terms are recognised. The author also postulates the existence of a conceptual relation. between the new terms and the term that has led to their recognition. This relationship is variable in accordance with the type of rule that is applied i.e., insertion or coordination rule. Permutation does not allow any relationship due to the phrasal nature of the relationship. All the language dependent data used by Fastr is stored in separated text files. This feature facilitates the use of the system in other languages as showed by the recent application of Fastr to Japanese, German and Spanish/Catalan. Recently Jacquemin has developed the detection of semantic variation using resources like WordNet or the Microsoft Word97 thesaurus (Jacquemin, 1999). Evaluation The main characteristic of FASTR is its ability for detecting term variants, an aspect often not considered by other systems. The fact of using already recognised and accepted terms is very useful, although, as the author admits, it places restrictions on the acquisition of new terms that are not related to the source terms. TE in Fastr implies that terms that are added to the list of valid terms after each cycle are not validated. Thus, a non-valid term may be added to the list so it is likely that in forecoming cycles more non-valid terms are added. Jacquemin (1996) believes that this is not an important error source because the system, in some way, corrects itself since “normally” non-correct candidates do not give way to new potential candidate terms. Actually this technique should not be isolately applied. Rather, it should be coordinated with other strategies as in (Jacquin &amp; Liscouet 1996) and (Daille 1998). AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 13 tokenizing, which identifies word and sentence boundaries. distributional and morphosyntactic features. lemmatization, which identifies the lemma candidates. 2.5. HEID Reference publication: Heid et al. (1996) Main goal: Term extraction Heid et al. (1996) believe that automatic TE has various applications and dictionary or glossary construction would be the major one. In dictionary construction from computerized corpora two phases are distinguished: linguistic pre-analysis and a term identification tool. Each of these phases requires specific computer tools. In the linguistic pre-processing phase the following processes are required8: a) b) morphosyntactic analysis, which identifies grammatical categories as well as c) POS tagging, which disambiguate morphosyntactic hypotheses. d) For term identification the system has a general corpus retrieval interface that includes a corpus query processor (CQP), a macroprocessor for the CQP query language and a key word in context (KWIC) program, to extract and sort concordances and lists of absolute and relative frequency of search items. TE is linked to a complex query language. The queries will be different according to the types of candidate terms searched for. Thus, for instance, queries about single-word terms are made from morphemes or typical components of compound or derived words (derivatives). In these queries it is assumed that NP affixed terms from specialised languages use more specific affixes and/or prefixes than others. All the word sequence extracted (N-A, N-N, N-V), are based on POS patterns. Heid et al. (1996) have applied these tools to technical texts on automobile engineering in German, which amounts to 35,000 words. The sample has been manually analysed before the application of the above procedures. The results are as follows: • With regard to single-word terms, there has been found a 90% of candidate terms and a 10% of silence. This rate varies from one scheme to another. • With regard to multiword terms, there are no concluding results. The results are less satisfactory and that the same problems as linguistic based are found: POS patterns do not constrain enough the context and produce too much noise. Heid et al. (1996) believe that by using a syntactic parser, as it is the case in English, noise would diminish. 8 Heid et al. (1996) note that a broad coverage morphosyntactic parser for German is not attained. Thus parser results are simulated using POS patterns. 14 Automatic Term Detection: a Review of Current Systems since Heid et al. (1996) consider the frequency criterion. • Finally, collocation extraction is shown to produce noise but not silence, The Ahmad’s statistical measure (Ahmad et al, 1992) of relative frequency in corpora of specialised and general language is applied to this corpus of 35,000 words. They show that the results produced by linguistic corpus query are included in the output of statistical methods. However noise in statistical methods is higher than in linguistic methods. Evaluation To tackle this system it should be taken into account the morphosyntactic features of the German language. Unlike Romance languages, German prefers to form compounds in a synthesising manner. It means that what other languages express via terminological phrases in German is expressed with a single-word term (by word is meant any segment found between two gaps). Thus it can be seen that in German automatic term detection does not depend much on term delimitation but on the terminological nature of a word. This is the reason why we need parameters to distinguish a term from a word of the general language, both having the same morphosyntactic structure. Like most of the reviewed programs, Heid focuses on NP terms although it can also extract collocations combining nouns and verbs. In this case Heid et al. (1996) note that the results are much worse. We do not have specific data about the performance and the results of this system. 2.6. Reference publication: Bourigault (1994) Main goal: Term extraction This system has been developed in the need of the EDF (Electricité de France) society for improving their indexation system. LEXTER aims at locating boundaries among which potentially terminological NPs could be isolated. LEXTER carries a superficial analysis and makes use of the text heuristics in order to obtain those NPs of maximum length that it regards as candidate terms. The program is composed of several modules and works as follows: 1. Morphological analysis and disambiguation module. Texts receive information about the POS and the lemma assigned to every word. 2. Delimitation module. At this stage a local syntactic analysis is carried so as to split the text into maximal-length NPs. For example: alimentation en eau (water supply), pompe d’extraction (extraction pump), alimentation electrique de la pompe de refoulement (electric supply of the forcing back supply). Here the system takes advantage of the negative knowledge about the parts of complex terms. Thus those patterns of a potential term −finite verbs, pronouns and conjunctions− that will never become part of a term are identified and considered LEXTER AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 15 as boundaries. Some of these patterns are simple whereas others are complex (sequences of preposition + determiner). A French example of the latter would be SUR (prep) + LE (definite article): the most common analysis is to propose that this sequence establishes a boundary between NPs as in: on raccorde le câble d’alimentation du banc sur le coffret de décharge batterie. However there is a rate (10%) in which this sequence is part of the term: action sur le bouton poussoir de réarmement or action sur le systeme d’alimentation de secours To solve this and other similar situations, the system uses an endogenous learning strategy of the patterns sub-categorisation. This strategy consists in looking at the corpus to find those sequences of (noun) + sur + le having different contexts on the right hand side. Then non-productive nouns are weeded out. Then sequences such as sur + le are considered sentence boundaries, except for those cases wherein sequences are preceded by the productive noun located in the learning phase. To see how this system works let us suppose that at a first analysis the sequences below are found. Le protection contre Protection contre il s’agit de maintenir la teneur en oxygène de cette eau dans on procède à l’injection d’eau dans on procède à l’injection d’eau dans le système permet l’aiguillage des automates sur le gel est assurée par les grands froids les limites fixées les limites fixées les générateurs de vapeur le prélèvement effectué Then productive sequences are not regarded as term boundaries whereas nonproductive sequences are viewed as external boundaries of the candidate term. In the example above protection contre and eau dans do not become boundaries whereas automates sur does. This strategy permits to detect a considerable amount of complex nouns which otherwise would have been lost. Unfortunately it also allows a great deal of undesirable material (between 10% and 50%). 3. Splitting module. NPs are analysed and their constituents are divided into head and expansion. For example the term candidate pompe d’extraction (extraction pump) is splitted into: pompe –head– (pump) + extraction –expansion– (extraction). At this point the system may find ambiguous situations such as “Noun Adj1 Adj2” and “Noun1 Prep Noun2 Adj” whose analysis is uncertain. To solve these cases an endogenous learning process is followed which is similar to that presented in the delimitation module. 4. Structuring module. The list of term candidates is organised in a terminological network. This network can be produced only by looking at a list of candidate terms and recognising the different parts of each candidate term, like in the following example: 16 Automatic Term Detection: a Review of Current Systems extraction pump forcing back pump pump N head N head E’ expansion electric supply of the forcing back pump head N’ electric supply E expansion E expansion N head E expansion extraction supply forcing back electric Additionally Lexter calculate some productivity figures based on links type occurrences. These coefficients do not become filters, but are passed on to the terminologist as a piece of data so as to facilitate the evaluation of candidate terms. 5. Navigation module. A consulting interface is built (called terminological hypertext) from the source corpus, the candidate term network and the abovementioned coefficients and lists. Although LEXTER is exclusively based on linguistic techniques it produces highly satisfying results and is currently used to exploit different corpora from EDF and different research projects. Besides it has been proved helpful in: text indexation, hypertextual consulting of technical documentation, knowledge acquisition and construction of terminological databases. LEXTER is also used as a terminology extractor in the terminological knowledge base designed by the Terminologie et Intelligence Artificielle (Terminology &amp; Artificial Intelligence) terminology group. SYCLADE (Habert, 1996), a tool for word classification also makes use of LEXTER. Evaluation LEXTER was born in an industrial environment and from the very beginning it sought a robust, accurate and domain-independent tools. These objectives were basically attained although mark-up and disambiguation errors weaken the capacity of the system. Some scholars note that this system (like those which make use of symbolic techniques) produce a considerable amount of noise. Thus of a corpus of 200,000 words there are obtained 20,000 candidate terms which, after the validation stage, amount to 10,000. Also, Bourigault stresses the silence problem, which he estimated around 5% of the total valid terms. Like the vast majority of systems, LEXTER only focuses on NPs since verbs are believed to be term boundary and so they are never part of candidate terms. One of the most remarkable achievements of this system is the endogenous learning mechanism that allow to work autonomously and so there is no need for a complex and large dictionary. In a similar vein it should be highlighted the usefulness of presenting the results hypertextually, since it facilitates the terminologist’s task. AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 17 2.7. NAULLEAU Reference publication: Naulleau (1998) Main goal: Noun phrase filtering system The model designed by Naulleau is a NP extraction system that proposes as term candidates those sequences that comply with certain user tailored profile. The whole process can be divided in two main stages: profile acquisition and profile application. To define its own profile the user chooses the set of phrases that s/he considers relevant for his task and discards the ones that s/he does not consider useful at that time. The data collected in such way is generalised according to their morphological, syntactical and semantic characteristics dynamically creating a set of positive and negative filters. A simple example of positive and negative filters is the following: (1) positive filter: metallic/automatic/nuclear/industrial taps important/recent/necessary/unreliable taps (2) negative filter: Then, those filters produced in the learning stage are applied to new sequences analysed. As a result, some noun arguments and/or PPs can be eliminated. Thus a NP can be divided or reduced and the resulting sequences are passed on to an expert to be evaluated. In doing so the author acknowledges the sociolinguistic nature of the term. It implies that there is no linguistic model that can tell whether a NP is a term or not beyond the scope of a field or even the application. Also, this procedure introduces the idea of how relevant a phrase is in relation to the interest profile of the user and assumes that such relevance may be evaluated on linguistic grounds. This is a fully symbolic approach that uses the AlethIP engine that produces sentences fully lemmatised, tagged and syntactically parsed. Then nouns and adjectives are semantically tagged according to both suffix information and semantic data from AlethIP and using a set of contextual rules for the more frequent and ambiguous words. The whole strategy is based on the evaluation of the relevance of simple syntactic dependencies. Such relevance is only based on the data provided by the user. According to the author, the results are encouraging. However it is difficult to evaluate due to the practical problem posed by such a detailed evaluation. Some additional experiments are described in (Nalleau, 1999). Evaluation This system may be considered the first one to use semantic data as a specific resource for proposing term candidates. Also, as far as we know, is the first time since the very beginning in the design of a TE systems that the user and the idea of relevance to an application are taken into account. 18 Automatic Term Detection: a Review of Current Systems In this way the user may adapt the system to its specific needs but also its intervention may crucially affect the performance of the system. The loss of specific data makes difficult to evaluate the tool behaviour in an actual context. 2.8. NEURAL Reference publication: Frantzi and Ananiadou (1995) Main goal: Term extraction Neural is a system for TE of a hybrid nature, that is it uses both linguistic (morphosyntactic patterns and a list of suffixes specific to the domain) and statistically knowledge (frequency and mutual information). Frantzi &amp; Ananiadou (1995) pays special attention to two different problems: detection of nested terms and detection of low frequency terms using statistical methods. The test bench is a corpus of 55,000 words in the domain of medicine (ophthalmology). The structures analysed are Noun-Noun and Adjective-Noun that are identified using a standard tagger. The list of suffixes includes those frequently found in terminological units in the field of ophthalmology like -oid, oma, -ium. The system is implemented using a Back-Propagation (BP) two layers neural network. The threshold has been set to .5 but this may vary. The BP neural network has been trained with a set of 300 compounds and the tests were made with another set of 300 words. It obtained a success rate of 70 %. The author and other scholars from the Manchester Metropolitan University have been active since 1995 developing specific statistical figures for TE. In this way it is necessary to mention those tasks related to the adding of context information (Frantzi, 1997, Maynard &amp; Ananiadou, 1999). Usually the context is discarded or, alternatively, considered as a bag of words although its relevance is signalled by many scholars. Here the basic assumption is that terms tend to appear grouped in real text, so the termhood figure of a candidate would increase if there are other terms (or candidates highly ranked) in the context. Both Frantzi, 1997 and Maynard &amp; Ananiadou, 1999 propose a similarity figure based on the distance between the candidate and the context words (nouns, adjectives and verbs). This figure is calculated by Frantzi (1997) using statistical and syntactic information while Maynard &amp; Ananiadou (1999) include also semantic information from a specialised thesaurus (UMLS semantic Network). In Maynard &amp; Ananiadou (1999) this similarity figure may also be used to take into account some kind of semantic disambiguation for the sense that gets a better value. A context factor (CF) is added to the figure already used to rank the candidates (Cvalue) and thus reordering the set of candidates as follows: SNCvalue(a) = 0.8*Cvalue(a) + 0.2*CF(a). The authors report improvements in the ranking of term candidates from his eye pathology corpus. Evaluation AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 19 The original system can be seen as a standard hybrid system. The linguistic knowledge includes Greek and Latin affixes and morphosyntactic patterns. The incorporation of this kind of suffixes should be highly productive. However the chosen patterns may well apply to English but not to Romance Languages. The incorporation of the context as part of the data available for evaluating the termhood of a candidate is a very interesting contribution to the behaviour of terms in real texts. It should also serve to increase the relevance of low frequency candidates but no specific figure is given. It is necessary to mention the use of semantic information as a kind of resource that is increasingly used in the TE field. 2.9. NODALIDA-95 Reference publication: Arppe (1995) Main goal: Term extraction NODALIDA, a product designed by the Lingsoft firm, is based on an enhanced version of NPtool that is a program developed at the Department of General Linguistics at the Helsinki University (Finland). NPtool (Voutilanen 1993) generates lists of NPs occurring in the sentences of a text and provides an assessment about whether these phrases are candidates terms or not (ok/?). From these lists all the acceptable sub-chains are obtained. Besides, the source list is multiplied. Let us see an actual example, for the sentence: “exact form of the correct theory of quantum gravity“ NPtool proposes the following additional NPs: form of the correct theory of quantum gravity exact form of the correct theory form of the correct theory Simultaneously there are a number of premises that become the first filter like in the following: “Those NPs preceded by a determiner, adjective or prefixed sentence (kind of, some, one, ...) are weeded out.” As for the remaining NPs, their occurrence frequency is calculated. Further, they are ordered and grouped according to their grammatical head and are presented to the terminologist together with their context. The NPtool module (Voutilanen, 1993) is at the heart of the system. It is a NP detector largely based on the constraint grammar formalism (Karlsson, 1990). Its main features are: (1) Morphological/syntactical descriptions are based on a large set of hand-coded linguistic rules, (2) both the grammar and the lexicon allow a corpus analysis with non-controlled text and (3) disambiguation is made according to only linguistic criteria. As a result, between 3% and 6% of the words remain ambiguous. correct theory gravity quantum gravity form exact form theory 20 Automatic Term Detection: a Review of Current Systems (“manifold” N NOM PL)) The text goes through a previous process so as to determine sentence boundaries, idiomatic expressions, compound forms, typographical signs, etc. Then it is morphologically analysed and a result like this is obtained9: (“the” DET CENTRAL ART SG/PL (@>N))) (“&lt;*the>” (“inlet” N NOM SG)) (“&lt;inlet>” (“&lt;and>” (“and” CC (@CC))) (“&lt;exhaust>” (“exhaust” &lt;SVO> V SUBJUNCTIVE VFIN (@V)) (“exhaust” &lt;SVO> V IMP VFIN (@V)) (“exhaust” &lt;SVO> V INF) (“exhaust” &lt;SVO> V PRES -SG3 VFIN (@V)) (“exhaust” N NOM SG)) (“&lt;manifold>” At this moment disambiguation takes place. For example in the sentence: ”The inlet and exhaust manifolds are mounted on opposite sides of the cylinder head“ two analyses are obtained: (1) on/@AH opposite/@N sides/@NH of/@N&lt; the/@>N cylinder/@NH head/@V (2) on/@AH opposite/@N sides/@NH of/@N&lt; the/@>N cylinder/@>N head/@NH What distinguishes these two analyses is the consideration of whether the final sequence (cylinder head) is a NP or not. The ongoing process gives only two possible analyses for each sentence. First, those NPs of a maximal length are preferred (NP-friendly) and, second, those NPs of a minimal length are preferred (NP-hostile). Then the system compares both strategies and labels each NP as ok/? by considering whether the analysis is shared or not by both strategies Thus the last sentence gets this analysis below: (3) ok : inlet and exhaust manifolds In order to validate this additional information the terminologist is provided with a list of candidate terms. The results reported by the NPtool module are pretty good (precision=95-98% and recall=98.5-100%) with a text of about 20 Kwords. Evaluation NODALIDA is based on the use of linguistic knowledge through a structural approach (i.e., detection of phrasal structures and structural disambiguation). Arppe (1995) presents high-quality results. However, the corpus should be enlarged, since so far tests have been made on quite small corpora. It is not clear how precision and recall figures are calculated, particularly how to determine which terms are deemed to be correct (i.e., those which have the ok signal or all of them). Also it should be stressed that NODALIDA has not been tested using the NPtool enhanced version in an actual situation of terminology problems. 9 The meaning of the syntactic function tags are: @>N = pre-modifier; @&lt; = post-modifier; @CC and @CS= coordination and subordination conjunction; @V = Verb; @NH = nominal head. Finally, “>” and “&lt;” indicate the direction of the phrasal head. ?: opposite sides of the cylinder ?: opposite sides of the cylinder head ok: exhaust manifolds AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 21 Taking into account that the disambiguator is one of the main error sources in this kind of systems, Arppe (1995) believes that a high-degree quality is achieved despite the fact that there are no data about terminology extraction in real situations. Besides, to achieve this quality NODALIDA proposes a great deal of rules, which yields management and control overhead. The list that is passed on to the terminologist to be validated comprises those candidates signalled with ok and ?. The way in which potential NPs are obtained by the system leads us to suspect that there are many candidate terms in the validation list that the terminologist has to analyse. 2.10. TERMIGHT Reference publication: Dagan and Church (1994) Main goal: Translation aid Termight is currently used by A&amp;T Business Translation Systems. It was created to be a tool for automating some stages of the professional translator terminological research. To do so it starts with a tagged and disambiguated text as well as a list of predetermined syntactic patterns that could be adjusted to every document. Thus, a list of candidate terms is obtained comprising one or more words. Single-word candidates are defined as all those words that are not included in a previously determined list of empty words (i.e. stop list). Multiword terms are referred to one of the predetermined syntactic patterns via regular expressions. Dagan and Church (1994) considered only noun sequences patterns. Candidate terms are grouped and classified according to their lemma (i.e. the right hand side noun) and frequency. Those candidates sharing the same lemma are classified alphabetically in accordance with the inverse order of their compounding words. Thus it is showed the order of changes of the English simple NPs. For each candidate term the corresponding concordances are obtained, which are alphabetically classified according to their context. This information enables the terminologist to evaluate whether each candidate is appropriate or not. Dagan and Church (1994) note that the rate of term list construction is of 150 and 200 terms per hour, which is twice faster than the average. As for the extraction quality, they state that, unlike exclusively statistical methods, Termight permits to extract low-frequency terms. Moreover this system has a bilingual module which, via statistical methods, obtains a word-level alignment from texts. Thus terms found in language A are referred to their counterparts in language B. This well-ordered list of candidate terms is again passed on to the terminologist to be evaluated. 22 Automatic Term Detection: a Review of Current Systems The Termight bilingual module does not seem to be developed and tested as the basic one. Tests have been made on 192 terms from a technical manual in English and German. The correct translation is found in the first suggested solution in 40% of the cases, whereas only 7% corresponds to the correct translation suggested in the second place. As for the remaining, the correct translation was in other places of the proposal list. Evaluation Termight is a remarkable system in that there is an accurate classification and presentation of candidate terms and it does not attempt to become an automatic system. Rather, it helps the translator. However, it presents a number of shortcomings: (1) The only syntactic pattern considered is very simple: noun sequences. This pattern may well be valid for English but not for Romance languages and (2) no numerical information about the recognition quality is given. The type of pattern considered may suppose high precision but low recall 2.11. TERMINO Reference publication: Plante and Dumas (1989) Main goal: Facilitation of the term extraction terminographer’s task. The TERMINO program is composed of several tools to facilitate TE in French. It is a help for the terminologist insofar as the identification of those discourse units that denominate notions or objects. Besides it provides every unit with the immediate context from which data relevant to the notions denominated by theses units can be obtained. There are a number of TERMINO versions which improve in some ways previous ones. This tool is based mainly on linguistic knowledge and it comprises 3 subsystems: a pre-editor, which separates texts into words and sentences and identifies proper nouns, a morphosyntactic parser and a record-drafting facility. The text does not get any special treatment: it is only required to be codified in ASCII form. With regard to term delimitation and extraction the more interesting sub-system is the morphosyntactic parser. It consists of 3 modules: a morphological parser; a syntactic parser and a synapsy detector. The morphological parser has two functions: a) automatic categorisation; b) lemma and tag identification. According to Plante and Dumas, 30% of words in French can be attributed to more than one category. This has led to the tagging of all the possible categories for each word. As a result, there is an overproduction of words with different tags. Categorisation and lemmatisation are obtained from the application of the LCML program, it is not a dictionary but a morphological parser of lexical forms so it can correctly lemmatise and tag new lexical forms. AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 23 The syntactic parser is responsible for weeding out the vast majority of ambiguities generated in previous stages. It is managed through the construction of a syntactic structure for each sentence. Finally, the synapsy detector (MRSF) selects, among the syntactic units from the parser, those lexical noun units that are likely to be terms. S. David (David and Plante, 1991) created MRSF especially for TERMINO. MRSF is based on principles of noun group construction. David’s understanding of synapsy is that of a polylexical unit of a syntactic nature that is the head of the NP. Thus, synapsies are only NPs groups: some of them will become terms and some of them will not. Further, some of them will only be “topics” that will enable the terminologist to know different concepts or grasp an overview of the text topics. The MRSF module comprises 5 sub-modules: (1) head hunter module; (2) expansion recogniser module; (3) categorisation module; (4) synapsy generator module and (5) representation and evaluation module. TERMINO has a set of software tools, which is much larger and comprises different modules that allow to manipulate terminological data. These tools help the terminologist decide whether a synapsy is a term or not, elaborate terminological filing forms and create terminological databases. TERMINO recognises between 70% and 74% of the complex terms. The fact that 30% of terms are not recognised by TERMINO can be explained by coordination (it is a signal of segment breaking), acronyms and common nouns in capital letters. Moreover, there is 28% of noise, of which 47% is due to a wrong mark-up and a 53% is due to synapsies belonging to general language. Evaluation TERMINO is one of the first candidate term extractors that worked and it is a linguistically-based extractor, composed of different independent modules. This system is based on the concept of synapsy. The synapsy detector is based on the establishment of a number of heuristic rules that may well be increased provided the corpus is delimited. There is a need to improve this system taking into account that it is still too noisy (28%), which could be improved, for example, with a different treatment of capital letters and acronyms. 2.12. TERMS Reference publication: Justeson and Katz (1995) Main goal: Term extraction Justeson and Katz (1995) hold the following views about terms: a) Terminological noun phrases (TNP) are different from non-terminological noun phrases (nTNP) in that the modifiers of the first ones are much shorter than those of the second ones. 24 Automatic Term Detection: a Review of Current Systems b) An entity introduced by a nTNP can be later referred to only by the head of the NP and often by other NP (synonyms, hyponyms, hyperonyms). By contrast, an entity introduced by a TNP is normally repeated identically in a given document, as a single omission of a modifier could yield a change of the referred entity. c) In technical texts lexical NPs are almost exclusively terminological. d) Multiword technical terms are nearly always composed of nouns and adjectives (97%) and some prepositions (3%) between two NPs. e) The average length of a TNP is of 1.91 words. The proposed filter finds strings with a frequency equal or higher than two. These strings follow with this regular expression: ((A|N)+ | ((A|N)*(N P)?)(A|N)*N. Those candidate terms of a length of 2 (2 patterns: AN and NA) and 3 (5 patterns: AAN, ANN, NAN, NNN and NPN) are by far the most commonly encountered. The purpose of this algorithm is to combine good coverage of the usual terminology from technical texts with high quality in the extraction phase. The algorithm prefers quality to coverage, since if it only made use of the grammatical constraints then the system would propose many irrelevant NPs. The vast majority of relevant NPs overcome the frequency constraint. Selection of grammatical patterns also affects quality. If prepositions are admitted within the pattern many candidates are introduced, although few will be valid. As a result, quality decreases whereas quantity increases and, accordingly, Justeson and Katz (1995) prefer not to take prepositions into consideration. The implementation of grammatical patterns also affects the quality/coverage trade-off. There are two ways in which a given linguistic unit is attributed to a grammatical category: disambiguation and filtering. The first one is rejected because disambiguators are not totally reliable yet. Filtering consist in parsing and lemmatising each word of the text. Then those sequences following the pattern are considered. If a word is not identified as a noun, adjective or preposition, it is discarded. Thus each word maintains its nominal, adjectival and prepositional values and in this order. The chain is weeded out if more than one word can be identified as a preposition or if it does not follow the pattern (e.g. if the pattern ends with a noun and there is more than one preposition then the word following the preposition is not a noun). Filtering has a coverage at least as good as what can be attained by a standard tagger. However, quality is not that good (e.g. fixed is only identified as an adjective –bug fixed–, but it can also become a verb: fixed disk drive). In contrast, filtering is much faster than parsing. In any case, Justeson and Katz (1995) suggest to control the patterns, the list of grammatical words and the frequency to adjust the performance of the system to each type of text. AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 25 This system has been applied to different domains (metallurgy, spatial engineering and nuclear energy) and it is used at IBM Translation Center. The TERMS results are presented on the basis of 3 technical texts (statistical classification of patterns, lexical semantics and chromatography). Coverage has only been estimated for one of the text and it is of 71%. Quality has been estimated between 77% and 96% of the instances. Evaluation Although Justeson and Katz (1995) present a detailed study on the performance of terminological units (wherein there are some overstatements), the proposed filter does not seem to take advantage of these previous analyses of terms. Further, it should be noted that this type of filtering based on quite simple patterns would not be so efficient if they were applied to languages other than English such as Romance languages. Also, this kind of patterns produces a lot of noise. 3. Contrastive Analysis Here we will contrast the systems’ main features, according to six relevant aspects when designing a new detection system of terminological units: linguistic resources, strategies of term delimitation, strategies of term filtering, classification of recognised terms and obtained results. For some of these criteria we have created a table containing the most significant data so as to make the system comparison easier. Linguistic resources 3.1. It has been observed that the vast majority of the reviewed systems make use of some sort of linguistic information, at least a list of empty words taken as boundaries. The standard process includes a morphological analysis followed by some kind of disambiguation system. The systems altering this procedure are the following: a. ANA: does not use any linguistic resource, just a list of auxiliary words b. TERMS: use its own disambiguation system: POS filtering c. Naulleau: introduces semantic information Additionally, for the systems that use an incremental strategy, like ANA and Fastr, it is necessary a set of initial terms to bootstrap the process. 3.2. All systems of terminology extraction have to determine at some point the beginning and the end of the candidate term, that is, delimit the potential terminological unit. The reviewed programs have different strategies to delimit Strategies of term delimitation 26 Automatic Term Detection: a Review of Current Systems X X X Other X X X X statistical - - - - - - - - - structure disamb. learning Other X terms: word-boundary elements, structural patterns, syntactic parser, text distribution, typographical elements, term lists, structure disambiguation. Below we show a summary of the different options adopted by each system: Table 1: Strategies of term telimitation term delimitation Patterns Parser boundaries X X X X X X X System 1 2 3 4 5 6 7 8 9 10 11 12 3.3. Term filtering is a key stage of any term detection system. This means that the list of candidates is reduced as much as possible. The following table shows the strategies found in all the reviewed systems: Table 2: Strategies of term filtering Name /Author ANA CLARIT Daille FASTR Heid LEXTER Naulleau NEURAL NODALIDA-95 Termight TERMINO TERMS Strategies of term filtering Term Filtering Freq.10 Linguisti X X System Name /Author ANA 1 CLARIT 2 Daille 3 FASTR 4 Heid 5 LEXTER 6 Naulleau 7 NEURAL 8 NODALIDA-95 9 10 Termight 11 TERMINO 12 TERMS 10 The technique of term filtering through frequency terms has been considered something in between those methods based on linguistic knowledge and those methods based on extralinguistic knowledge. statistical + linguistic X linguistic + statistical X X X reference terms X X user defined X c X X X X X X AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 27 3.4. Classification of recognised terms Some of the analysed systems classify recognised terms by grouping them according to some criteria. Thus, the related terms stay close to each other. Even FASTR attempts to infer an ontology from the recognised terms. Those systems which show some classification of recognised terms are the following: a. ANA: it builds a semantic network from the detected terms. b. FASTR: it builds a graph to relate recognised terms. Also it proposes the construction of partial ontologies for some terms. c. LEXTER: it builds a terminological network splitting terms into head and expansion. 3.5. Results The table below summarises for each system the type of corpus used for the tests and the results attained: Table 3: Results Test corpora System Domain Aviation engineering 1 ANA Acoustics 2 CLARIT11 News Telecommunications 3 Daille Medicine (abstracts) 4 FASTR Engineering 5 Heid Engineering 6 LEXTER Technical 7 Naulleau Medicine 8 NEURAL 9 NODALIDA-95 Cosmology Technical text 10 Termight Computer science Medicine 11 TERMINO Statistics 12 TERMS Semantics Chromatography Terms % Language Size.[Kw.] precision French English ? English ? French 86.7 French German ? French 95 French ? ? English 95-98 English English ? French 72 77 English 86 96 recall ? ? 81.6 ? 74.9 ? ? ? 70 98.5-100 ? 70-74 120 25 240 Mb 800 1.560 35 3.250 ? 55 20 ? ? 2.3 6.3 14.9 Name /Author</body>
  <conclusion>We can reach some conclusions after having analysed and evaluated some of the main systems of TE designed in the last decade: 11 The system has been intensively tested with regard to the indexing frequency, but not in relation to the quality of the extracted terms. 28 Automatic Term Detection: a Review of Current Systems a) The efficiency of the extraction presents a high degree of variation from one to another. Broadly speaking, there is neither clear nor measurable explanation of the final results. Besides, we have to bear in mind that these systems are tested with small and highly specialised corpora. This lack of data makes it difficult to evaluate and compare them. However, it does not prevent pinpointing those solutions, which are considered valid to solve specific problems. b) None of the systems is entirely satisfactory due to two main reasons. First, all systems produce too much silence, especially statistically-based systems. Second, all of them generate a great deal of noise, especially linguistically-based systems. c) Taking into account the noise generated, all systems propose large lists of candidate terms, which at the end of the process have to be manually accepted or rejected. d) Most of the TE systems are related to only one language: French or English. Usually the language specific data is embedded in the tool. This makes difficult to use the system in a language other than the original. e) As has been already pointed out, training corpora tend to be small (from 2.3 to 12 Kwords) and highly specialised with regard to the topic as well as the specialisation degree. This allows for a quite precise patterns and lexicosemantic, formal and morphosyntactic heuristics albeit this only applies to highly specialised corpora. f) All systems focus entirely on NPs and none of them deals with verbal phrases. This is because there is a high rate of terminological NPs in specialised texts. This rate can vary according to the topic and the specialisation degree. Despite what has just been noted, it is noteworthy that all specialised languages have their own verbs (or specific combinations of a verbal nature), no matter how low the ratio is in comparison with nouns. g) As a result, none of the systems refers to the distinction between nominal collocations and nominal terminological units of a syntactic nature. Nor do they refer to phraseology. h) Many of the systems make use of a number of morphosyntactic patterns to identify complex terms. However they account for most of the terminological units they are still too few and also not very constraining. Thus, for English are AN and NN, for French NA and N prep N. Some terms present structures other than these ones and they are never detected. Those systems based only on these types of linguistic techniques generate too much noise. i) It is generally agreed that frequency is a good criterion to indicate that a candidate term is actually a terminological unit. However, frequency is not on its own a sufficient criterion, as it yields a great deal of noise. j) Only a few recent systems use semantic information to recognise and delimit terminological units although its use takes place at different levels. AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 29 k) None of the systems uses extensively the combinatory features of terms from specialised languages in relation to a given domain. It is needed more studies about the type of constraints that terminological units present with regard to conceptual field and text type. l) Only one of the analysed systems take profit of the possibilities given by the alignment of specialised text. i) Most of the authors consider the POS disambiguation as one of the most important error sources. However, they do not provide exact figures about its incidence degree. To improve these systems of terminology extraction and lessen the noise and silence that are generated, two type of studies should be encouraged. First, it is required more linguistic oriented studies on the semantic relationships among terms, the semantic relationships among constituents of a terminological unit, semantico-lexical representation, constraints of terminological units within a given specialised domain and in a given text type, all the grammatical categories that are likely to become terms in specialised domains, the influence of the syntactic function of terminological phrases on texts, the relationships between terms and their arrangement in texts. Second, we should focus on software systems that: combine in a more active manner statistical and linguistic methods; improve statistical measures; combine more than one strategy; are easily applicable to more than one language; improve interfaces to facilitate the machine-user interaction. Also it should be very useful, as suggested in Kageura et al. (1998), the development of a common test bench for aiding the evaluation/comparison of extracting methods. In sum, should we progress in the field of automatic terminology extraction, statistical and linguistic methods have to actively be combined. It means that they are not either-or approaches but complementary ones. The final goal is to reduce the amount of silence and noise so that the process of terminological extraction becomes as automatic and precise as possible. In the future, we believe that any current terminology extractor, apart from accounting for the morphological, syntactic and structural aspects of terminological units, has to necessarily include semantic aspects if the efficiency of the system is to be improved with regard to the existing ones.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Arppe, A. 1995. “Term extraction from unrestricted text”. Lingsoft Web Site: http://www.lingsoft.com Ahmad, K., Davies, A., Fulford, H. and Rogers, M. 1992. “What is a term? The semiautomatic extraction of terms from text”. Translation Studies – an interdiscipline. Amsterdam: John Benjamins. Bourigault, D. 1994. LEXTER, un Logiciel d'EXtraction de TERminologie. Application à l'acquisition des connaissances à partir de textes. PhD Thesis. Paris: École des Hautes Études en Sciences Sociales. Bourigault, D., Gonzalez-Mullier, I. and Gros, C. 1996. “LEXTER, a Natural Language Processing Tool for Terminology Extraction”. Proceedings of the 7th EURALEX International Congress. Göteborg. Brown, P. F., Cocke, F., Pietra, S., Felihek. F., Merces, R. and Rossin, P. (1988) A statistical approach to language translation. Procedings of 12th International Conference of Computational Linguistic (Coling-88). Budapest, Hungary. Cabré, M.T. 1999. Terminology. Theory, methods and applications. Amsterdam: John Benjamins. Church, K. 1989. “Word association norms, mutual information and lexicography”. Proceedings of the 27th annual meeting of the ACL. Vancouver, 76-83. Condamines, A. 1995. “Terminology: new needs, new perspectives”. Terminology, 2, 2: 219-238. Dagan, I. and Church, K. 1994. “Termight: Identifying and translating technical terminology”. Proceedings of the Fourth Conference on Applied Natural Language Processing, 34-40. Daille, B. 1994. Approche mixte pour l'extraction de terminologie: statistique lexicale et filtres linguistiques. PhD dissertation. Paris: Université Paris VII. Daille, B. and Jacquemin, C 1998. “Lexical database and information access: a fruitfull association?”. First International Conference on LREC. Granada. AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 31 David, S. and Plante, P. 1991. “Le progiciel TERMINO: de la nécessité d'une analyse morphosyntaxique pour le dépouillement terminologique des textes”. Proceedings of the Montreal Colloquium Les industries de la langue: perspectives des années 1990, 1: 71-88. Enguehard, C. and Pantera, L. 1994. “Automatic Natural Acquisition of a Terminology”. Journal of Quantitative Linguistics, 2, 1: 27-32. Estopà, R. 1999. Extracció de terminologia: elements per a la construcció d’un SEACUSE (Sistema d’extracció automàtica de candidats a unitats de significació especialitzada). PhD thesis, Barcelona: Universitat Pompeu Fabra. Estopà, R. and Vivaldi, J. 1998. “Systèmes de détection automatique de (candidats à) termes: vers une proposition intégratrice”. Actes des 7èmes Journées ERLA-GLAT, Brest, 385-410 Evans, D.A. and Zhai, C. 1996. “Noun-phrase Analysis in Unrestricted Text for information retrieval”. Proceedings of ACL, Santa Cruz, University of California, 17-24. Frantzi, K. and Ananiadou, S. 1995. Statistical measures for terminological extraction. Working paper of the Department of Computing of Manchester Metropolitan University. Frantzi, K. T. 1997. “Incorporating context information for extraction of terms”. Proceedings of ACL/EACL, Madrid, 501-503. Habert, B., Naulleau, E. and Nazarenko, A. 1996. “Symbolic word clustering for medium-size corpora”. Proceedings of Coling’96: 490-495. Heid, U., Jauss, S., Krüger, K. and Hohmann, A. 1996. “Term extraction with standard tools for corpus exploration. Experience from German”. In: TKE ‘96: Terminology and Knowledge Engineering,, 139-150. Berlin: Indeks Verlag. Jacquemin, C. 1994. “Recycling Terms into a Partial Parser”. Proceedings of ANLP’94, 113-118. Jacquemin, C. 1999. “Syntagmatic and paradigmatic representations of term variation”. Proceedings of ACL'99, University of Maryland, 341-348. Jacquin, C. and Liscouet, M. 1996. “Terminology extraction from texts corpora: application to document keeping via Internet”. In: TKE ‘96: Terminology and Knowledge Engineering, 74-83. Berlin: Indeks Verlag. Justeson, J. and Katz, S. 1995. “Technical terminology: some linguistic properties and an algorithm for identification in text”. Natural Language Engineering, 1, 1: 9-27. Kageura, K. and Umino, B. 1996. “Methods of Automatic Term Recognition”. Papers of the National Center for Science Information Systems, 1-22. 32 Automatic Term Detection: a Review of Current Systems Kageura, K., Yoshioka, M., Koyama, T. and Nozue, T. 1998. “Towards a common testbed for corpus-based computational terminology”. Proceedings of Computerm ‘98, Montreal, 81-85. Karlsson, F. 1990. “Constraint grammar as a framework for parsing running text”. Proceedings of the 13th International conference on computational linguistic, 3: 168-173. Lauriston, A. 1994. “Automatic recognition of complex terms: Problems and the TERMINO solution”. Terminology, 1, 1: 147-170. Maynard, D. and Ananiadou, S. 1999. “Identifying contextual information for multi-word term extraction”. In: TKE ‘99: Terminology and Knowledge Engineering, 212-221. Vienna: TermNet. Nakagawa, H. and Mori , T. 1998. “Nested collocation and Compound Noun for Term Extraction”. Proceedings of Computerm ’98, Montreal, 64-70. Naulleau, E 1998. Apprentissage et filtrage syntaxico-sémantique de syntagmes nominaux pertinents pour la recherche documentaire. PhD thesis. Paris: Université Paris 13. Naulleau, E. 1999. “Profile-guided terminology extraction”. In: TKE‘99: Terminology and Knowledge Engineering. 222-240. Vienna: TermNet. Plante, P. and Dumas, L. 1998. “Le Dépoulliment terminologique assisté par ordinateur”. Terminogramme, 46, 24-28. Shieber, S.N. 1986. “An Introduction to Unification-Based Approaches to grammar”. CSLI Lecture Notes of University Press, 4. Smadja, F. 1991. Extracting collocations from text. An application : language generation. Columbia: Columbia University. Department of Computer Science. [Unpublished doctoral dissertation] Voutilainen, A. 1993. “NPtool, a detector of English noun phrases”. Proceedings of the Workshop on Very Large Corpora. Zhai, C., Tong, X., Milic-Frayling, N. and Evans, D.A. 1996. “Evaluation of syntactic phrase indexing CLARIT. NLP track report”. Proceedings of the TREC-5. TREC Web Site: http://trec.nist.gov/pubs/trec5/t5_proceedings.html</biblio>


  <preamble>IPM1481.pdf</preamble>
  <titre>A hybrid approach to managing job offers and candidates</titre>
  <auteurs>
    <auteur>
      <name>Remy Kessler</name>
      <mail>remy.kessler@univ-avignon.fr</mail>
      <affiliation>LIA/Université d’Avignon et des Pays de Vaucluse, 339 chemin des Meinajariès, 84911 Avignon, France</affiliation>
    </auteur>
    <auteur>
      <name>Nicolas bechet</name>
      <mail>nicolas.bechet@inria.fr</mail>
      <affiliation>INRIA Domaine de Voluceau, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
    </auteur>
    <auteur>
      <name>Mathieu roche</name>
      <mail>mathieu.roche@lirmm.fr</mail>
      <affiliation>LIRMM, CNRS Université Montpellier 2, 161 rue Ada, 34392 Montpellier, France</affiliation>
    </auteur>
    <auteur>
      <name>Juan-Manuel Torres-Moreno</name>
      <mail>juan-manuel.torres@univ-avignon.fr</mail>
      <affiliation>École Polytechnique de Montréal, CP 6079, succ. Centre-ville, Montréal (Québec) Canada H3C 3A7</affiliation>
    </auteur>
    <auteur>
      <name>Marc El-Bèze</name>
      <mail>marc.elbeze@univ-avignon.fr</mail>
      <affiliation>LIA/Université d’Avignon et des Pays de Vaucluse, 339 chemin des Meinajariès, 84911 Avignon, France</affiliation>
    </auteur>
  </auteurs>
  <abstract>The evolution of the job market has resulted in traditional methods of recruitment becoming insufficient. As it is now necessary to handle volumes of information (mostly in the form of free text) that are impossible to process manually, an analysis and assisted categorization are essential to address this issue. In this paper, we present a combination of the E-Gen and CORTEX systems. E-Gen aims to perform analysis and categorization of job offers together with the responses given by the candidates. E-Gen system strategy is based on vectorial and probabilistic models to solve the problem of profiling applications according to a specific job offer. CORTEX is a statistical automatic summarization system. In this work, E-Gen uses Cortex as a powerful filter to eliminate irrelevant information contained in candidate answers. Our main objective is to develop a system to assist a recruitment consultant and the results obtained by the proposed combination surpass those of E-Gen in standalone mode on this task. (cid:2) 2012 Elsevier Ltd. All rights reserved.</abstract>
  <introduction>The evolution of the job market has resulted in that traditional methods of recruitment becoming insufficient. The Internet has introduced a new way of managing human resources. Theoretically, shifting job search and recruitment activ-ities to the Internet improves the quality of job matching by reducing search costs, increasing contact opportunities and rationalizing the screening process of job applicants (Marchal, Mellet, Rieucau, 2007). Over the last few years, there has been a significant expansion of online recruitment (e.g. August 2003: 177,000 job offers, May 2008: 500,000 job offers). The Internet has become essential in this process because it allows a better flow of information, either through job search sites or by e-mail exchanges. Nowadays, job seekers can send their curriculum vitae (CV) directly to companies (by e-mail or uploaded to dedicated servers on the Web). The job search task is becoming easier and less time consuming. The Internet makes every user a potential job seeker. Employees may be constantly in search of new career opportunities and job candidates may provide more interaction than can be managed efficiently by companies (Bourse, LeclFre, Morin, Trichet, 2004). As intellectual capital has become one of the most strategic assets of successful organizations in the last decade, the capability of managing people’s expertise, skills and experience represents a key factor in facing up to the increasing competitiveness of the global market (Colucci et al., 2003). Even though a browser has become a universal and easy tool for users, they frequently have to enter data into Web forms from paper sources and the need to ‘‘copy and paste’’ data between different applications is symptomatic of the issues of data integration. In this context, electronic recruitment tends to automate matching between the published information about the candidates and job offers. The Laboratoire Informatique d’Avignon (LIA), 2 the Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier(LIRMM), 3 and Aktor Interactive 4 are developingthe E-Gen system to resolve this issue. E-Gen is a Natural Language Processing (NLP) and Information Retrieval (IR) system composed of three main modules: 1. The first one extracts the information from a corpus of e-mails of job offers from Aktor’s database. 2. The second module analyses the candidate’s answers (i.e. splitting e-mails into cover letter (CL) and curriculum vitae). 3. The third module analyses and computes a relevant ranking of the candidate’s answers. Our first work (Kessler, Torres-Moreno, El-Bèze, 2007) presented the first module: the identification of different parts of a job offer and the extraction of relevant information (type of contract, salary, localization, etc.). The second module analyses the content of a candidate’s e-mail, using a combination of rules and machine learning methods (Support Vector Machines, SVM) and was presented in Kessler, Torres-Moreno, and El-Bèze (2008b). Furthermore, it separates the distinct parts of CV and CL with a precision of 0.98 and a recall of 0.96. Reading a large number of candidate answers for a job is a very time consuming task for a recruiting consultant. In order to facilitate this task, we propose a system capable of providing an initial evaluation of candidate answers according to various criteria. We do not seek the best or even a good candidate as no scoring is involved, but simply a candidate who has a close application to those already selected. Our previous work (Kessler, Béchet, Roche, El-Bèze, Torres-Moreno, 2009) presented an approach based on a process of relevance feedback, permitting a reinforcement learning (Sutton Barto, 1998). In this paper, we present an original combination of the E-Gen and CORTEX systems. Each document contains a number of additional information, present in many applications and which is partially removed by classical pre-processing. Each application added by the process of relevance feedback adds relevant information but also multiplies additional information. CORTEX allows us to filter these sentences and keep only the most relevant sentences at the evaluation step. Some related studies are briefly discussed in Section 2. Section 3 shows a general system overview. In Section 4, we describe the E-Gen pre-processing task, the strategy used to rank the candidateanswers with relevance feedback and the coupling of E-Gen with the CORTEX summarization system. In Section 5, we present statistics.</introduction>
  <body>Many approaches have been proposed in the literature to reduce the costly and tedious task of managing human resources. Candidate answers to a job offer come as ad hoc documents, and require semantic approaches to analyse them. The BONOM system is based on an indexing method (Morin, LeclFre, ; Trichet, 2004; Cazalens ; Lamarre, 2001). This method consists in using distributional attributes of documents to locate each part for the final indexation of the document. A semantic-based method to select candidate answers and to discuss the economic impacts on the German government was proposed by Tolksdorf, Mocho, Heese, Oldakowski, and Christian (2006). In the same way (Gorenak ; Mlaker KaF, 2010), perform a comparison between Slovenian, German, and British online job advertisements (ads). More recently (Marchal et al., 2007), present a comparison between French and English job search sites and newspapers as well as the various shortcoming of current matching systems. They propose a comparative analysis of job offers posted on the Internet with those posted in newspapers and they observe that search engine toolkits have a considerable impact on ad content which is generally more standardized and quantified than before. Mocho, Paslaru, and Simperl (2006) discuss the relevance of a common ontology (HR ontology) to work efficiently with this kind of document. Using the same model (Dorn ; Naz, 2007), outline a HR-XML based prototype dedicated to the job search task. The prototype selects and favors relevant information (paycheck, topic, abilities, etc.) from many job-service websites, such as Jobs.net, aftercollege.com, Directjobs.com, etc. Bourse et al. (2004) describe an efficient model and a management tool used for the selection of candidate-answers. They propose a prototype job portal which uses semantically annotated job offers and applicants to obtain a more accurate job search with query approximation. The limitations of current systems for automatic selection of candidate answers are presented in Rafter, Bradley, and Smyt (2000). They propose a system based on collaborative filters (ACF) to automatically select profiles of candidate answers on the Jobfinder website. Enrica and Iezzi (2006) present a model for ranking skills in the field of information technology in Italy with multidimensional scaling and cluster analysis. In the same way, Colucci et al. (2003) present a semantic based approach to the issue of skills detection in an ontology supported framework. Based on Description Logics formalization and reasoning, they propose a skill matching approach with contradiction matches and partial matches between skill profiles. Loth et al. 2 http://www.lia.univ-avignon.fr. 3 http://www.lirmm.fr. 4 A French recruitment agency specialized in recruiting on the internet, (http://www.aktor.fr). Author's personal copy 1126 R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 (2010) combine, through the SIRE project (Semantics-Internet-Recruitment-Employment) a linguistic approach and machine learning methods to perform an extraction of key terms of job ads in order to improve the categorization of each job offer. The study of the most relevant document – the CV – to use it automatically has been a major subject of research. Ben Abdessalem Karaa (2009) presents a system for analyzing and structuring CVs with an extension of General Architecture of Text Engineering (GATE5). They obtain good results in precision/recall for each part of the document (personal information, experience, skill, and so forth) on a small corpus of CVs in French. Yahiaoui, Boufaı¨da, and Prié (2006) provide a semantic approach to generating some annotations of CVs and job offers with the help of a specialized ontology to match graduates and the level of a job offer. They present interesting results on a sample of data. Clech and Zighed (2003) propose a data mining approach. Their aim is to build automats which recognize CV topologies and candidate/job offer profiles. A first step differentiates the CV of employed executives from other CV. They use a specific term extraction to obtain a categorization with the C4.5 decision tree algorithm (Quilan, 1993). This method focuses on the specificity of selected terms or concepts, such as education level or relevant abilities, to build a classifier. The results of this method are still poor (an accuracy between 0.5–0.6 of correctly categorized CV). Roche and Kodratoff (2006) and Roche and Prince (2008) have made a terminology study of corpus composed of CVs (of the Vediorbis company (http://www.vediorbis.com)). Their approach extracts collocations from a CV corpus based on syntactic patterns such as Noun-Noun, Adjective-Noun, etc. Then, these collocations are ranked according to relevance to build a specialized ontology. There are few studies on the treatment of the cover letter. Audras and Ganascia (2006) use cover letters to detect the usual errors in the field of acquisition of written French as a foreign language. The approach proposed is the detection of syntactic patterns particular to a group of learners, and which are absent or little used among native speakers. The study focuses in part on cover letter writing. Among the innovative solutions on the market, Twitter6 has launched the job search site http://www.twitterjobsearch.com based on the concept of short messages (less than 140 characters) and ZaPoint7 with an original solution, SkillsMapper, which transforms each CV into graphic format with various curves (training, education, etc.). In this paper, we present an approach to the application ranking by using a combination of similarity measures, relevance feedback and summaries of a CV and CL. Our approach is distinguished from other work by a purely statistical approach as well as reinforcement learning through the process of relevance feedback. 3. System overview Nowadays technology proposes new approaches to the online employment market. E-Gen is a system which meets this challenge as fast and judiciously as possible. We chose emails as the input format, which is the most frequent mode of communication in this field. An e-mail inbox receives messages sometimes with an attached file containing the job offer. When a job offer is published online, a particular segmentation is required by the job search sites. firstly, the job offer language is identified by using n-grams. Then, E-Gen parses the e-mail, splits the job offer into thematic segments, and retrieves relevant information (contract, salary, starting date, location, etc.) to generate an XML document for the job offer. Subsequently, a filtering and lemmatisation process is applied to the text, and is represented in a vector space model (VSM). A categorization of text segments (preamble, skills or profile, mission) is obtained by using a SVM classifier (Fan, Chen, ; Lin, 2005). This preliminary classification is then transmitted to a ‘‘corrective’’ post-process which improves the quality of the solution (Module 1, described in Kessler et al., 2007). Preliminary experiments showed that segment categorization without segment position in job posting is not enough and may be a source of errors. In order to avoid this kind of error, we have decided to consider each job posting as produced by a succession of states in a Markov machine and we have applied a post-processing, based on the Viterbi algorithm (Viterbi, 1967). During the publication of a job offer, Aktor generates a temporary e-mail address for applying to the job. Each e-mail is redirected to human resources software (Gestmax8) to be read by a recruiting consultant. At this step, E-Gen analyses the candidate’s answers to identify each part of the application and extracts the text from the e-mail and attached files (by using wvWare9 and pdftotext10). After a pre-processing task, we use a combination of rules and machine learning methods to separate each distinct part (CV or CL). We use a vector representation of each document with a label (CV or CL). With a learning set of 2.000 documents of each type, the system gets very good performance (F-score between 0.95 and 0.98). This process (Module 2 represented by the lowest box in fig. 1) is more fully described in Kessler et al. (2008b). Once the CL and CV have been identified, the CORTEX system is applied to each document (Cover Letter and CV) and a summary is generated by concatenating high-scoring sentences. Afterwards, E-Gen performs an automated profiling of this application by using measures of similarity and a small number of applications that have been previously validated as relevant by a recruitment consultant (Module 3). The whole chain is summarized in fig. 1. 5 http://gate.ac.uk/. 6 http://twitter.com. 7 http://www.zapoint.com. 8 http://www.gestmax.fr. 9 http://wvware.sourceforge.net. 10 http://www.bluem.net/downloads/pdftotext_en. Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 1127 companies Internet Candidate Job offer publication Title Description Mission Profile LIA CORTEX System CL CV Job offer processing Module 1 Splitting candidate’s e-mails Module 2 fig. 1. System overview. Profiling Module 3 candidatures ranking Relevance Feedback 4. Coupling E-Gen profiling module and the CORTEX system 4.1. E-Gen profiling module 4.1.1. Linguistic pre-processing firstly, we remove information such as e-mail adresses, the names of candidates, addresses, names of cities in order to ensure that the applications become anonymous. Then, classic pre-processing is applied to textual information (job offer, CV, and CL). French accents are deleted and capital letters are converted to lower case. This pre-processing task is performed to obtain a representation well suited for the Vector Space Model (VSM). In order to avoid the introduction of noise into the models, the following items are also deleted: verbs and functional words (to be, to have, to need, etc.), common expressions with a stop word11 list (for example, that is, each of, etc.), numbers (in numeric and/or textual format), symbols such as ‘‘$’’, ‘‘#’’, ‘‘⁄’’. finally, lemmatisation12 is performed to significantly reduce the size of the lexicon. All these processes allow us to represent the collection of documents through the bag-of-words paradigm (a matrix of frequencies of terms (columns) for each candidate answer (rows)). To improve filtering, we tried parsing applications with different significant terms (like ‘‘Personal Information’’, ‘‘Education’’, ’’Work Experience’’, etc.) and extract only paragraphs with the relevant information, but initial tests showed a decline in results due to the great variability of signifiant terms and order of paragraphs. 4.1.2. Proximity between applications and job offer using similarity measures After the step of linguistic pre-processing, each document is transformed into a vector with weights characterizing the frequency of terms Tf. Some tests with Tf-idf (Salton ; Mcgill, 1986) were made but they offered no improvement. We have established a strategy using measures of similarity, to rank all applications in relation to a job offer. We combined different similarity measures between the candidate’s answers (CV and CL) and the associated job offer. We decided to use several similarity measures as defined in Bernstein, Kaufmann, Kiefer, and Bnrki (2005): Cosine (Eq. (1)), which calculates the angle between job offer and each candidate answer, Minkowski distances (Eq. (2)) (p = 1 for Manhattan, p = 2 for Euclidean). The last measure used is Okabis (Eq. (3)) (Bellot ; El-Bèze, 2001). Based on the formula of Okapi (Robertson, Walker, Jones, Hancock-Beaulieu, ; Gatford, 1994), this measure is often used in Information Retrieval. To combine these measures, we use an Algorithm Decision (AD) (Boudin ; Torres Moreno, 2007), which weights the values obtained by each measure of similarity. Several other similarity measures (Overlap, Enertex, Needleman-Wunsch, Jaro-Winkler, Jensen-Shannon divergence) have been tested but they are not retained in this study, because the results obtained were disapointing. All measures used and their combinations are described in Kessler, Béchet, Roche, El-Bèze, and Torres-Moreno (2008a). 11 http://sites.univ-provence.fr/veronis/donnees/index.html. 12 Lemmatisation finds the root of verbs and transforms plural and/or feminine words into masculine singular form. So we conﬂate terms developer, development, developing, to develop into develop. Author's personal copy 1128 (cid:4)1 p ð1Þ cosineðj; dÞ ¼ R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 Pn ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ q i¼1ji (cid:2) di i (cid:2)Pn Pn i¼1d2 i¼1j2 i (cid:3) 1 þ Pn 1 Minkowskiðj; dÞ ¼ i¼1jji (cid:3) dijp Pn Okabisðj; dÞ ¼ P ﬃﬃﬃﬃjdj i¼1ji (cid:2) di Pn p i¼1ji (cid:2) di þ i2d\j Md where j is a job offer, d is a candidate answer, i a term, ji and di occurrence of i respectively in j and d, and Md their average size. ð2Þ ð3Þ 4.1.3. Relevance Feedback We previously changed the system to incorporate a process of Relevance Feedback (Sparck Jones, 1970). Relevance Feedback is a standard method used particulary for manual query reformulation. For example, the user carefully checks the answer set resulting from an initial query, and then reformulates the query. Rocchio’s algorithm (Rocchio, 1971) and variations have found wide usage in Information Retrieval and related areas such as Text Categorisation (Joachims, 1997). Relevance Feedback has been proposed in Smyth and Bradley (2003) to help the user to find a job with server logs from the jobfinder site.13 In our system, Relevance Feedback takes into account the recruiting consultant’s choice during a first evaluation of a few CVs. Our goal is not a system capable of finding the best candidate, but a system capable of reproducing the judgement of the recruitment consultant. It is critical for recruiters not to miss a promising candidate that they may have unfortunately rejected. The goal of this Relevance Feedback approach is to help them to avoid this kind of error. We assume that successful candidates have similar profiles or, at least, that they have much in common. This approach uses documents returned in response to a first request to improve the search results (Salton ; Buckley, 1990). In this case, we randomly take a few candidate answers (1–6 in our experiments) from all relevant candidate answers. These selected candidate answers are added to the job offer. So, we use manual Relevance Feedback to reﬂect user judgements in the resulting ranking. We increase the vector representation with the terms from the candidates considered relevant by a recruitment consultant. The system will recompute the similarity between the candidate’s answer that we evaluate and the job offer enriched with relevant candidates. This allows Sim0 to be recalculed for each measure of similarity between the application evaluated and the job offer expanded by relevant applications of the relevance feedback process: Sim0 measureðj; dÞ ¼ Simmeasureðj; dkp1k(cid:2)(cid:2)(cid:2)kpnÞ ð4Þ where j is a job offer, d is a candidate’s response, pi is a relevant candidate’s response, n are numbers of retained applications for Relevance Feedback and k is the concatenation operator. The results, presented in Kessler et al. (2009) and hereafter called ISMIS Result showed an improvement in the quality of the ranking obtained for each application added to the process of relevance feedback. However, we suspected that a lot of unnecessary information was still kept in the evaluation and we wanted to use a filter to take into account the content of sentences. Each document contains additional information (hobbies, greeting and complimentary close, etc.) and standard pre-processing only partially removes it. The idea was to use a system of automatic summarization, coupled to E-Gen, as a powerful filter capable of removing non-essential information contained in CV and Cover Letters. 4.2. The CORTEX summarization system Automatic summarization is useful to cope with ever increasing volumes of information. An abstract is, by far, the most concrete and recognized kind of text condensation. However, the CV is already a kind of summary, with a very important structure. We suspect that the filtering system of automatic summarization may not be useful in this case. Since the CL is in free text, we used CORTEX (Torres-Moreno, St-Onge, Gagnon, El-Bèze, ; Bellot, 2009, 2001), an efficient state-of-art summarization system, in order to retain the more informative segments of the CL. Each document of the application is transmitted to the CORTEX system which provides a summary based on the requested size. CORTEX is a document extract summarization system using an optimal decision algorithm that combines several metrics. These metrics result from processing statistical and informational algorithms on the document vector space representation. fig. 2 presents an overview of the system. The idea is to represent the text in an appropriate vectorial space and apply numeric processings to it. In order to reduce complexity, a pre-processing of the document is performed: words are filtered, lemmatized, and stemmed. Based on the terms that remain in the text after filtering, a frequency matrix c is built in the following way: Each element cl i of this matrix represents the number of occurrences of the word i in the sentence l. 13 Jobfinder (http://www.jobfinder.com). Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 1129 ; ð5Þ c ¼ fig. 2. CORTEX overview. c1 2 c2 2 . .. cl 2 . .. cNS 2 i 2 f0; 1; 2; . . .g cl . . . c1 NL . . . c2 NL . . . . .. . . . cl NL . . . . .. . . . cNS NL c1 . . . i c2 . . . i . . . . .. cl . . . i . . . . .. . . . cNS i ) 3 2 7777777777775 6666666666664 c1 1 c2 1 . .. cl 1 . .. cNS 1 ( Another matrix n, called a binary virtual or presence matrix, is defined as: i ¼ 1 ð6Þ nl Each line of these matrices represents a sentence of the text. Matrices c and cT are the frequency matrix of the sentences and frequency matrix of the titles respectively. The CORTEX system can use up to C = 11 metrics (Torres-Moreno, Velazquez-Morales, ; Meunier, 2002) to evaluate the sen i – 0 0 elsewhere if cl (cid:5)(cid:5) (cid:5)(cid:5) > 0:5 kv s (cid:5)(cid:5) (cid:5)(cid:5) < 0:5 kv s tence’s relevance. The system scores each sentence with a decision algorithm which relies on the normalized metrics. Two averages are calculated, a positive ks > 0.5, and a negative ks < 0.5 tendency (the case ks = 0.5 is ignored). The following algorithm combines the vote of each metric: (cid:5)(cid:5) (cid:5)(cid:5) (cid:3) 0:5 (cid:4); (cid:3) a ¼ PC Ps kv s (cid:5)(cid:5) (cid:5)(cid:5) (cid:4); (cid:3) b ¼ PC Ps v¼1 0:5 (cid:3) kv s v¼1 (cid:7) (cid:6) Cis the number of metrics and v is the index of the metrics. The value given to each sentence s is calculated with: if Ps a >Ps b ¼ 0:5 þPsa=C: retain s ¼ 0:5 (cid:3)Psb=C: not retain s then Scorecortex s else Scorecortex s Author's personal copy 1130 R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 The sentences are then ranked according to the obtained values. Depending on the desired compression rate, the sorted sentences will be used to produce the summary. The CORTEX system is applied to each document (Cover Letter) and a summary is generated by concatenating high-scoring sentences. We generated several abstracts with a variable compression rate (5%, 10%, 20%, . . ., 50%, 75% of the size of the documents, in sentences) in order to test the impact of our powerful filter on the E-Gen system. The entire process chain is illustrated in fig. 1. The best compression rates are generally with 30% (TorresMoreno et al., 2009). The results are presented in Section 5.3. 5. Experiments We selected a data subset from Aktor’s database composed of 1917 candidates. This subset is called the Mission Corpus. It has a size of 10 MB of raw texts and contains 1,375,000 words. The Mission Corpus is composed of a set of 12 job offers covering various themes (jobs in accountancy, business, computer science, etc.) and their candidates. Each Job Offer is associated with at least six candidates identified as relevant. As described in Kessler et al. (2008a), each document is segmented to keep the relevant parts (we remove the description of the company (D) for the job offer). Each candidate answer is tagged as relevant or irrelevant. A relevant value corresponds to a potential candidate for a specific job chosen by the recruiting consultant. An irrelevant value is associated with an unsuitable candidate for the job (this is a decision made by the manager of a human resources company). Our study was conducted on French job offers because the French market represents Aktor’s main activity. Table 1 shows a few statistics about the Mission Corpus. 5.1. Example of CL summaries fig. 3 presents14 an example of an original Cover Letter and fig. 4. Its corresponding summary15generated by the CORTEX sys tem with a 30% compression rate (in number of sentences). All the documents of Mission Corpus were previously made anonymous. We observe that the original CL contains a number of useless information for ranking, such as addresses, phone numbers or form of address at the beginning or end of the letter. The last part of the CL is generally as ‘‘Yours faithfully’’, ‘‘Yours sincerely’’, ‘‘Best regards’’, all of which represent irrelevant information. We further observe in fig. 4 that the summary obtained with CORTEX removes all this information. 5.2. Experimental protocol We measured the similarity between a job offer and its candidate’s responses. These measures (Section 4.1.2) rank the candidate’s answers by computing a similarity between a job offer and the associated candidate answers. We use the ROC curves to evaluate the quality of the ranking obtained. ROC curves (Ferri, Flach, ; Hernandez-Orallo, 2002) come from the field of signal processing. They are used in medicine to evaluate the validity of diagnostic tests. In our case, ROC curves show the rate of irrelevant candidate answers on the X-axis and the rate of relevant candidate answers on the Y-axis. The 14 Pierre ASPRE 26 years old 19 Verdun street 92870 Vannes 06-06-06-06-06. Subject: collaboration offer Vannes, November 27th, 2008 Dear Sir, The Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in tax and social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices and knowing how to deal with the late settlement of invoices. Therefore I offer my skills. They allow me to: – Manage with rigueur the accounts of a company. – Ensure legal compliance activities (payroll, tax billing etc.). – Provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis. I suggest we meet to discuss all the terms of our future cooperation. I look forward to hearing from you. Best regards. Pierre ASPRE. 15 Pierre ASPRE Subject: collaboration offer The Accountant is a key player not only for the proper functioning of the enterprise, but also in increasing profitability. With his legal knowledge in tax and social issues, he can make substantial savings: he is a key player for maintaining a cash reserve by ensuring the payment of customer invoices and knowing how to deal with late settlement of invoices. – ensure legal compliance activities (payroll, tax billing etc.). – provide advice particularly important in times of assessment, all thanks to my seriousness, my strength and my analysis. Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 1131 Table 1 Mission corpus statistics. Job title Number Number of candidate answers 34861 31702 33633 34865 34783 33746 33553 33725 31022 31274 34119 31767 Total Sales engineer Accountant, department suppliers Sales engineer Accountant assistant Accountant assistant 3 chefs Trade commissioner Urban sales consultant Recruitment assistant Accountant assistant junior Sales assistant Accountant assistant junior 40 55 65 67 108 116 117 118 221 224 257 437 1917 Number of Relevant 14 23 18 10 9 60 17 43 28 26 10 51 323 Irrelevant 26 32 47 57 99 56 100 75 193 198 247 386 1594 fig. 3. Example of full Cover Letter. fig. 4. Summary of Cover Letter (see fig. 3) at a 30% compression rate. Area Under the Curve (AUC) can be interpreted as the effectiveness of a measurement of interest. In the case of candidate answers ranking, a perfect ROC curve corresponds to obtaining all relevant candidate answers at the beginning of the list and all irrelevant ones at the end. This situation corresponds to AUC = 1. The diagonal line corresponds to the performance of a random system, progress of the rate of relevant candidates being accompanied by an equivalent degradation in the rate of irrelevant candidates. This situation corresponds to AUC = 0.5, as explained in Fawcett (2006). An effective measurement Author's personal copy 1132 R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 Table 2 Results of CL or CV according to the compression rate of Cortex and part of job offer (with or without Description part). CORTEX compression rate (%) 100 (full text) 75 50 40 30 20 10 5 CV + DTMP 0.622 0.565 0.558 0.552 0.549 0.520 0.559 0.550 CV + TMP 0.648 0.575 0.569 0.565 0.560 0.558 0.559 0.542 CL + DTMP 0.567 0.563 0.553 0.561 0.569 0.564 0.543 0.521 CL + TMP 0.560 0.556 0.560 0.565 0.571 0.566 0.554 0.523 CORTEX compression rate (%) Table 3 Results for CV and cover letter according to the compression rate. CV and CL summaries DTMP 0.634 0.521 0.556 0.544 0.570 0.569 0.564 0.546 100 (full text) 75 50 40 30 20 10 5 Full CV and CL summary DTMP 0.634 0.639 0.643 0.643 0.646 0.641 0.631 0.638 TMP 0.642 0.641 0.649 0.651 0.653 0.652 0.645 0.649 TMP 0.642 0.581 0.551 0.568 0.587 0.533 0.534 0.547 of interest to order candidate’s answers consists in obtaining the highest AUC value. This is strictly equivalent to minimizing the sum of the ranks of the relevant candidate’s answers. ROC curves are resistant to imbalance (for example, an imbalance in the number of positive and negative examples) (Roche ; Kodratoff, 2006). For each job offer, we evaluated the quality of the ranking obtained by this method. Candidate answers considered are only those composed of CV and CL. 5.3. Results In this section, we present the results obtained by combining the CORTEX system with the E-Gen ranking application. CORTEX was used as an additional filter which generates a summary of each document before E-Gen evaluation. We keep the structure of data for job offers as described in Kessler et al. (2008a). A job offer is composed of a Description (D), a Title (T), a Mission (M), and a Profile (P). For these experiments, we use two combinations of a job offer content, keeping only Title, Mission, Profile (TMP) and all information of a job offer (DTMP). Results are presented in Tables 2 and 3. Each column presents a part of the application with different sizes of summaries for each line (75%, 50%, . . ., 5%). Full text is a result obtained with 100% of the document and was published previously in Kessler et al. (2008a, 2009). Table 2 presents results obtained for each part of the application separately. We observe that AUC of CVs remains below the baseline whatever the percentage of compression. We notice however a gradual decrease in AUC scores depending on the percentage of compression. We explain this by the fact that a CV is already a summary of the most important information about the candidates and thereby attempting to summarize degrades final results. We apply the same process with cover letters. Performance is still low overall for CLs in comparison with CVs, however, there is a slight increase in AUC scores with a compression rate of 30%. We explain these results by particular information contained in a cover letter such as the form of address at the beginning or end of the letter (see fig. 4) which are noise for the ranking system of E-Gen. Results with TMP segmentation (i.e. conserving only Title, Mission, and Profile of job offer) are of better quality. Table 3 presents the results obtained by combining both parts of the application. Full text values are computed with the whole documents of the application. The first two columns show the results obtained by combining the summary of the CV and the CL. We observe again a deterioration in the results when trying to summarize the CV. Even if results are lower, it should be noted, however, that the best score is again obtained at 30%. The last two columns present the results with a summarized CL and the full CV. We observe an overall improvement of the AUC score and the best results with a compression rate of 30% of the Cover Letter. Next step is to combine summaries of the cover letter, which suppresses noise and enriches the offer with the Relevance Feedback process. Table 4 presents the results obtained with different sizes of Relevance Feedback (RF1 corresponds to one application added to the job offer, RF2 two applications added to the job offer, etc.). Each application added with the relevance feedback process consists in a full CV and a summary of the cover letter with a compression rate of 30%. A random distribution of applications produces an AUC approximately at 0.5 like explained in Fawcett (2006). We compare ISMIS Result with those obtained using a summary of the cover letter. Each test is carried out 100 times with a random distribution of relevant applications for Relevance Feedback. Then we compute an average of AUC scores obtained (the curve shows the Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 1133 Table 4 Comparison of AUC score for each size of Relevance Feedback with CORTEX summarization system. Size of Relevance Feedback Random distribution RF0 RF1 RF2 RF3 RF4 RF5 RF6 ISMIS result 0.500 0.642 0.654 0.657 0.659 0.659 0.660 0.661 Full CV and CL summary 30% compression rate 0.500 0.653 0.658 0.659 0.661 0.659 0.662 0.663 fig. 5. Results of Relevance Feedback with and without summaries of CL. average for each size). In fact, we compute the Residual Ranking (Billerbeck ; Zobel, 2006): Documents that are used for Relevance Feedback are removed from the collection before ranking with the reformulated query. We assume that the Relevance Feedback process would behave as a reinforcement learning (Sutton ; Barto, 1998) but it is impossible to experiment RFn with n > 6 with this corpus because the number of relevant candidates is too small for some job offers (see Table 1). We observe a slight improvement in results for almost any size of Relevance Feedback. We are conscious that the performance gain is low, however, it confirms previous results on the Cover Letter. fig. 5 shows this improvement. This figure confirms that the addition of just one relevant candidate (RF1) enables the AUC value to be enhanced (i.e. an improvement of 0.5–1.2%). This Relevance Feedback (i.e. RF1) is not very time-consuming for the expert. fig. 6 shows detailed results of one test. For clarity reasons, we present only 3 of the 12 jobs of our dataset in order to compare results with and without CORTEX (for each job, RFC are AUC scores with CORTEX and RF without CORTEX). For standard system, we observe a positive progress from 1% to 10% for 10 jobs between RF0 and RF1 (e.g. five jobs have an improvement between 5% and 10%). Note that between RF0 and RF6, 6 jobs have a significant positive progress between 10% and 12%. The combination of the E-Gen and CORTEX systems improve standard system results for five jobs from 1% to 5% between RF0 and RF1. Between RF0 and RF6, the Cortex version improves E-Gen’s results for eight jobs from 1% to 5%. The study of the results shows that job offer 31702 contains some relevant applications with a bad labeling (CV are labeled CL and CL are only a hyperlink to a CV). The reduction of information on the main document of the application leads the system version using summaries to degrade the AUC scores. Job offer 34861 shows a good improvement with each size of relevance feedback (RF0:0.65, RF1:0.70, RF6:0.73) and with CORTEX (RF0:0.68, RF1:0.72, RF6:0.79). The detailed study of results shows that job offer 33746 contains some empty applications labeled relevant. This leads the system with and without CORTEX to degrade final results. In the same way, an application added without CL explains the identical score in RF2 between RF and RFC for job offer 31274. </body>
  <conclusion>Job offer processing is a difficult and highly subjective task. The retrieval of relevant information concerning job descriptions and skills is not a trivial task (Loth et al., 2010) and results on this type of document have been quite low (Clech ; Author's personal copy 1134 R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 0,85 0,80 0,75 0,70 0,65 0,60 0,55 0,50 0,45 0,40 e r o c s C U A 34861 RF 34861 RFC 31274 RF 31274 RFC 31702 RF 31702 RFC RF0 RF1 RF5 RF6 RF3 RF2 RF4 Relevance Feeback size fig. 6. Comparison of detailed results for 3 jobs with and without summaries of CL. For each job, RFC means AUC scores with CORTEX and RF without CORTEX. Zighed, 2003). The information we use in this kind of process is not well formated in natural language, but follows a conventional structure. This paper deals with the CORTEX summarizer and the E-Gen system for processing job offers. E-Gen assists an employer in the recruitment task. This paper focuses on candidate answers to job offers. We rank the candidate answers by using different similarity measures and different document representations in a vector space model. We use a process of relevance feedback to perform reinforcement learning, whereby each new application added to the process assists in the decision-making. We choose to evaluate the quality of our approaches by computing Area Under the Curve. CORTEX is a summarization system using an optimal decision algorithm that combines several metrics. We present the results obtained by combining both systems. AUC obtained with summarized cover letter at 30% of compression size and a full CV shows a slight improvement in the results. As future work, we plan to apply other techniques, such as finding discriminant features of irrelevant applications using the Rocchio algorithm (Rocchio, 1971), weighting the different parts of an application, etc. in order to improve results. We also plan to use a categorization of jobs to take into consideration similar jobs, such as ’’developer’’ and ‘‘programmer’’. finally we propose to measure the CV quality by building an evaluation on an Internet portal. Our aim with this evaluation is to present a job-seeker with a list of the most suitable job ads according to his profile.</conclusion>
  <discussion>N/A</discussion>
  <biblio>Audras, I., ; Ganascia, J.-G. (2006). Apprentissage du frantais langue TtrangFre et TALN: Analyses de corpus Tcrits a l’aide d’outils d’extraction automatique du langage. In J.-M. Viprey (Ed.), 8Fmes JournTes d’Analyse de DonnTes Textuelles (pp. 67–78). Univ. de Franche ComtT, Besanton 2006 Bellot, P., ; El-Bèze, M. (2001). Classification et segmentation de textes par arbres de dTcision. In TSI (Vol. 20, pp. 107–134). HermFsBen Abdessalem Karaa, W. (2009). Web-based recruiting: A framework for cvs handling. In Second international conference on web and information technologies ‘‘ICWIT’09’’, kerkennah Island, Sfax, Tunisia, June 12–14 (pp. 395–406) Bernstein, A., Kaufmann, E., Kiefer, C., ; Bnrki, C. (2005). Simpack: A generic java library for similarity measures in ontologies. Tech. rep., University of Zurich Department of Informatics Billerbeck, B., ; Zobel, J. (2006). Efficient query expansion with auxiliary data structures. Information Systems, 31(7), 573–584Boudin, F., ; Torres Moreno, J. M. (2007). Neo-cortex: A performant user-oriented multi-document summarization system. In CICLing (pp. 551–562)Bourse, M., LeclFre, M., Morin, E., ; Trichet, F. (2004). Human resource management and semantic web technologies. In ICTTA 2004 Damascus Syria (pp. 641– 642) Cazalens, S., ; Lamarre, P. (2001). An organization of internet agents based on a hierarchy of information domains. In Proceedings MAAMAW’2001, Annecy, France (pp. 573–584) Clech, J., ; Zighed, D. A. (2003). Data mining et analyse des cv: une expérience et des perspectives. In EGC’03 Revue des Sciences et Technologies de l’Information (Vol. 17, pp. 83–92). Lyon Colucci, S., Di Noia, T., Di Sciascio, E., Donini, F. M., Mongiello, M., ; Mottola, M. (2003). A formal approach to ontology-based semantic match of skills descriptions. Journal of Universal Computer Science, Special issue on Skills Management, 9, 1437–1454 Dorn, J., ; Naz, T. (2007). Meta-search in human resource management. In Proceedings of 4th international conference on knowledge systems ICKS’07 Bangkok,Thailand (pp. 105–110) Enrica, A., ; Iezzi, D. F. (2006). Recruitment via web and information technology: A model for ranking the competences in job market. In JADT’2006, Besanton, France (pp. 79–88) Fan, R.-E., Chen, P.-H., ; Lin, C.-J. (2005). Working set selection using the second order information for training SVM. Journal of Machine Learning Research, 1889–1918 Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27, 861–874Ferri, C., Flach, P., ; Hernandez-Orallo, J. (2002). Learning decision trees using the area under the ROC curve. In Proceedings of ICML 2002: Sydney, NSW, Australia (pp. 139–146) Gorenak, I., ; Mlaker KaF, S. S. O. (2010). Cross-cultural comparison of online job advertisements. JLST, Journal of Logistics and Sustainable Transport, 2, 37–52 Author's personal copy R. Kessler et al. / Information Processing and Management 48 (2012) 1124–1135 1135 Joachims, T. (1997). A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. In ICML 1997, Nashville, Tennessee, USA (pp. 143–151) San Francisco, CA, USA Kessler, R., Béchet, N., Roche, M., El-Bèze, M., ; Torres-Moreno, J. M. (2008a). Automatic profiling system for ranking candidates answers in human resources. In OTM ’08 in Monterrey, Mexico (pp. 625–634) Kessler, R., Béchet, N., Roche, M., El-Bèze, M., ; Torres-Moreno, J. M. (2009). Job offer management: How improve the ranking of candidates. Prague: ISMIS 431–441 Kessler, R., Torres-Moreno, J. M., ; El-Bèze, M. (2007). E-Gen: Automatic job offer processing system for human ressources. In MICAI, Aguscalientes, Mexique (pp. 985–995) Kessler, R., Torres-Moreno, J. M., ; El-Bèze, M. (2008b). E-Gen: Profilage automatique de candidatures. In TALN 2008, Avignon, France (pp. 370–379)Loth, R., Battistelli, D., Chaumartin, F., De Mazancourt, H., Minel, J. L., ; Vinckx, A. (2010). Linguistic information extraction for job ads (SIRE project). In RIAO’2010 9th conference 28–30 April, Paris, France (pp. 300–303) Marchal, E., Mellet, K., ; Rieucau, G. (2007). Job board toolkits: Internet matchmaking and changes in job advertisements. Human Relations, 60(7), 1091–1113 Mocho, M., Paslaru, E., ; Simperl, B. (2006). Practical guidelines for building semantic e-recruitment applications. In I-Know’06 special track on advanced semantic technologies, Graz, Austria, September 2006 Morin, E., LeclFre, M., ; Trichet, F. (2004). The semantic web in e-recruitment. In The first European symposium of semantic Web (ESWS’2004) (pp. 67–78)Quilan, J. (1993). C4.5: Programs for machine learning. San Mateo, CA, San Francisco, CA, USA: Morgan KaufmannRafter, R., Bradley, K., ; Smyt, B. (2000). Automated collaborative filtering applications for online recruitment services. In International conference on adaptive hypermedia and adaptive web-based systems, Trento, Italy (pp. 363–368) Robertson, S., Walker, S., Jones, S., Hancock-Beaulieu, M. M., ; Gatford, M. (1994). Okapi at trec-3. NIST Special Publication 500-225: TREC-3, pp. 109–126Rocchio, J. (1971). Relevance feedback in information retrieval. In The smart system: Experiments in automatic document processing (pp. 313–323). Prentice Hall Roche, M., ; Kodratoff, Y., 2006. Pruning terminology extracted from a specialized corpus for CV ontology acquisition. In OTM’06, Montpellier, France (pp 1107–1116) Roche, M., ; Prince, V. (2008). Evaluation et dTtermination de la pertinence pour des syntagmes candidats a la collocation. In JADT (pp. 1009–1020)Salton, G., ; Buckley, C. (1990). Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 288–297Salton, G., ; Mcgill, M. J. (1986). Introduction to modern information retrieval. New York, NY, USA: McGraw-Hill IncSmyth, B., ; Bradley, K. (2003). Personalized information ordering: A case-study in online recruitment. Journal of Knowledge-Based Systems, 269–275Sparck Jones, K. (1970). Some thoughts on classification for retrieval. Journal of Documentation, 89–101Sutton, R. S., ; Barto, A. G. (1998). Reinforcement learning: An introduction (adaptive computation and machine learning). The MIT PressTolksdorf, R., Mocho, M., Heese, R., Oldakowski, R., ; Christian, B (2006). Semantic-Web-Technologien im Arbeitsvermittlungsprozess Wirtschaftsinformatik, 17–26 Torres-Moreno, J. M., Velázquez-Morales, P., ; Meunier, M. (2001). CORTEX, un algorithme pour la condensation automatique de textes. In ARCo (Vol. 2, pp 365–371) Torres-Moreno, J. M., St-Onge, P.-L., Gagnon, M., El-Bèze, M., ; Bellot, P. (2009). Automatic summarization system coupled with a question-answering system (qaas). In CoRR abs/0905.2990 Torres-Moreno, J. M., Velazquez-Morales, P., ; Meunier, J. (2002). Condensés de textes par des méthodes numériques. JADT, St Malo, France, 2, 723–734Viterbi, A. J. (1967). Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13, 260–269 Yahiaoui, L., Boufaı¨da, Z., ; Prié, Y. (2006). Semantic annotation of documents applied to e-recruitment. In SWAP 2006 – Semantic web applications and perspectives. ISSN: 1613-0073</biblio>
</article>