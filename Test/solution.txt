Nom du fichier pdf : Boudin-Torres-2006.pdf

Titre:
    A Scalable MMR Approach to Sentence Scoring
for Multi-Document Update Summarization

Auteurs:
    Florian Boudin : florian.boudin@univ-avignon.fr
    Marc El-Bèze : juan-manuel.torres@univ-avignon.fr
    Juan-Manuel Torres-Moreno : marc.elbeze@univ-avignon.fr


Abstract:
    We present S MMR , a scalable sentence
scoring method for query-oriented up-
date summarization. Sentences are scored
thanks to a criterion combining query rele-
vance and dissimilarity with already read
documents (history). As the amount of
data in history increases, non-redundancy
is prioritized over query-relevance. We
show that S MMR achieves promising re-
sults on the DUC 2007 update corpus.

Bibliographie:
    Boudin, F. and J.M. Torres-Moreno. 2007. A Co-
sine Maximization-Minimization approach for User-
Oriented Multi-Document Update Summarization.
InRecent Advances in Natural Language Processing
(RANLP) , pages 81–87.
Carbonell, J. and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In 21st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval , pages 335–336.
ACM Press New York, NY , USA.Hachey, B., G. Murray, and D. Reitter. 2005. The
Embra System at DUC 2005: Query-oriented Multi-
document Summarization with a Very Large Latent
Semantic Space. In Document Understanding Con-
ference (DUC) .
Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC’s
GISTexter at DUC 2007: Machine Reading for Up-
date Summarization. In Document Understanding
Conference (DUC) .
Lin, Z., T.S. Chua, M.Y . Kan, W.S. Lee, L. Qiu, and
S. Ye. 2007. NUS at DUC 2007: Using Evolu-
tionary Models of Text. In Document Understanding
Conference (DUC) .
Lin, C.Y . 2004. Rouge: A Package for Automatic
Evaluation of Summaries. In Workshop on Text Sum-
marization Branches Out , pages 25–26.
Mani, I. and M.T. Maybury. 1999. Advances in Auto-
matic Text Summarization . MIT Press.
Mani, I. and G. Wilson. 2000. Robust temporal pro-
cessing of news. In 38th Annual Meeting on Asso-
ciation for Computational Linguistics , pages 69–76.
Association for Computational Linguistics Morris-
town, NJ, USA.
Murray, G., S. Renals, and J. Carletta. 2005. Extractive
Summarization of Meeting Recordings. In Ninth Eu-
ropean Conference on Speech Communication and
Technology . ISCA.
Salton, G., A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM , 18(11):613–620.
Swan, R. and J. Allan. 2000. Automatic generation
of overview timelines. In 23rd annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval , pages 49–56.
Winkler, W. E. 1999. The state of record linkage and
current research problems. In Survey Methods Sec-
tion, pages 73–79.
Witte, R., R. Krestel, and S. Bergler. 2007. Generat-
ing Update Summaries for DUC 2007. In Document
Understanding Conference (DUC) .
Ye, S., L. Qiu, T.S. Chua, and M.Y . Kan. 2005. NUS
at DUC 2005: Understanding documents via con-
cept links. In Document Understanding Conference
(DUC) .

Nom du fichier pdf : Das_Martins.pdf

Titre:
    A Survey on Automatic Text Summarization

Auteurs:
    Dipanjan Das : dipanjan@cs.cmu.edu
    André F.T. Martins : afm@cs.cmu.edu


Abstract:
    The increasing availability of online information has necessitated intensive
research in the area of automatic text summarization within the Natural Lan-
guage Processing (NLP) community. Over the past half a century, the prob-
lem has been addressed from many different perspectives, in varying domains
and using various paradigms. This survey intends to investigate some of the
most relevant approaches both in the areas of single-document and multiple-
document summarization, giving special emphasis to empirical methods and
extractive techniques. Some promising approaches that concentrate on specific
details of the summarization problem are also discussed. Special attention is
devoted to automatic evaluation of summarization systems, as future research
on summarization is strongly dependent on progress in this area.

Bibliographie:
    Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla-
tions of Mathematical Monographs) . Oxford University Press. [20]
Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable
summarizer with knowledge acquired from robust nlp techniques. In Mani, I.
and Maybury, M. T., editors, Advances in Automatic Text Summarization , pages
71{80. MIT Press. [4, 5]
Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization.
InProceedings ISTS'97 . [8]
Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the
context of multi-document summarization. In Proceedings of ACL '99 . [12, 13,
14, 16]
Baxendale, P. (1958). Machine-made index for technical literature - an experiment.
IBM Journal of Research Development , 2(4):354{361. [2, 3, 5]
Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The
mathematics of statistical machine translation: parameter estimation. Comput.
Linguist. , 19(2):263{311. [18]
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and
Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05:
Proceedings of the 22nd international conference on Machine learning , pages 89{
96, New York, NY, USA. ACM. [8]
Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking
for reordering documents and producing summaries. In Proceedings of SIGIR '98 ,
pages 335{336, New York, NY, USA. [12, 14, 15]
Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing .
PhD thesis, University of Pennsylvania. [13, 20]
Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov
models. In Proceedings of SIGIR '01 , pages 406{407, New York, NY, USA. [6]
Cover, T. and Thomas, J. (1991). Elements of Information Theory . Wiley. [25]
Daumé e III, H. and Marcu, D. (2002). A noisy-channel model for document com-
pression. In Proceedings of the Conference of the Association of Computational
Linguistics (ACL 2002). [20]
Daumé e III, H. and Marcu, D. (2004). A tree-position kernel for document compres-
sion. In Proceedings of DUC2004 . [20]
Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the
ACM , 16(2):264{285. [2, 3, 4]
28Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu-
tions. IEEE Transactions on Information Theory , 49(7):1858{1860. [26]
Evans, D. K. (2005). Similarity-based multilingual multi-document summarization.
Technical Report CUCS-014-05, Columbia University. [12, 17]
Gous, A. (1999). Spherical subfamily models. [20, 21]
Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for
natural language processing and information retrieval. In Proc. 17th International
Conf. on Machine Learning , pages 351{358. Morgan Kaufmann, San Francisco,
CA. [20]
Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In
Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza-
tion, pages 81{94. MIT Press. [17]
Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen-
tence compression. In AAAI/IAAI , pages 703{710. [19]
Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer.
InProceedings SIGIR '95 , pages 68{73, New York, NY, USA. [4]
Lebanon, G. (2006). Sequential document representations and simplicial curves. In
Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence . [22]
Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words
framework for document representation. J. Mach. Learn. Res. , 8:2405{2441. [21,
22]
Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of
CIKM '99 , pages 55{62, New York, NY, USA. [5]
Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In
Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro-
ceedings of the ACL-04 Workshop , pages 74{81, Barcelona, Spain. [8, 23, 24,
25]
Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap-
proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL
'06, pages 463{470, Morristown, NJ, USA. [25, 26]
Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of
the Fifth conference on Applied natural language processing , pages 283{290, San
Francisco, CA, USA. [5]
Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In
Proceedings of the ACL-02 Workshop on Automatic Summarization , pages 45{51,
Morristown, NJ, USA. [23]
29Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of
Research Development , 2(2):159{165. [2, 3, 6, 8]
Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search
and matching. In AAAI/IAAI , pages 622{628. [15, 16]
Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In
Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215 , pages
206{215, Montreal, Canada. [9, 10, 20]
Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of
natural language texts . PhD thesis, University of Toronto. Adviser-Graeme Hirst.
[10]
McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E.
(1999). Towards multidocument summarization by reformulation: Progress and
prospects. In AAAI/IAAI , pages 453{460. [11, 12, 13, 14, 16]
McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news
articles. In Proceedings of SIGIR '95 , pages 74{82, Seattle, Washington. [8, 11,
12]
Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM ,
38(11):39{41. [4, 9]
Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned
from the document understanding conference. In Proceedings of AAAI 2005,
Pittsburgh, USA . [7]
Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical
structure extraction. In Proceedings of Coling '94 , pages 344{348, Morristown,
NJ, USA. [9]
Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings
of the ACL'02 Workshop on Automatic Summarization , pages 1{8, Morristown,
NJ, USA. [7]
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for
automatic evaluation of machine translation. In Proceedings of ACL '02 , pages
311{318, Morristown, NJ, USA. [24]
Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue
on summarization. Computational Linguistics. , 28(4):399{408. [1, 2]
Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization
of multiple documents: sentence extraction, utility-based evaluation, and user
studies. In NAACL-ANLP 2000 Workshop on Automatic summarization , pages
21{30, Morristown, NJ, USA. [12, 16, 17]
30Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza-
tion of multiple documents. Information Processing and Management 40 (2004) ,
40:919{938. [16, 17]
Radev, D. R. and McKeown, K. (1998). Generating natural language summaries
from multiple on-line sources. Computational Linguistics , 24(3):469{500. [12]
Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in
automatic information. In Proceedings of SIGIR '88 , pages 147{160, New York,
NY, USA. [15]
Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic
indexing. Communications of the ACM , 18:229{237. [20]
Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving
hard satisfiability problems. In AAAI , pages 440{446. [11]
Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document
summarization by combining RankNet and third-party sources. In Proceedings of
the EMNLP-CoNLL , pages 448{457. [7, 8]
Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract):
a statistical approach to generating highly condensed non-extractive summaries.
InProceedings of SIGIR '99 , pages 315{316, New York, NY, USA. [18]

Nom du fichier pdf : Gonzalez_2018_Wisebe.pdf

Titre: WiSeBE: Window-Based Sentence Boundary Evaluation

Auteur:
    Carlos-Emiliano González-Gallardo : carlos-emiliano.gonzalez-gallardo@alumni.univ-avignon.fr
    Juan-Manuel Torres-Moreno : juan-manuel.torres@univ-avignon.fr

Abstract:
    Sentence Boundary Detection (SBD) has been a major
research topic since Automatic Speech Recognition transcripts have been
used for further Natural Language Processing tasks like Part of Speech
Tagging, Question Answering or Automatic Summarization. But what
about evaluation? Do standard evaluation metrics like precision, recall,
F-score or classiﬁcation error; and more important, evaluating an auto-
matic system against a unique reference is enough to conclude how well
a SBD system is performing given the ﬁnal application of the transcript?
In this paper we propose Window-based Sentence Boundary Evaluation
(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary
Detection systems based on multi-reference (dis)agreement. We evalu-
ate and compare the performance of diﬀerent SBD systems over a set
of Youtube transcripts using WiSeBE and standard metrics. This dou-
ble evaluation gives an understanding of how WiSeBE is a more reliable
metric for the SBD task.
Keywords: Sentence Boundary Detection · Evaluation
Transcripts · Human judgment

Bibliographie:
    1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog-
nized speech for web presentation of large audio archive. In: 2012 35th International
Conference on Telecommunications and Signal Processing (TSP), pp. 441–445.
IEEE (2012)
2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over
a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,
A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134–138. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-41552-9 14
3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented
transcript based on word vector. In: LREC (2016)
4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.
76(5), 378 (1971)
5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net-
works. In: IEEE International Conference on Information Systems and Economic
Intelligence (2017)
6. González-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran-
scripts informativeness study: an approach based on automatic summarization. In:
Conférence en Recherche d’Information et Applications (CORIA), Rennes, France,
May (2018)
7. González-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for
French with subword-level information vectors and convolutional neural networks.
arXiv preprint arXiv:1802.04559 (2018)
8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.
In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium
ISCA Tutorial and Research Workshop (ITRW) (2000)
9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni-
tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),
82–97 (2012)
10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech
recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308–318
(2015)
11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com-
put. Linguist. 32(4), 485–525 (2006)
12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts
using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology
Workshop (SLT), pp. 433–440. IEEE (2016)
13. Kolář, J., Lamel, L.: Development and evaluation of automatic punctuation for
French and english speech-to-text. In: Thirteenth Annual Conference of the Inter-
national Speech Communication Association (2012)
14. Kolář, J., Švec, J., Psutka, J.: Automatic punctuation annotation in Czech broad-
cast news speech. In: SPECOM 2004 (2004)
15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine
learning from imbalanced data for sentence boundary detection in speech. Comput.
Speech Lang. 20(4), 468–494 (2006)
16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran-
dom ﬁelds. In: Proceedings of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing. pp. 177–186. Association for Computational Linguistics
(2010)
17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:
Conference on Empirical Methods in Natural Language Processing (1996)
18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg-
mentation of speech for automatic summarization. In: 2006 IEEE International
Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.
IEEE (2006)
19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro-
ceedings of the Fourth Conference on Applied Natural Language Processing, pp.
78–83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,
USA (1994)
20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua-
tion. Comput. Linguist. 23(2), 241–267 (1997)
21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.
R. Soc. Lond. 58, 240–242 (1895)
22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical
phrase-based translation. In: Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014)
23. Rott, M., Červa, P.: Speech-to-text summarization using automatic phrase extrac-
tion from recognized text. In: Sojka, P., Horák, A., Kopeček, I., Pala, K. (eds.) TSD
2016. LNCS (LNAI), vol. 9924, pp. 101–108. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-45510-5 12
24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based
study. In: Proceedings of the Fourth International Conference on Spoken Language,
1996. ICSLP 1996, vol. 3, pp. 1868–1871. IEEE (1996)
25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:
Proceedings of the sixth conference on Applied natural language processing, pp.
84–89. Association for Computational Linguistics (2000)
26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational
speech. In: Proceedings of the Fourth International Conference on Spoken Lan-
guage, 1996. ICSLP 1996, vol. 2, pp. 1005–1008. IEEE (1996)
27. Strassel, S.: Simple metadata annotation speciﬁcation v5. 0, linguistic data consor-
tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE
V5.0.pdf
28. Tilk, O., Alumäe, T.: Bidirectional recurrent neural network with attention mech-
anism for punctuation restoration. In: Interspeech 2016 (2016)
29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen-
tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704
(2017)
30. Ueﬃng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation
prediction for spoken and written text. In: Interspeech, pp. 3097–3101 (2013)
31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disﬂuency removal for
improving spoken language translation. In: 2010 IEEE International Conference on
Acoustics Speech and Signal Processing (ICASSP), pp. 5214–5217. IEEE (2010)
32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network
approach for sentence boundary detection in broadcast news. In: Fifteenth Annual
Conference of the International Speech Communication Association (2014)
33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://
doi.org/10.1007/978-1-4471-5779-3


Nom du fichier pdf : Iria_Juan-Manuel_Gerardo.pdf

Titre: On the Development of the RST Spanish Treebank

Auteur:
    Iria da Cunha : iria.dacunha@upf.edu
    Juan-Manuel Torres-Moreno : juan-manuel.torres@univ-avignon.fr
    Gerardo Sierra : gsierram@iingen.unam.mx

Abstract:
    In this article we present the RST Spanish
Treebank, the first corpus annotated with
rhetorical relations for this language. We
describe the characteristics of the corpus,
the annotation criteria, the annotation
procedure, the inter-annotator agreement,
and other related aspects. Moreover, we
show the interface that we have developed
to carry out searches over the corpus’
annotated texts.

Bibliographie:
    Ron Artstein, and Massimo Poesio. 2008. Survey
Article: Inter-Coder Agreement for Computational
Linguistics. Computational Linguistics, 34(4):555-
596.
Nadjet Bouayad-Agha, Leo Wanner, and Daniel
Nicklass. 2006. Discourse structuring of dynamic
content. Procesamiento del lenguaje natural, 37:207-
213.
M. Teresa Cabré (1999). La terminología:
representación y comunicación. Barcelona: IULA-
UPF.
Lynn Carlson and Daniel Marcu. 2001. Discourse
Tagging Reference Manual. ISI Technical Report
ISITR-545. Los Ángeles: University of Southern
California.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002a. RST Discourse Treebank.
Pennsylvania: Linguistic Data Consortium.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2002b. Building a Discourse-Tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of the 2nd SIGDIAL
Workshop on Discourse and Dialogue, Eurospeech
2001.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 20(1):37-46
Iria da Cunha, Eric SanJuan, Juan-Manuel Torres-
Moreno, Marina Lloberes, and Irene Castellón. 2010.
Discourse Segmentation for Spanish based on
Shallow Parsing. Lecture Notes in Computer
Science, 6437:13-23.
Iria da Cunha, and Mikel Iruskieta. 2010. Comparing
rhetorical structures of different languages: The
influence of translation strategies. Discourse Studies,
12(5):563-598.
Iria da Cunha, Leo Wanner, and M. Teresa Cabré. 2007.
Summarization of specialized discourse: The case of
medical articles in Spanish. Terminology, 13(2):249-
286.
Dmitriy Dligach, Rodney D. Nielsen, and Martha
Palmer. 2010. To Annotate More Accurately or to
Annotate More. In Proceedings of the 4th Linguistic
Annotation Workshop (LAW-IV). 48th Annual
Meeting of the Association for Computational
Linguistics.
Joseph L. Fleis. 1971. Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378-382.
Eduard Hovy. 2010. Annotation. A Tutorial. Presented
at the 48th Annual Meeting of the Association for
Computational Linguistics.
Nancy Ide and Pustejovsky, J. (2010). What Does
Interoperability Mean, anyway? Toward an
Operational Definition of Interoperability. In
Proceedings of the Second International Conference
on Global Interoperability for Language Resources
(ICGL 2010).
William C. Mann, and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional
theory of text organization. Text, 8(3):243-281.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing Summarization. Massachusetts:
Institute of Technology.
Mitchell P. Marcus, Beatrice Santorini, Mary A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treenbank.
Computational Linguistics, 19(2):313-330.
Michael O’Donnell. 2000. RSTTOOL 2.4 – A markup
tool for rhetorical structure theory. In Proceedings of
the International Natural Language Generation
Conference. 253-256.
Martha Palmer, and Nianwen Xue. 2010. Linguistic
Annotation. Handbook of Computational Linguistics
and Natural Language Processing.
Martha Palmer, Randee Tangi, Stephanie Strassel,
Christiane Fellbaum, and Eduard Hovy (on-line).
Historical Development and Future Directions in
Data Resource Development. MINDS report.
http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, Ralph Weischedel. 2007.
OntoNotes: A Unified Relational Semantic
Representation. In Proceedings of the First IEEE
International Conference on Semantic Computing
(ICSC-07).
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni
Miltsakaki, Livio Robaldo, Aravind Joshi, and
Bonnie Webber. 2008. The Penn Discourse Treebank
2.0. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008).
David Reitter, and Mandred Stede. 2003. Step by step:
underspecified markup in incremental rhetorical
analysis. In Proceedings of the 4th International


Nom du fichier pdf : jing-cutepaste.pdf

Titre: Cut and Paste Based Text Summarization

Auteur:
    Hongyan Jing : hjing@cs.columbia.edu
    Kathleen R. McKeown : kathy@cs.columbia.edu

Abstract:
    We present a cut and paste based text summa-
rizer, which uses operations derived from an anal-
ysis of human written abstracts. The summarizer
edits extracted sentences, using reduction to remove
inessential phrases and combination to merge re-
suiting phrases together as coherent sentences. Our
work includes a statistically based sentence decom-
position program that identifies where the phrases of
a summary originate in the original document, pro-
ducing an aligned corpus of summaries and articles
which we used to develop the summarizer.

Bibliographie:
    ANSI. 1997. Guidelines for abstracts. Technical Re-
port Z39.14-1997, NISO Press, Bethesda, Mary-
land.
L. Baum. 1972. An inequality and associated max-
imization technique in statistical estimation of
probabilistic functions of a markov process. In-
equalities, (3):1-8.
Edward T. Cremmins. 1982. The Art of Abstracting.
ISI Press, Philadelphia.
Brigitte Endres-Niggemeyer, Kai Haseloh, Jens
Mfiller, Simone Peist, Irene Santini de Sigel,
Alexander Sigel, Elisabeth Wansorra, Jan
Wheeler, and Brfinja Wollny. 1998. Summarizing
Information. Springer, Berlin.
Raya Fidel. 1986. Writing abstracts for free-text
searching. Journal of Documentation, 42(1):11-
21, March.
Hongyan Jing and Kathleen R. McKeown. 1998.
Combining multiple, large-scale resources in a
reusable lexicon for natural language generation.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and the
17th International Conference on Computational
Linguistics, volume 1, pages 607-613, Universit6
de Montreal, Quebec, Canada, August.
Hongyan Jing and Kathleen R. McKeown. 1999.
The decomposition of human-written summary
sentences. In Proceedings of the P2nd In-
ternational ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval(SIGIR'99), pages 129-136, University of
Berkeley, CA, August.
Hongyan Jing. 2000. Sentence reduction for au-
tomatic text summarization. In Proceedings of
ANLP 2000.
Aravind.K. Joshi. 1987. Introduction to tree-
adjoining grammars. In A. Manaster-Ramis, ed-
itor, Mathematics of Language. John Benjamins,
Amsterdam.
Inderjeet Mani, David House, Gary Klein, Lynette
Hirschman, Leo Obrst, Therese Firmin, Michael
Chrzanowski, and Beth Sundheim. 1998. The
TIPSTER SUMMAC text summarization eval-
uation final report. Technical Report MTR
98W0000138, The MITRE Corporation.
Inderjeet Mani, Barbara Gates, and Erie Bloedorn.
1999. Improving summaries by revising them. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics(A CL '99),
pages 558-565, University of Maryland, Mary-
land, June.
Michael MeCord, 1990. English Slot Grammar.
IBM.
Samuel Thurber, editor. 1924. Prgcis Writing for
American Schools. The Atlantic Monthly Press,
INC., Boston.
A.J. Viterbi. 1967. Error bounds for convolution
codes and an asymptotically optimal decoding al-
gorithm. IEEE Transactions on Information The-
ory, 13:260-269.


Nom du fichier pdf : kessler94715.pdf

Titre: Extraction of terminology in the field of construction

Auteur:
    Rémy Kessler : remy.kessler@univ-ubs.fr
    Nicolas Béchet : nicolas.bechet@irisa.fr
    Giuseppe Berio : giuseppe.berio@univ-ubs.fr

Abstract:
    We describe a corpus analysis method to extract
terminology from a collection of technical specifications in the
field of construction. Using statistics and word n-grams analysis,
we extract the terminology of the domain and then perform
pruning steps with linguistic patterns and internet queries to
improve the quality of the final terminology. Results are evaluated
by using a manual evaluation carried out by 6 experts in the field.
Index Terms—terminology extraction, Internet queries, linguis-
tic patterns.

Bibliographie:
    [1] S. Meignier and T. Merlin, “Lium spkdiarization: an open source toolkit
for diarization,” in in CMU SPUD Workshop, 2010.
[2] P. Pauwels and W. Terkaj, “Express to owl for construction industry:
Towards a recommendable and usable ifcowl ontology,” Automation in
Construction, vol. 63, pp. 100–133, 03 2016.
[3] H. Cunningham, “Gate, a general architecture for text engineering,” in
Computers and the Humanities, vol. 36, 2002, pp. 223–254.
[4] P. Cimiano and J. Völker, “text2onto,” in International conference on
application of natural language to information systems. Springer, 2005,
pp. 227–238.
[5] P. Kluegl, M. Toepfer, P.-D. Beck, G. Fette, and F. Puppe, “Uima ruta:
Rapid development of rule-based information extraction applications,”
Natural Language Engineering, vol. 22, no. 1, p. 1–40, 2016.
[6] M. Roche and Y. Kodratoff, “Exit: Un système itératif pour l’extraction
de la terminologie du domaine à partir de corpus spécialisés,” in
Proceedings of JADT 4, 2004, pp. 946–956.
[7] B. Biébow, S. Szulman, and A. J. B. Clément, “Terminae: A linguistics-
based tool for the building of a domain ontology,” in Knowledge
Acquisition, Modeling and Management, D. Fensel and R. Studer, Eds.,
1999, pp. 49–66.
[8] P. Drouin, “Term extraction using non technical corpora as point of lever-
age,” in John Benjamins Publishing Company: Amsterdam/Philadelphia,
n. Terminology, vol. 9, Ed., 2003, pp. 99–115.[9] M. F. M. Chowdhury, A. M. Gliozzo, and S. M. Trewin, “Domain-
specific terminology extraction by boosting frequency metrics,” Sep. 27
2018, uS Patent App. 15/469,766.
[10] G. Bouma, “Normalized (pointwise) mutual information in collocation
extraction,” Proceedings of GSCL, 2009.
[11] Y. Bestgen, “Evaluation de mesures d’association pour les bigrammes et
les trigrammes au moyen du test exact de fisher,” Proceedings of TALN
2017, pp. 10–19, 2017.
[12] A. L. Meyers, Y. He, Z. Glass, J. Ortega, S. Liao, A. Grieve-Smith,
R. Grishman, and O. Babko-Malaya, “The termolator: Terminology
recognition based on chunking, statistical and search-based scores,”
Frontiers in Research Metrics and Analytics, vol. 3, p. 19, 2018.
[13] L. Gillam, M. Tariq, and K. Ahmad, “Terminology and the construction
of ontology,” TERMINOLOGY, vol. 11, pp. 55–81, 2005.
[14] A. Panchenko, S. Faralli, E. Ruppert, S. Remus, H. Naets, C. Fairon, S. P.
Ponzetto, and C. Biemann, “TAXI at semeval-2016 task 13: a taxonomy
induction method based on lexico-syntactic patterns, substrings and
focused crawling,” in Proceedings of the 10th International Workshop
on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA,
USA, June 16-17, 2016, 2016, pp. 1320–1327.
[15] E. Lefever, L. Macken, and V. Hoste, “Language-independent
bilingual terminology extraction from a multilingual parallel corpus,”
in Proceedings of the 12th Conference of the European Chapter
of the Association for Computational Linguistics, ser. EACL ’09.
Stroudsburg, PA, USA: Association for Computational Linguistics,
2009, pp. 496–504. [Online]. Available: http://dl.acm.org/citation.cfm?
id=1609067.1609122
[16] L. Macken, E. Lefever, and V. Hoste, “Texsis: bilingual terminology
extraction from parallel corpora using chunk-based alignment,”
Terminology, vol. 19, no. 1, pp. 1–30, 2013. [Online]. Available:
http://dx.doi.org/10.1075/term.19.1.01mac
[17] G. Dias and H.-J. Kaalep, “Automatic extraction of multiword units
for estonian : Phrasal verbs,” in Languages in Development, 2003, p.
41:81–91.
[18] B. Daille, “Study and implementation of combined techniques for
automatic extraction of terminology,” The Balancing Act: Combining
Symbolic and Statistical Approaches to Language, 12 2002.
[19] C. Lang, R. Schneider, and K. Suchowolec, “Extracting specialized
terminology from linguistic corpora,” GRAMMAR AND CORPORA, p.
425, 2018.
[20] E. Amjadian, D. Inkpen, T. S. Paribakht, and F. Faez, “Local-global
vectors to improve unigram terminology extraction,” in Proceedings of
the 5th International Workshop on Computational Terminology, 2016.
[21] G. Wohlgenannt and F. Minic, “Using word2vec to build a simple
ontology learning system.” in International Semantic Web Conference
(Posters & Demos), 2016.
[22] H. Schmid, “Probabilistic part-of-speech tagging using decision trees,”
1994.
[23] E. Altman, “Financial ratios, discriminant analysis and the predic-
tion of corporate bankruptcy.” in The Journal of Finance, 23(4).
doi:10.2307/2978933, 1968, pp. 589–609.
[24] J. Cohen, “A Coefficient of Agreement for Nominal Scales,” Educational
and Psychological Measurement, vol. 20, no. 1, p. 37, 1960.
[25] M. L. McHugh, “Interrater reliability: the kappa statistic,” in Biochemia
medica, 2012.
[26] M. Apidianaki, S. M. Mohammad, J. May, E. Shutova, S. Bethard,
and M. Carpuat, “Proceedings of the 12th international workshop on
semantic evaluation,” in Proceedings of The 12th International Workshop
on Semantic Evaluation. Association for Computational Linguistics,
2018. [Online]. Available: http://aclweb.org/anthology/S18-1000


Nom du fichier pdf : kesslerMETICS-ICDIM2019.PDF

Titre: A word embedding approach to explore a collection of discussions of people in psychological distress

Auteur:
    Rémy Kessler : remy.kessler@univ-ubs.fr
    Nicolas Béchet : nicolas.bechet@irisa.fr
    Gudrun Ledegen : gudrun.ledegen@univ-rennes2.fr
    Frederic Pugnière-Saavedra : frederic.pugniere-saavedra@univ-ubs.fr

Abstract:
    In order to better adapt to society, an association
has developed a web chat application that allows anyone to
express and share their concerns and anguishes. Several thousand
anonymous conversations have been gathered and form a new
corpus of stories about human distress and social violence. We
present a method of corpus analysis combining unsupervised
learning and word embedding in order to bring out the themes
of this particular collection. We compare this approach with a
standard algorithm of the literature on a labeled corpus and
obtain very good results. An interpretation of the obtained
clusters collection confirms the interest of the method.
Keywords—word2vec, unsupervised learning, word embedding.

Bibliographie:
    [1] D. Fassin, “Et la souffrance devint sociale,” in Critique. 680(1), 2004,
pp. 16–29.
[2] ——, “Souffrir par le social, gouverner par l’écoute,” in Politix. 73(1),
2006, pp. 137–157.
[3] MacQueen, J., “Some methods for classification and analysis of multi-
variate observations,” in Proceedings of the Fifth Berkeley Symposium
on Mathematical Statistics and Probability, Vol. 1: Statistics. USA:
University of California Press, 1967, pp. 281–297.
[4] D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful
seeding,” Proceedings of the Eighteenth Annual ACM-SIAM Symposium
on Discrete Algorithms, pp. 1027–1035, 2007.
[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids.
Delft University of Technology : reports of the Faculty of
Technical Mathematics and Informatics, 1987. [Online]. Available:
https://books.google.fr/books?id=HK-4GwAACAAJ
[6] G. N. Lance and W. T. Williams, “A general theory of classificatory
sorting strategies1. hierarchical systems,” The Computer Journal 4, pp.
373–380, 1967.
[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood
from incomplete data via the em algorithm,” in Journal of the royal
society, series B, 1977, pp. 1–38.
[8] J. D. Banfield and A. E. Raftery, “Model-based gaussian and non-
gaussian clustering,” in Biometrics, vol. 49, 1993, pp. 803–821.
[9] T. Kohonen, “Self-organized formation of topologically correct feature
maps,” Biological Cybernetics, pp. 59–69, Jan 1982.
[10] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm
to detect community structures in large-scale networks.” Physical review.
E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007.
[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,
J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, “Sentiment analysis
of suicide notes: A shared task,” Biomedical Informatics Insights, pp.
3–16, 2012.
[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Azé, S. Bringay, and
P. Poncelet, “Mining twitter for suicide prevention,” in Natural Language
Processing and Information Systems: 19th International Conference on
Applications of Natural Language to Information Systems, NLDB 2014,
Montpellier, France, June 18-20, 2014. Proceedings. Springer, 2014,
pp. 250–253.
[13] R. Kessler, J.-M. Torres, and M. El-Bèze, “Classification thématique de
courriel par des méthodes hybrides,” Journée ATALA sur les nouvelles
formes de communication écrite, 2004.
[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed
representations of words and phrases and their compositionality,” in
Proceedings of NIPS’13. USA: Curran Associates Inc., 2013,
pp. 3111–3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=
2999792.2999959
[15] C. D. Manning and H. Schütze, Foundations of Statistical Natural
Language Processing. Cambridge, MA, USA: MIT Press, 1999.
[16] C. Goutte and E. Gaussier, “ A Probabilistic Interpretation of Precision,
Recall and F-Score, with Implication for Evaluation,” ECIR 2005, pp.
345–359, 2005.
[17] M. Hoffman, F. R. Bach, and D. M. Blei, “Online learning for latent
dirichlet allocation,” in Advances in Neural Information Processing
Systems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
and A. Culotta, Eds. 23, 2010, pp. 856–864.


Nom du fichier pdf : mikheev J02-3002.pdf

Titre: Periods, Capitalized Words, etc.

Auteur: Andrei Mikheev : mikheev@cogsci.ed.ac.uk

Abstract:
    In this article we present an approach for tackling three important aspects of text normaliza-
tion: sentence boundary disambiguation, disambiguation of capitalized words in positions where
capitalization is expected, and identification of abbreviations. As opposed to the two dominant
techniques of computing statistics or writing specialized grammars, our document-centered ap-
proach works by considering suggestive local contexts and repetitions of individual words within
a document. This approach proved to be robust to domain shifts and new lexica and produced per-
formance on the level with the highest reported results. When incorporated into a part-of-speech
tagger, it helped reduce the error rate significantly on capitalized words and sentence boundaries.
We also investigated the portability to other languages and obtained encouraging results.

Bibliographie:
    Aberdeen, John S., John D. Burger, David S.
Day, Lynette Hirschman, Patricia
Robinson, and Marc Vilain. 1995. “Mitre:
Description of the alembic system used
for MUC-6.” In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
Columbia, Maryland, November. Morgan
Kaufmann.
Baldwin, Breck, Christine Doran, Jeffrey
Reynar, Michael Niv, Bangalore Srinivas,
and Mark Wasson. 1997. “EAGLE: An
extensible architecture for general
linguistic engineering.” In Proceedings of
Computer-Assisted Information Searching on
Internet (RIAO ’97), Montreal, June.
Baum, Leonard E. and Ted Petrie. 1966.
Statistical inference for probabilistic
functions of finite Markov chains. Annals
of Mathematical Statistics 37:1559–1563.
Bikel, Daniel, Scott Miller, Richard
Schwartz, and Ralph Weischedel. 1997.
“Nymble: A high performance learning
name-finder.” In Proceedings of the Fifth
Conference on Applied Natural Language
Processing (ANLP’97), pages 194–200.
Washington, D.C., Morgan Kaufmann.
Brill, Eric. 1995a. Transformation-based
error-driven learning and natural
language parsing: A case study in
part-of-speech tagging. Computational
Linguistics 21(4):543–565.
Brill, Eric. 1995b. “Unsupervised learning of
disambiguation rules for part of speech
tagging.” In David Yarovsky and Kenneth
Church, editors, Proceedings of the Third
Workshop on Very Large Corpora, pages
1–13, Somerset, New Jersey. Association
for Computational Linguistics.
Burnage, Gavin. 1990. CELEX: A Guide for
Users. Centre for Lexical Information,
Nijmegen, Netherlands.
Chinchor, Nancy. 1998. “Overview of
MUC-7.” In Seventh Message Understanding
Conference (MUC-7): Proceedings of a
Conference Held in Fairfax, April. Morgan
Kaufmann.
Church, Kenneth. 1988. “A stochastic parts
program and noun-phrase parser for
unrestricted text.” In Proceedings of the
Second ACL Conference on Applied Natural
Language Processing (ANLP’88), pages
136–143, Austin, Texas.
Church, Kenneth. 1995. “One term or two?”
In SIGIR’95, Proceedings of the 18th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 310–318, Seattle,
Washington, July. ACM Press.
Clarkson, Philip and Anthony J. Robinson.
1997. “Language model adaptation using
mixtures and an exponentially decaying
cache.” In Proceedings IEEE International
Conference on Speech and Signal Processing,
Munich, Germany.
Cucerzan, Silviu and David Yarowsky. 1999.
“Language independent named entity
recognition combining morphological and
contextual evidence.” In Proceedings of
Joint SIGDAT Conference on EMNLP and
VLC.
Francis, W. Nelson and Henry Kucera. 1982.
Frequency Analysis of English Usage: Lexicon
and Grammar. Houghton Mifflin, New
York.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. “One sense per
discourse.” In Proceedings of the Fourth
DARPA Speech and Natural Language
Workshop, pages 233–237.
Grefenstette, Gregory and Pasi Tapanainen.
1994. “What is a word, what is a
sentence? Problems of tokenization.” In
The Proceedings of Third Conference on
Computational Lexicography and Text
Research (COMPLEX’94), Budapest,
Hungary.
Krupka, George R. and Kevin Hausman.
1998. Isoquest Inc.: Description of the
netowl extractor system as used for
MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
Fairfax, VA. Morgan Kaufmann.
Kuhn, Roland and Renato de Mori. 1998. A
cache-based natural language model for
speech recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence
12:570–583.
Kupiec, Julian. 1992. Robust part-of-speech
tagging using a hidden Markov model.
Computer Speech and Language.
Mani, Inderjeet and T. Richard MacMillan.
1995. “Identifying unknown proper
names in newswire text.” In B. Boguraev
and J. Pustejovsky, editors, Corpus
Processing for Lexical Acquisition. MIT Press,
Cambridge, Massachusetts, pages 41–59.
Marcus, Mitchell, Mary Ann Marcinkiewicz,
and Beatrice Santorini. 1993. Building a
large annotated corpus of English: The
Penn treebank. Computational Linguistics
19(2):313–329.
Mikheev, Andrei. 1997. Automatic rule
induction for unknown word guessing.
Computational Linguistics 23(3):405–423.
Mikheev, Andrei. 1999. A knowledge-free
method for capitalized word
disambiguation. In Proceedings of the 37th
Conference of the Association for
Computational Linguistics (ACL’99), pages
159–168, University of Maryland, College
Park.
Mikheev, Andrei. 2000. “Tagging sentence
boundaries.” In Proceedings of the First
Meeting of the North American Chapter of the
Computational Linguistics (NAACL’2000),
pages 264–271, Seattle, Washington.
Morgan Kaufmann.
Mikheev, Andrei, Clair Grover, and Colin
Matheson. 1998. TTT: Text Tokenisation Tool.
Language Technology Group, University
of Edinburgh. Available at
http://www.ltg.ed.ac.uk/software/ttt/
index.html.
Mikheev, Andrei, Clair Grover, and Marc
Moens. 1998. Description of the ltg
system used for MUC-7. In Seventh
Message Understanding Conference
(MUC–7): Proceedings of a Conference Held in
Fairfax, Virginia. Morgan Kaufmann.
Mikheev, Andrei and Liubov Liubushkina.
1995. Russian morphology: An
engineering approach. Natural Language
Engineering 1(3):235–260.
Palmer, David D. and Marti A. Hearst. 1994.
“Adaptive sentence boundary
disambiguation.” In Proceedings of the
Fourth ACL Conference on Applied Natural
Language Processing (ANLP’94), pages
78–83, Stuttgart, Germany, October.
Morgan Kaufmann.
Palmer, David D. and Marti A. Hearst. 1997.
Adaptive multilingual sentence boundary
disambiguation. Computational Linguistics
23(2):241–269.
Park, Youngja and Roy J. Byrd. 2001.
“Hybrid text mining for finding
abbreviations and their definitions.” In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMLP’01), pages 16–19, Washington,
D.C. Morgan Kaufmann.
Ratnaparkhi, Adwait. 1996. “A maximum
entropy model for part-of-speech


Nom du fichier pdf : Mikolov.pdf

Titre: Efficient Estimation of Word Representations in Vector Space

Auteur:
    Tomas Mikolov : tmikolov@google.com
    Kai Chen : kaichen@google.com
    Greg Corrado : gcorrado@google.com
    Jeffrey Dean : jeff@google.com

Abstract:
    We propose two novel model architectures for computing continuous vector repre-
sentations of words from very large data sets. The quality of these representations
is measured in a word similarity task, and the results are compared to the previ-
ously best performing techniques based on different types of neural networks. We
observe large improvements in accuracy at much lower computational cost, i.e. it
takes less than a day to learn high quality word vectors from a 1.6 billion words
data set. Furthermore, we show that these vectors provide state-of-the-art perfor-
mance on our test set for measuring syntactic and semantic word similarities.

Bibliographie:
    [1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3:1137-1155, 2003.
[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-
chines, MIT Press, 2007.
[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine
translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language
Processing and Computational Language Learning, 2007.
[4] R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In International Conference on Machine Learning,
ICML, 2008.
[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-
guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-
2537, 2011.
[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.
Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.
[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.
[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations
via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational
Linguistics, 2012.
[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-
tributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,
MIT Press, 1986.
[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring
degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), 2012.
[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for
sentiment analysis. In Proceedings of ACL, 2011.
[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-
versity of Technology, 2007.
[14] T. Mikolov, J. Kopecký, L. Burget, O. Glembek and J. Černocký. Neural network based lan-
guage models for higly inflective languages, In: Proc. ICASSP 2009.
[15] T. Mikolov, M. Karafiát, L. Burget, J. Černocký, S. Khudanpur. Recurrent neural network
based language model, In: Proceedings of Interspeech, 2010.
[16] T. Mikolov, S. Kombrink, L. Burget, J. Černocký, S. Khudanpur. Extensions of recurrent neural
network language model, In: Proceedings of ICASSP 2011.
[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Černocký. Empirical Evaluation and Com-
bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.
[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. Černocký. Strategies for Training Large Scale
Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-
ing, 2011.
[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-
sity of Technology, 2012.
[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-
tations. NAACL HLT 2013.
[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of
Words and Phrases and their Compositionality. Accepted to NIPS 2013.
[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,
2007.
[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural
Information Processing Systems 21, MIT Press, 2009.
[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language
models. ICML, 2012.
[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,
2005.
[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-
propagating errors. Nature, 323:533.536, 1986.
[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,
2007.
[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and
Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.
[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for
Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.
[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-
tional Joint Conference on Artificial Intelligence, 2005.
[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for
Measuring Relational Similarity. NAACL HLT 2013.
[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft
Research Technical Report MSR-TR-2011-129, 2011.


Nom du fichier pdf : Nasr.pdf

Titre: MACAON An NLP Tool Suite for Processing Word Lattices

Auteur:
    Alexis Nasr : alexis.nasr@lif.univ-mrs.fr
    Frédéric Béchet : frederic.bechet@lif.univ-mrs.fr
    Jean-François Rey : jean-francois.rey@lif.univ-mrs.fr
    Benoı̂t Favre : benoit.favre@lif.univ-mrs.fr
    Joseph Le Roux : joseph.le.roux@lif.univ-mrs.fr

Abstract:
    MACAON is a tool suite for standard NLP tasks
developed for French. MACAON has been de-
signed to process both human-produced text
and highly ambiguous word-lattices produced
by NLP tools. MACAON is made of several na-
tive modules for common tasks such as a tok-
enization, a part-of-speech tagging or syntac-
tic parsing, all communicating with each other
through XML files . In addition, exchange pro-
tocols with external tools are easily definable.
MACAON is a fast, modular and open tool, dis-
tributed under GNU Public License.

Bibliographie:
    Anne Abeillé, Lionel Clément, and François Toussenel.
2003. Building a treebank for french. In Anne
Abeillé, editor, Treebanks. Kluwer, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information,
Prague, Czech Republic, pages 8–15.
M. Attia, J. Foster, D. Hogan, J. Le Roux, L. Tounsi, and
J. van Genabith. 2010. Handling Unknown Words in
Statistical Latent-Variable Parsing Models for Arabic,
English and French. In Proceedings of SPMRL.
Lucie Barque, Alexis Nasr, and Alain Polguère. 2010.
From the definitions of the trésor de la langue française
to a semantic database of the french language. In EU-
RALEX 2010, Leeuwarden, Pays Bas.
Frédéric Béchet and Alexis Nasr. 2009. Robust depen-
dency parsing for spoken language understanding of
spontaneous speech. In Interspeech, Brighton, United
Kingdom.
Olivier Blanc, Matthieu Constant, and Eric Laporte.
2006. Outilex, plate-forme logicielle de traitement de
textes écrits. In TALN 2006, Leuven.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural language engineering, 10(3-4):211–225.
M. Mohri, F. Pereira, and M. Riley. 2000. The design
principles of a weighted finite-state transducer library.
Theoretical Computer Science, 231(1):17–32.
P. Nocera, G. Linares, D. Massonié, and L. Lefort. 2006.
Phoneme lattice based A* search algorithm for speech
recognition. In Text, Speech and Dialogue, pages 83–
111. Springer.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In ACL.
Benoı̂t Sagot and Pierre Boullier. 2008. Sxpipe 2:
architecture pour le traitement présyntaxique de cor-
pus bruts. Traitement Automatique des Langues,
49(2):155–188.
Benoı̂t Sagot, Lionel Clément, Eric Villemonte de la
Clergerie, and Pierre Boullier. 2006. The lefff 2 Syn-
tactic Lexicon for French: Architecture, Acquisition,
Use. In International Conference on Language Re-
sources and Evaluation, Genoa.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on Spo-
ken Language Processing, Denver, Colorado.
S.J. Young. 1994. The HTK Hidden Markov Model
Toolkit: Design and Philosophy. Entropic Cambridge
Research Laboratory, Ltd, 2:2–44.


Nom du fichier pdf : Torres.pdf

Titre:Summary Evaluation with and without References

Auteur:
    Juan-Manuel Torres-Moreno : juan-manuel.torres@univ-avignon.fr
    Horacio Saggion : horacio.saggion@upf.edu
    Iria da Cunha : iria.dacunha@upf.edu
    Eric SanJuan : eric.sanjuan@univ-avignon.fr
    Patricia Velázquez-Morales : patricia velazquez@yahoo.com

Abstract:
    We study a new content-based method for
the evaluation of text summarization systems without
human models which is used to produce system rankings.
The research is carried out using a new content-based
evaluation framework called F RESA to compute a variety of
divergences among probability distributions. We apply our
comparison framework to various well-established content-based
evaluation measures in text summarization such as C OVERAGE,
R ESPONSIVENESS, P YRAMIDS and ROUGE studying their
associations in various text summarization tasks including
generic multi-document summarization in English and French,
focus-based multi-document summarization in English and
generic single-document summarization in French and Spanish.
Index Terms—Text summarization evaluation, content-based
evaluation measures, divergences.

Bibliographie:
    [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
no. 6, pp. 1506–1520, 2007.
[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
USA: NIST, November 17-19 2008.
[4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.
[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.
[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.
[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.
[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.
[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.
[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 145-151, 1991.
[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.
[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.
[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, 1998.
[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
“A French Human Reference Corpus for multi-documents
summarization and sentence compression,” in LREC’10, vol. 2,
Malta, 2010, p. In press.
[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
of Associative Memories: performants applications of Enertex algorithm
in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
861–871.
[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,
“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,
St Malo, France, 2002, pp. 723–734.
[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,
“Automatic summarization using terminological and semantic
resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,
p. In press.
[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
systems of automatic text summarization,” Automatic Documentation
and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
[24] C. D. Manning and H. Schütze, Foundations of Statistical Natural
Language Processing.
Cambridge, Massachusetts: The MIT Press,
1999.
[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,
vol. 43, no. 6, pp. 1449–1481, 2007.
[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized
discourse: The case of medical articles in spanish,” Terminology, vol. 13,
no. 2, pp. 249–286, 2007.
[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
Student Research Workshop.
Toulouse, France: Association for
Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
Singapore, August 2009, pp. 23–30.
[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
Sentence compression,” in Proceedings of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2000, pp. 703–710.


Nom du fichier pdf : Torres-moreno1998.pdf

Titre: Efficient Adaptive Learning for Classification Tasks with Binary Units

Auteur:
    J. Manuel Torres Moreno : Pas d'adresse mail
    Mirta B. Gordon : Pas d'adresse mail

Abstract:
    This article presents a new incremental learning algorithm for classi-
fication tasks, called NetLines, which is well adapted for both binary
and real-valued input patterns. It generates small, compact feedforward
neural networks with one hidden layer of binary units and binary output
units. A convergence theorem ensures that solutions with a finite num-
ber of hidden units exist for both binary and real-valued input patterns.
An implementation for problems with more than two classes, valid
for any binary classifier, is proposed. The generalization error and
the size of the resulting networks are compared to the best published
results on well-known classification benchmarks. Early stopping is shown
to decrease overfitting, without improving the generalization perfor-
mance.

Bibliographie:
    Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un-
published doctoral dissertation, Ecole Polytechnique Fédérale de Lausanne,
Switzerland.
Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical
Review A, 44, 6888.
Bottou, L., & Vapnik, V. (1992). Local learning algorithms. Neural Computation,
4(6), 888–900.
Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department
of Statistics, University of California at Berkeley.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
and regression trees. Monterey, CA: Wadsworth and Brooks/Cole.
Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., & Hopfield, J.
(1987). Large automatic learning, rule extraction, and generalization. Complex
Systems, 1, 877–922.
Depenau, J. (1995). Automated design of neural network architecture for classification.
Unpublished doctoral dissertation, Computer Science Department, Aarhus
University.
Drucker, H., Schapire, R., & Simard, P. (1993). Improving performance in neu-
ral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &
C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42–
49). San Mateo, CA: Morgan Kaufmann.
Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architec-
ture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,
2 (pp. 524–532). San Mateo: Morgan Kaufmann.
Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree
networks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural
Information Processing Systems, 6 (pp. 1035–1042). San Mateo, CA: Morgan
Kaufmann.
Frean, M. (1990). The Upstart algorithm: A method for constructing and training
feedforward neural networks. Neural Computation, 2(2), 198–209.
Frean, M. (1992). A “thermal” perceptron learning rule. Neural Computation, 4(6),
946–957.
Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality
(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.
Fritzke, B. (1994). Supervised learning with growing cell structures. In
J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural informa-
tion processing systems, 6 (pp. 255–262). San Mateo, CA: Morgan Kaufmann.
Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern
Recognition, Oct. 28–31, Paris, vol. 4.
Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).
5èmes Journées Nationales du PRC-IA Teknea, Nancy.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the
bias/variance dilemma. Neural Computation, 4(1), 1–58.
Goodman, R. M., Smyth, P., Higgins, C. M., & Miller, J. W. (1992). Rule-based
neural networks for classification and probability estimation. Neural Compu-
tation, 4(6), 781–804.
Gordon, M. B. (1996). A convergence theorem for incremental learning with real-
valued inputs. In IEEE International Conference on Neural Networks, pp. 381–
386.
Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule
that finds the optimal weights. In M. Verleysen (Ed.), European Symposium on
Artificial Neural Networks (pp. 105–110). Brussels: D Facto.
Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature
dependent algorithm. Europhysics Letters, 29(3), 257–262.
Gordon, M. B., Peretto, P., & Berchier, D. (1993). Learning algorithms for percep-
trons from statistical physics. Journal of Physics I (France), 3, 377–387.
Gorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered
network trained to classify sonar targets. Neural Networks, 1, 75–89.
Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In
W. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses. Sin-
gapore: World Scientific.
Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using
the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:
Carnegie Mellon University.
Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A
stepwise procedure for building and training a neural network. In J. Hérault
& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications
(pp. 41–50). Berlin: Springer-Verlag.
Marchand, M., Golea, M., & Ruján, P. (1990). A convergence theorem for sequen-
tial learning in two-layer perceptrons. Europhysics Letters, 11, 487–492.
Martinez, D., & Estève, D. (1992). The offset algorithm: Building and learning
method for multilayer neural networks. Europhysics Letters, 18, 95–100.
Mézard, M., & Nadal, J.-P. (1989). Learning in feedforward layered networks:
The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191–2203.
Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time al-
gorithm for generating neural networks for pattern classification: Its stability
properties and some test results. Neural Computation, 5(2), 317–330.
Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural net-
work. Int. J. Neur. Syst., 1, 55–59.
Prechelt, L. (1994). PROBEN1—A set of benchmarks and benchmarking rules for neu-
ral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,
Faculty of Informatics.
Raffin, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror,
a temperature dependent learning algorithm. Neural Computation, 7(6), 1206–
1224.
Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category
learning. Biological Cybernetics, 45, 35–41.
Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm
for the construction and training of a class of multilayer perceptron. Neural
Networks, 6(1), 535–545.
Sirat, J. A., & Nadal, J.-P. (1990). Neural trees: A new tool for classification.
Network, 1, 423–438.
Solla, S. A. (1989). Learning and generalization in layered neural networks: The
contiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks
from Models to Applications. Paris: I.D.S.E.T.
Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled
with optimal perceptron learning for classification. In M. Verleysen (Ed.),
European Symposium on Artificial Neural Networks. Brussels: D Facto.
Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar
signals benchmark. Neural Proc. Letters, 7(1), 1–4.
Trhun, S. B., et al. (1991). The monk’s problems: A performance comparison of different
learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie
Mellon University.
Vapnik, V. (1992). Principles of risk minimization for learning theory. In
J. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), Advances in neural informa-
tion processing systems, 4 (pp. 831–838). San Mateo, CA: Morgan Kaufmann.
Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neu-
ral networks. In M. Verleysen (Ed.), European Symposium on Artificial Neural
Networks (pp. 359–364). Brussels: D Facto.
Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern
separation for medical diagnosis applied to breast cytology. In Proceedings of
the National Academy of Sciences, USA, 87, 9193–9196.
